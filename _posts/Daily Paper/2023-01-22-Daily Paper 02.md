---
layout: post
title: Daily Paper (2023.1.3 - 2023.1.22)
categories: [Daily Paper]
description: Classical CV papers
keywords: Computer Vision
---

## Convolution

### [14_Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)

- This paper introduces a novel way to visualize the Convolutional Network. The convolutional network likes a blackbox. It is meaningful to find out how it functions inside. To realize this purpose, three key methods are used: **Unpooling, Rectification(ReLU) and Filtering**.

- Deconvnet performs the work that maps features to pixels reverse to convnet maps pixels to features. Unpooling uses switches variables which recording the locations of the max within each pooling region to place the reconstructions from the layer above into appropriate locations. Rctification uses the *relu* function to reconstruct. Filering flips each filter vertically and horizontally. The details of deconvnet are shown below.

![Deconvnet](/images/DailyPaper/02/14.png "Deconvnet")

- After deconvnet and visualize the layers, the visualization of features of a network performed on ImageNet are shown below. Even though the CNN could not be explained throughtly, it can be observed that the features that CNN learned is divided by layers. The lower layer learns more abstract features and the high layer will learn more specific features. This is similar to human's visual system. Generally speaking, the deeper neural network will learn more generalize features. From my perspective, this work is meaningful to the further CNN works and the future CNN works also provide examples to verify the kernel of this paper. For instance, the deeper *VGGNet*.

![Visualization of Features](/images/DailyPaper/02/15.png "Visualization of Features")

- What's more, the lower layer converge after a few epochs and the higher layer needs more epochs to converge. There is another important point in this paper: the convolution performs good invariance in the vertical translation and scale and performs bad invariance in the rotation. The graph of this experiment is shown below. This provides the idea of data argumentation that rotated images could be used as the new data-set to train the network. What's more, the following experiment shows that the deep neural network can implicitly establish the correspondence between some locations in the image.

![Translation, Scale and Rotation Invariance](/images/DailyPaper/02/16.png "Translation, Scale and Rotation Invariance")

### [14_Learning Deconvolution Network for Semantic Segmentation](http://arxiv.org/abs/1505.04366)

- This paper proposes the deconvolution network by add the deconvolution network which is composed of deconvolution, ReLU and unpooling layers at the top of VGG16. Making the modification is because of the two obvious disadvantages of Fully Connected Network(FCN). First, the network can handle only a single scale semantics within image due to the fixed-size receptive field. Second, the detailed structures of an object are often lost or smoothed because the label map, input to the deconvolutional layer, is too coarse and deconvolution procedure is overly simple. Simply speaking, FCN often judges the large objects as unconsitent labels and misses the small objects and more details. The improved deconvolution network's structure is shown below. It is obvious that it is the Decoder-Encoder structure. The more details about Unpooling and Deconvolution will not be discussed here since other parts in the blog has discussed. The illustration of deconvolution and unpooling operations are shown as below. In the following test performed in PASCAL VOC 2012, this network performs very well.

![Structure of Deconvolution Network](/images/DailyPaper/02/20.png "Structure of Deconvolution Network")

![Illustration of Deconvolution and Unpooling Operations](/images/DailyPaper/02/21.png "Illustration of Deconvolution and Unpooling Operations")

- This network treated the segmentation as instance-wise segmentation rather than pixel-wise segmentation. In the following experiment part, the EDevonnet combined with CRF also actually shows this kind of method performs well. Moreover, in the training part, Batch Normalization, Two-stage Training, Aggeregation Instance-wise Segmentation Maps and Ensemble with FCN are also adopted to help the network train better in the small-samples dataset.

### [15_Dilated Residual Networks](http://ieeexplore.ieee.org/document/8099558/)

- **Dilated Residual Network (DRN)** is rose to copy with the problem of the loss of spatial acuity caused by reducing resolution. The other methods to keep the high resolution are *up-convolutions, skip connections, and other post-hoc measure.* This paper improves the resolution of input image of ResNet by 4 times. The comparison between ResNet and DRN architecture is shown below.

![DRN](/images/DailyPaper/02/3.jpeg "ResNet and DRN")  

The details of Dilated Convolution is shown below.

![Dilated Convolution](/images/DailyPaper/02/1.gif "Dilated Convolution")

- However, **Degridding** will appear after using DRN. Gridding artifacts occur when a feature map has higher-frequency content than the sampling rate of the dilated convolution. The example of a gridding artifact is shown below. To handle with this problem, the structure of DRN is modified.

![A Gridding Artifact](/images/DailyPaper/02/5.jpeg "A Gridding Artifact")

- In DRN-B and DRN-C, the pooling layer is removed since the max pooling operation leads to high-amplitude high-frequency activations. Besides, a 2-dilated residual block followed by a 1-dilated block is used as the layer-7 and layer-8 of DRN-B and DRN-C. And DRN-C removes the residual connections. The details of DRN-A, DRN-B and DRN-C are shown below. In the experiemnt section, DRN-C achieves the best performance. The visualization of DRN-C is also the best.

![Improved DRN](/images/DailyPaper/02/4.jpeg "Improved DRN Structure")

### [15_Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)

- This paper proposes **Deformable Convolution and Deformable RoI Pooling** to enhance the transformation modeling capabiliy of CNNs. The resulting CNNs are called *Deformable Convolutional Networks or Deformable ConvNets*

- Deformable Convolution adds 2D offsets to the rgular sampling locations in the standard convolution. The details of deformable convolution are shown below. The offset contributes to the irregular convolution shape. Therefore, the convolution kernel will fit with the shape and size of objection to increase the accuracy of the network.

![Deformable Convolution](/images/DailyPaper/02/9.jpeg "Deformable Convolution")

![3x3 Deformable Convolution](/images/DailyPaper/02/8.jpeg "3x3 Deformable Convolution")

- Deformable RoI Pooling adds an offset to each bin position in the regular bin partition of the previous RoI pooling. The details of deformable RoI Pooling are shown below. The position-sentitive RoI pooling adopts a specific positive-sensitive score map $x_{i,j}$ to obtain the real offset $\Delta p_{i,j}$.

![3x3 Deformable RoI Pooling](/images/DailyPaper/02/6.jpeg "3x3 Deformable RoI Pooling")

![3x3 Deformable Position-Sensitive RoI Pooling](/images/DailyPaper/02/7.jpeg "3x3 Deformable Position-Sensitive RoI Pooling")

- In the following experiments, deformable convolution is added in different location of ResNet. Besides, big and small objects are used to test and validation. What's more, this paper points that **deformable convolution is a generalization of dilated convolution**. The improvement in performance is not very significant, and the major improvement is based on the methods.(The performance improves in the Deformable ConvNets v2.)

- From my perspective, it is similar to the application of attention mechanism. Maybe the deformable convolution can combine with attention mechanism to improve the model performance.

### [29_Deformable ConvNets v2_More Deformable, Better Results](https://arxiv.org/pdf/1811.11168.pdf)

- This paper improves the problem occurred in Deformable ConvNet v1 which is the cover of irrelevant context. In this paper, three metrics are rose: **Effective Reception Fields, Effective Sampling/Bin Locations, and Error-bounded Saliency Regions**. In the experiment part, the Deformable ConvNet is coorporated into  *Fast R-CNN and Mask R-CNN* with different backbone.

- What's more, three methods are tackle to improve the Deformable ConvNet v2. First, more deformable convolutions are used in the network. Second, a new weigh is trained and applied to alleviate the effecy of irrelevant. The new weight is represented as: $y(p)=\sum_{k=1}^K\omega_k x(p+p_k+\Delta p_k) \Delta m_k$. Third, mimiching the R-CNN feature to elimiate the irrelevant information. In R-CNN, crop and resize the input images can effectively remove the irrelevant information in the background. The information of crop patch is more valuable. The network training with R-CNN feature mimicking is shown as below. The performance of this new version network improves about 2% than original network.

![Network Training with R-CNN Feature Mimicking](/images/DailyPaper/02/12.jpeg "Network Training with R-CNN Feature Mimicking")

## Classical Models

### [29_LeNet_Gradient-Based Learning Applied to Document Recognition](https://ieeexplore.ieee.org/document/726791)

- **"Hello World"**

- This paper raises the LeNet-5 network to solve the problem of recognizing the hand-written digits problem. The general architecture is the following:  
Input layer(32*32) -> C1(kernel size = 5x5, stride  = 1, depth = 6) -> S2(Kernel size = 2x2, stride = 2, Sigmoid) -> C3(kernel size = 5x5, stride = 1; To control the parameters and achieve the asymetric, the feature map is not generated by S2.) -> S4(Kernel size = 2x2, stride = 2) -> C5(Kernel size = 5x5, stride = 1) -> F6(Full connected layer) -> Output layer
The architecture of LeNet-5 is shown below. In Caffe, the architecture of LeNet used **ReLU** to replace Sigmoid, and increase the kernels to meet the hardware conditions nowadays to improve the accuracy and speed.

![The Architecture of LeNet](/images/DailyPaper/02/11.png "The Architecture of LeNet")

- The resulting effect of this network is shown below. This gif is cited from [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)

![LeNet](/images/DailyPaper/02/10.gif "LeNet")

### [15_AlexNet_Imagenet-classification-with-deep-convolutional-neural-networks-Paper](https://dl.acm.org/doi/10.1145/3065386)

- **AlexNet** is the classical network. In this paper, **Dropout** is proposed to reduce overfutting in the fully-connected layers. The traning and testing dataset is *ImageNet LSVRC-2010*. Also, **Rectified Linear Units (ReLUs)** are applied to improve the speed of network. AlexNet's structure is similar to LeNet. The overall architecture of AlexNet is shown below. It consists of 5 convolution layers and 3 fully connected layers. Because of the huge calculate parameters, 2 GPUs are used to train in parallel.

![Dilated Convolution](/images/DailyPaper/02/2.jpeg "AlexNet")

- What's more, **Local Response Normalization (LPN)** is used to increase the generalization ability. The equation of LPN is given by:
$b_{x,y}^i=a_{x,y}^i/(k=\alpha \sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(\alpha_{x,y}^i)^2)^\beta$
What's more, **Overlapping Pooling** is applied to decrease the top-1 and top-5 errors.

- To reduce the overfiting problem, two methods are applied. The first one is **Data Augmentation**. This method has two forms: *generate image translations and horizontal reflections, alter the intensities of the RGB channels in training images.* The second method is the **Dropout** layer. This layer randomly ignores some neural to avoid the overfitting problem.

### [16_VGGNet_VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](http://arxiv.org/abs/1409.1556)

- This paper focuses on the effect of network depth and proposes **VGGNet**. The dataset used to test is *ImageNet Challenge 2014*. This paper also points that the LRN is not helpful but increases memory consumption and computation time.  The ConvNet configurations are shown below. There are two kinds of VGGNet, one is VGG16 and the other one is VGG19. The mainly difference between these two network is the depth.

![VGGNet](/images/DailyPaper/02/13.jpeg "VGGNet")

- To increase the depth and keep the same receptive field, **three 3 x 3 convolutional kernels replace the 7 x 7 convolution kernel and two 3 x 3 convolutional kernels replace the 5 x 5 convolution kernel**. The convoluton diagram is shown below. After replacing, the number of parameters also decreases from $7^2 C^2$ to $3^3 C^2$. What's more, it can seen from the table that the 1 x 1 convolutional layer incorporates in the network. **The function of 1 x 1 conv. layer is to increase the non-linearity of the decision function without affecting the receptive fields of the convolutional layer.** The 1 x 1 conv. layer works like a kind of non-linearity shift. In the following experiments, the deeper network performs well which shows that **the depth of conv. net is very critical**. However, the deeper network will consumer more computational resource and memory. The high consumer mostly is because of the fully connected layers. Some papers in the following indicates that the fully connected layer could be removed without affecting the performance of the network. The details will be introduced in the following blog. Also, VGGNet contributes a general pretrained model for the future network which is meaningful for the researcher.

![Small Convolutional Kernel](/images/DailyPaper/02/17.jpeg "3 x 3 Convolutions Replaces 5 x 5 and 7 x  7Convolutions")

### [16_ZFNet_Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901/)

- The analysis about ZFNet is shown in the first paper analysis of this blog.

### [17_Inceptionv1_GoogLeNet_Going deeper with convolutions](http://arxiv.org/abs/1409.4842)
GoogLeNet / Inception v1
sparse structure
1*1 conv + ReLu to decrease the complexity
### [17_Inceptionv2_Batch Normalization_Accelerating Deep Network Training](http://arxiv.org/abs/1502.03167)

- The difference between LPN and BN is that, https://www.jianshu.com/p/ef689144c86e
Inception v2
Solve the internal covariant shift problem
By Batch normalization
1. Learning rate can be set large and change fast
2. Dropout and L2 reguliazation could be eliminated
3. Shuffle the samples
### [17_Inceptionv3_Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)
Inception v3

Inception v2 + BN auxiliary
Balance

Factorization
224->299
Reduce grid
Label Smoothing regularization (LSR): Let the model not so confident
### [14__Xception_ Deep Learning with Depthwise Separable Convolutions](http://arxiv.org/abs/1610.02357)

- Depthwise separable convolution to replace the Inception to improve the performance
Residual network is very helpful (F(x)+x to prevent the gradient disappearance)
Non-linearity is harmful for the shallow space

### [17_Inceptionv4, Inception-ResNet and the impact of residual connections on Learning](http://arxiv.org/abs/1602.07261)
Inception v4
Inception-ResNet
replace the filter concatenation stage of the Inception architecture with residual connections
More inception and the better connections
it is useful to scale the residuals to avoid the 0 occurs when  the number of filters exceeded 1000

### [18_ResNet_Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385)

- This paper is an amazing paper at the evolution process of computer vision. This paper proposes **Residual Learning Framework** to ease the training pressure of training deep network.

### [18_ResNeXt_Aggregated Residual Transformations for Deep Neural Networks](http://ieeexplore.ieee.org/document/8100117/)

### [19_DenseNet_Densely Connected Convolutional Networks](https://ieeexplore.ieee.org/document/8099726/)

### [19_HRNet v1_Deep High-Resolution Representation Learning](http://arxiv.org/abs/1908.07919)

### [19_HRNet v2_High-Resolution Representations for Labeling Pixels and Regions](http://arxiv.org/abs/1904.04514)

### [19_SENet_Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)

### [110_CSPNET A NEW BACKBONE THAT CAN ENHANCE LEARNING](http://arxiv.org/abs/1911.11929)

### [110_EfficientNet Rethinking Model Scaling for Convolutional Neural Networks](http://arxiv.org/abs/1905.11946)

## Image Classification

### [111_Highway Networks](http://arxiv.org/abs/1505.00387)

### [111_Application of Highway Network_Training Very Deep Networks](http://arxiv.org/abs/1507.06228)

### [111_PReLu_Delving Deep into Rectifiers_Surpassing Human-Level Performance on ImageNet Classification](http://ieeexplore.ieee.org/document/7410480/)

- This paper proposes **Parametric Rectified Linear Unit(PReLU)** which is the upgraded version of ReLu and LReLu. The definition of PReLU is **f($y_i$) = $y_i$, if $y_i$ > 0; $a_i$$y_i$, if $y_i$ $\le$ 0 = max(0, $y_i$) + $a_i$min(0, $y_i$)**. The comparision between ReLU and PReLU is shown below. If $a_i$ is very small and fixed, PReLU could be seen as LReLU. The upgrate of $a_i$ could be achieved by back-propagation whose details will not be shown here.

![ReLU and PReLU](/images/DailyPaper/02/18.jpeg "ReLU and PReLU")

- Only a very small number of parameters are added in PReLU, which means that the computational effort of the network and the risk of over-fitting are increased only a little. In the following experiments, ReLU is replaced by PReLU. The improved network receives excellent results in ImageNet 2012. What's mmore, a new initialization method based on PReLU is developed which converges much faster than Xavier method.

## Image Segmentation

### [112_FCN_Fully Convolutional Networks for Semantic Segmentation](http://arxiv.org/abs/1411.4038)

### [112_FPN_Feature Pyramid Networks for Object Detection](http://arxiv.org/abs/1612.03144)

### [112_U-Net_Convolutional Networks for Biomedical](http://arxiv.org/abs/1505.04597)

## GAN

### [112_GAN_Generative Adversarial Nets](http://arxiv.org/abs/1406.2661)

- This paper proposes a framework for $estimating generative models via an adversarial process$. This framework needs two models: $a generative model(G) and a discriminative model(D)$. The relation between these two models likes the police and thief. G will be trained to maximize the probability of D making a mistake while D will estimates the probability that a sample came from the training data rather than G. That is why the network is called **Generative Adversarial Nets(GAN)**

![Training Process](/images/DailyPaper/02/19.png "Training Process of Generative and Discriminative")

After k cycles of updating the discriminator, the parameters of the generator are updated once using a smaller learning rate, and the generator is trained to minimize the gap between the generated samples and the real samples, which is equivalent to making the discriminator discriminate errors as much as possible.
Having a good discriminator first makes it possible to distinguish the real samples from the generated samples well before the generator can be updated more accurately.
A sufficient condition for a GAN global minimum is when the probability distribution of the generator and the probability distribution of the true values are the same Loop

Fixed "Discriminator D", trained "Generator G
Fix "Generator G" and train "Discriminator D

## Object Detection

### [113_Selective Search for Object Recognition](http://link.springer.com/10.1007/s11263-013-0620-5)

### [113_OverFeat_Integrated Recognition, Localization and Detection using Convolutional Networks](http://arxiv.org/abs/1312.6229)

### [114_R-CNN_Rich feature hierarchies for accurate object detection and semantic segmentation](http://arxiv.org/abs/1311.2524)

### [114_SPPNet_Spatial Pyramid Pooling in Deep Convolutional](http://arxiv.org/abs/1406.4729)

### [114_Fast R-CNN](http://arxiv.org/abs/1504.08083)

### [114_Faster R-CNN_Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)

### [114_Mask R-CNN](http://arxiv.org/abs/1703.06870) (This paper focuses on Image Segmentation)

### [115_R-FCN_Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409)

### [115_SSD_Single Shot MultiBox Detector](http://arxiv.org/abs/1512.02325)

### [115_YOLOv1_You Only Look Once_Unified Real-Time Object Detection](http://arxiv.org/abs/1506.02640)

### [116_YOLOv2_YOLO9000_Better, Faster, Stronger](http://arxiv.org/abs/1612.08242)

### [116_YOLOv3_An Incremental Improvement](http://arxiv.org/abs/1804.02767)

### [116_YOLOv4_Optimal Speed and Accuracy of Object Detection](http://arxiv.org/abs/2004.10934)

### [117_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_Detection on Drone-Captured Scenarios](https://ieeexplore.ieee.org/document/9607487/)

### [117_YOLO v5_Improved YOLOv5 network for real-time multi-scale traffic sign detection](https://link.springer.com/10.1007/s00521-022-08077-5)

### [117_YOLOV_ Making Still Image Object Detectors Great at Video Object Detection](http://arxiv.org/abs/2208.09686)

### [118_PP-YOLOE_An evolved version of YOLO](http://arxiv.org/abs/2203.16250)

### [118_YOLOX_Exceeding YOLO Series in 2021](http://arxiv.org/abs/2107.08430)

### [118_YOLO-Z_Improving small object detection in YOLOv5 for autonomous vehicles](http://arxiv.org/abs/2112.11798)

## Light Networks

### [119_MobileNets_Efficient Convolutional Neutral Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)

### [119_MobileNetV2_Inverted Residuals and Linear Bottlenecks](https://ieeexplore.ieee.org/document/8578572/)

### [119_MobileNetv3_Searching for MobileNetV3](https://ieeexplore.ieee.org/document/9008835/)

### [120_ShuffleNet_An Extremely Efficient Convolutional Neural Network for Mobile Device](https://arxiv.org/abs/1707.01083)

### [120_ShuffleNet V2_Practical Guidelines for Efficient CNN Architecture Design](https://link.springer.com/10.1007/978-3-030-01264-9_8)

### [120_ShuffleNet v3_An efficient solution for semantic segmentation_ShuffleNet v2 with atrous seperable convolutions](http://link.springer.com/10.1007/978-3-030-20205-7_4)

## Graph Neural Networks

### [121_A Comprehensive Survey on Graph Neural Networks](http://arxiv.org/abs/1901.00596)

### [121_Deep Learning on Graphs_A Survey](http://arxiv.org/abs/1812.04202)

### [121_Graph neural networks_A review of methods and applications](https://arxiv.org/abs/1812.08434)

### [121_Temporal Convolutional Networks for Action Segmentation and Detection](https://arxiv.org/abs/1611.05267)

### [121_Temporal Convolutional Networks_A Unified Approach to Action Segmentation](https://arxiv.org/abs/1608.08242)

## Vision Transformer

### [122_ViT v1_Attention Is All You Need](http://arxiv.org/abs/1706.03762)

### [122_ViT v2_AN IMAGE IS WORTH 16X16 WORDS_TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/abs/2010.11929)

### [122_Swin Transformer V1_Hierarchical Vision Transformer using Shifted Windows](https://ieeexplore.ieee.org/document/9710580/)

### [122_Swin Transformer V2_Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)

### [122_ViTAE_Vision Transformer Advanced by Exploring Intrinsic Inductive Bias](http://arxiv.org/abs/2106.03348)

### [122_ViTAEv2_Vision Transformer Advanced by Exploring Inductice Bias for Image Recognition and Beyond](http://arxiv.org/abs/2202.10108)