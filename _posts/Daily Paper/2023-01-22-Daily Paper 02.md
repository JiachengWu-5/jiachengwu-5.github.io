---
layout: post
title: Daily Paper (2023.1.3 - 2023.1.22)
categories: [Daily Paper]
description: Classical CV papers
keywords: Computer Vision
---

This blog shows some classical CV papers and the evolution process of CV models.

## Convolution

### [14_Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)

- This paper introduces a novel way to visualize the Convolutional Network. The convolutional network is like a black box. It is meaningful to find out how it functions inside. Three essential methods are used: **Unpooling, Rectification(ReLU), and Filtering** to realize the purpose.

- Deconvnet performs the work that maps features to pixels reverse to convnet maps pixels to features. Unpooling uses switches variables that record the max locations within each pooling region to place the reconstructions from the layer above into appropriate locations. Rectification uses the *relu* function to reconstruct. Filtering flips each filter vertically and horizontally. The details of the deconvnet are shown below.

![Deconvnet](/images/DailyPaper/02/14.png "Deconvnet")

- After deconvnet and visualizing the layers, the visualization of features of a network performed on ImageNet is shown below. Even though CNN could not be explained thoroughly, it can be observed that the features that CNN learned are divided into layers. The lower layer learns abstract features, while the high layer learns specific features. This kind of learning characteristic is similar to the human visual system. Generally speaking, the deeper neural network will learn more generalized features. From my perspective, this work is meaningful to the further CNN works, and future CNN works also provide examples to verify the kernel of this paper. For instance, the deeper *VGGNet*.

![Visualization of Features](/images/DailyPaper/02/15.png "Visualization of Features")

- What's more, the lower layer converges after a few epochs, and the higher layer needs more epochs to converge. Another essential point in this paper is that the convolution performs good invariance in the vertical translation and scale and performs bad invariance in the rotation. The graph of this experiment is shown below. The graph provides the idea of data argumentation that rotated images could be used as the new data set to train the network. Moreover, the following experiment shows that the deep neural network can implicitly establish the correspondence between some locations in the image.

![Translation, Scale and Rotation Invariance](/images/DailyPaper/02/16.png "Translation, Scale and Rotation Invariance")

### [14_Learning Deconvolution Network for Semantic Segmentation](http://arxiv.org/abs/1505.04366)

- This paper proposes the deconvolution network by adding the deconvolution network composed of deconvolution, ReLU, and unpooling layers at the top of VGG16. Modifying is because of the two apparent disadvantages of a Fully Connected Network(FCN). First, the network can handle only a single scale semantics within the image due to the fixed-size receptive field. Second, the detailed structures of an object are often lost or smoothed because the label map, input to the deconvolutional layer, is too coarse, and the deconvolution procedure is overly simple. FCN often judges large objects as inconsistent labels and misses the small objects and more details. The improved deconvolution network's structure is shown below. It is the Decoder-Encoder structure. More details about Unpooling and Deconvolution will not be discussed here since other blog parts have been discussed. The illustration of deconvolution and unpooling operations is shown below. This network performs very well in the following test performed in PASCAL VOC 2012.

![Structure of Deconvolution Network](/images/DailyPaper/02/20.png "Structure of Deconvolution Network")

![Illustration of Deconvolution and Unpooling Operations](/images/DailyPaper/02/21.png "Illustration of Deconvolution and Unpooling Operations")

- This network treated the segmentation as instance-wise segmentation rather than pixel-wise segmentation. In the following experiment part, the EDevonnet combined with CRF shows this method performs well. Moreover, in the training part, Batch Normalization, Two-stage Training, Aggregation Instance-wise Segmentation Maps, and Ensemble with FCN are also adopted to help the network train better in the small-samples dataset.

### [15_Dilated Residual Networks](http://ieeexplore.ieee.org/document/8099558/)

- **Dilated Residual Network (DRN)** is raised to cope with the problem of the loss of spatial acuity caused by reduced resolution. The other methods to keep the high resolution are *up-convolutions, skip connections, and other post-hoc measures.* This paper improves the resolution of the input image of ResNet by four times. The comparison between ResNet and DRN architecture is shown below.

![DRN](/images/DailyPaper/02/3.jpeg "ResNet and DRN")  

The details of Dilated Convolution are shown below.

![Dilated Convolution](/images/DailyPaper/02/1.gif "Dilated Convolution")

- However, **Degridding** will appear after using DRN. Gridding artifacts occur when a feature map has higher-frequency content than the sampling rate of the dilated convolution. An example of a gridding artifact is shown below. The structure of DRN is modified to handle this problem.

![A Gridding Artifact](/images/DailyPaper/02/5.jpeg "A Gridding Artifact")

- In DRN-B and DRN-C, the pooling layer is removed since the max pooling operation leads to high-amplitude, high-frequency activations. Besides, a 2-dilated residual block followed by a 1-dilated block is used as layer-7 and layer-8 of DRN-B and DRN-C. Moreover, DRN-C removes the residual connections. The details of DRN-A, DRN-B, and DRN-C are shown below. In the experiment section, DRN-C achieves the best performance. The visualization of DRN-C is also the best.

![Improved DRN](/images/DailyPaper/02/4.jpeg "Improved DRN Structure")

### [15_Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)

- This paper proposes **Deformable Convolution and Deformable RoI Pooling** to enhance the transformation modeling capability of CNNs. The resulting CNNs are called *Deformable Convolutional Networks or Deformable ConvNets*

- Deformable Convolution adds 2D offsets to the regular sampling locations in the standard convolution. The details of deformable convolution are shown below. The offset contributes to the irregular convolution shape. Therefore, the convolution kernel will fit with the shape and size of the objection to increasing the accuracy of the network.

![Deformable Convolution](/images/DailyPaper/02/9.jpeg "Deformable Convolution")

![3x3 Deformable Convolution](/images/DailyPaper/02/8.jpeg "3x3 Deformable Convolution")

- Deformable RoI Pooling adds an offset to each bin position in the regular bin partition of the previous RoI pooling. The details of deformable RoI Pooling are shown below. The position-sensitive RoI pooling adopts a specific positive-sensitive score map $x_{i,j}$ to obtain the actual offset $\Delta p_{i,j}$.

![3x3 Deformable RoI Pooling](/images/DailyPaper/02/6.jpeg "3x3 Deformable RoI Pooling")

![3x3 Deformable Position-Sensitive RoI Pooling](/images/DailyPaper/02/7.jpeg "3x3 Deformable Position-Sensitive RoI Pooling")

- In the following experiments, deformable convolution is added in different locations of ResNet. Besides, big and small objects are used for testing and validation. Moreover, this paper points out that **deformable convolution is a generalization of dilated convolution**. The performance improvement is not very significant, and the major improvement is based on the methods. (The performance improves in the Deformable ConvNets v2.)

- From my perspective, it is similar to applying the attention mechanism. The deformable convolution may combine with the attention mechanism to improve the model performance.

### [29_Deformable ConvNets v2_More Deformable, Better Results](https://arxiv.org/pdf/1811.11168.pdf)

- This paper improves the problem that occurred in Deformable ConvNet v1, which is the cover of irrelevant context. This paper raises three metrics: **Effective Reception Fields, Effective Sampling/Bin Locations, and Error-bounded Saliency Regions**. The Deformable ConvNet is incorporated into  *Fast R-CNN and Mask R-CNN* with different backbones in the experiment part.

- Moreover, three methods are tackled to improve the Deformable ConvNet v2. First, more deformable convolutions are used in the network. Second, a new weight is trained and applied to alleviate the effects of irrelevant. The new weight is represented as: $y(p)=\sum_{k=1}^K\omega_k x(p+p_k+\Delta p_k) \Delta m_k$. Third, mimicking the R-CNN feature to eliminate irrelevant information. In R-CNN, cropping and resizing the input images can effectively remove the irrelevant information in the background. The information on crop patches is more valuable than the information in the full image. The network training with the R-CNN feature mimicking is shown below. The performance of this new version network has improved by about 2% more than the original network.

![Network Training with R-CNN Feature Mimicking](/images/DailyPaper/02/12.jpeg "Network Training with R-CNN Feature Mimicking")

## Classical Models

### [29_LeNet_Gradient-Based Learning Applied to Document Recognition](https://ieeexplore.ieee.org/document/726791)

- **"Hello World"**

- This paper raises the LeNet-5 network to solve the problem of recognizing hand-written digits problem. The general architecture is the following:  
Input layer(32*32) -> C1(kernel size = 5x5, stride  = 1, depth = 6) -> S2(Kernel size = 2x2, stride = 2, Sigmoid) -> C3(kernel size = 5x5, stride = 1; To control the parameters and achieve the asymetric, the feature map is not generated by S2.) -> S4(Kernel size = 2x2, stride = 2) -> C5(Kernel size = 5x5, stride = 1) -> F6(Full connected layer) -> Output layer
The architecture of LeNet-5 is shown below. In Caffe, the architecture of LeNet used **ReLU** to replace Sigmoid and increase the kernels to meet the hardware conditions nowadays to improve accuracy and speed.

![The Architecture of LeNet](/images/DailyPaper/02/11.png "The Architecture of LeNet")

- The effect of this network is shown below. This gif is cited from [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)

![LeNet](/images/DailyPaper/02/10.gif "LeNet")

### [15_AlexNet_Imagenet-classification-with-deep-convolutional-neural-networks-Paper](https://dl.acm.org/doi/10.1145/3065386)

- **AlexNet** is the classical network. In this paper, **Dropout** is proposed to reduce overfitting in the fully-connected layers. The training and testing dataset are *ImageNet LSVRC-2010*. Also, **Rectified Linear Units (ReLUs)** are applied to improve the network's speed. AlexNet's structure is similar to LeNet. The overall architecture of AlexNet is shown below. It consists of 5 convolution layers and three fully connected layers. Because of the enormous calculation parameters, 2 GPUs train in parallel.

![Dilated Convolution](/images/DailyPaper/02/2.jpeg "AlexNet")

- What's more, **Local Response Normalization (LPN)** is used to increase the generalization ability. The equation of LPN is given by:
$b_{x,y}^i=a_{x,y}^i/(k=\alpha \sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(\alpha_{x,y}^i)^2)^\beta$
In addition, **Overlapping Pooling** is applied to decrease the top-1 and top-5 errors.

- To reduce the overfitting problem, two methods are applied. The first one is **Data Augmentation**. This method has two forms: *generate image translations and horizontal reflections, altering the RGB channels' intensities in training images.* The second method is the **Dropout** layer. This layer randomly ignores some neural to avoid the overfitting problem.

### [16_VGGNet_VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](http://arxiv.org/abs/1409.1556)

- This paper focuses on the effect of network depth and proposes **VGGNet**. The dataset used to test is *ImageNet Challenge 2014*. This paper also points out that the LRN is not helpful but increases memory consumption and computation time. The ConvNet configurations are shown below. There are two kinds of VGGNet, one is VGG16, and the other one is VGG19. The main difference between these two networks is the depth.

![VGGNet](/images/DailyPaper/02/13.jpeg "VGGNet")

- To increase the depth and keep the same receptive field, **3 x 3 convolutional kernels replace the 7 x 7 convolution kernel, and two 3 x 3 convolutional kernels replace the 5 x 5 convolution kernel**. The convolution diagram is shown below. After replacing, the number of parameters decreases from $7^2 C^2$ to $3^3 C^2$. Besides, it can be seen from the table that the 1 x 1 convolutional layer incorporates into the network. **The function of the 1 x 1 convolution layer is to increase the decision function's nonlinearity without affecting the convolutional layer's receptive fields.** The 1 x 1 conv. layer works like a kind of nonlinearity shift. In the following experiments, the deeper network performs well, which shows that **the depth of convolution net is very critical**. However, the deeper network will consume more computational resources and memory. The high consumer mostly is because of the fully connected layers. Some papers in the following indicate that the fully connected layer could be removed without affecting the performance of the network. The details will be introduced in the following blog. Also, VGGNet contributes a general pre-trained model for the future network, which is meaningful for the researcher.

![Small Convolutional Kernel](/images/DailyPaper/02/17.jpeg "3 x 3 Convolutions Replaces 5 x 5 and 7 x  7Convolutions")

### [16_ZFNet_Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901/)

- The analysis of ZFNet is shown in the first paper analysis of this blog.

### [17_Inceptionv1_GoogLeNet_Going deeper with convolutions](http://arxiv.org/abs/1409.4842)

- This paper proposes a deep convolutional neural network architecture named **Inception v1** and the **GoogLeNet**. At that time, the method to improve the network's performance is to increase depth and width. However, this will result in overfitting because of a large number of parameters, increased use of computational resources, and difficulty training the network because of the disappearing gradient. This paper proposes using sparse structures to replace the fully connected layers, the Inception module, to solve these problems. The structure of the Inception module is shown below. The 1 x 1 convolutions before the 3 x 3 and 5 x 5 convolutions decrease the computation complexity and deepen the layer to increase the ability. Moreover, this paper suggests that the Inception module should be used in the high layer of the network. One of the main benefits of this architecture is that it increases the number of units at each stage significantly without an uncontrolled blow-up in computational complexity. Another valuable aspect of this design is that it aligns with the intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from different scales simultaneously. The role of Inception is to replace the manual determination of the type of filter in the convolutional layer or whether to create convolutional and pooling layers, allowing the network to learn what parameters it exactly needs.

![Inception Module](/images/DailyPaper/02/22.png "Inception Module")

- Based on the Inception module, this paper builds the GoogLeNet. The structure of GooLeNet is shown below. In the following testing in ILSVRC 2014 classification challenge, the GoogLeNet performs very well.

![GoogLeNet Incarnation of the Inception Srchitecture](/images/DailyPaper/02/23.png "GoogLeNet Incarnation of the Inception Srchitecture")

### [17_Inceptionv2_Batch Normalization_Accelerating Deep Network Training](http://arxiv.org/abs/1502.03167)

- This paper mainly raises a revolution algorithm: **Batch Normalization** to solve the Internal Covariate Shift in training the deep neural network and improve **Inception V2**. What is the *Internal Covariate Shift*? 'The distribution of each layer's inputs changes during training as the parameters of the previous layers change. Internal Covariate Shift slows down the training by requiring lower learning rates and careful parameter initialization and makes it notoriously hard to train models with saturating nonlinearities.' In short, the parameters in the n layer will change in every epoch. The batch normalization idea is from the whitened images. The batch normalization algorithm is shown below. The difference between LRN and Batch Normalization is that LRN is a local normalization of the position of a point on a feature map, while BN is a normalization operation for a feature map.

![Batch Normalization](/images/DailyPaper/02/24.png "Batch Normalization")

- The training process of the batch-normalized network is shown below.

![Training a Batch Normalized Network](/images/DailyPaper/02/25.png "Training a Batch Normalized Network")

- Besides, the Inception V2 module use two 3 x 3 convolutions to replace 5 x 5 convolution. The structure of Inception V2 is shown below. The Inception V2 network also *increases the learning rate, removes Dropout and L2 regulation, accelerates the learning rate decay, removes Local Response Normalization(LRN), shuffled training samples, and reduces the photometric distortions* to fasten and improve the accuracy.

![Inception V2](/images/DailyPaper/02/26.png "Inception V2")

### [17_Inceptionv3_Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)

- This paper proposes an **Inception v3** module and modifies some layers in GoogLeNet. Moreover, this paper raises some general design principles: avoid representational bottlenecks, higher dimensional representations are easier to process locally within a network, spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power, and balance the width and depth of the network. 

- To improve the Inception module, this paper suggests some methods. The first one is to factorize a large convolution to small convolutions (e.g., 5 x 5 to be two 3 * 3) or irregular convolutions(Factorize n x n to be 1 x n and n x 1). The figure below illustrates factorizing 3 x 3 to 1 x 3 and 3 x 1.

![Factorization](/images/DailyPaper/02/27.png "Factorization")  
After adopting this unregulated factorization, the 3xInception, 5xInception, and 2xInception modules will be like the below.

![3xInception](/images/DailyPaper/02/28.png "3xInception")

![5xInception](/images/DailyPaper/02/29.png "5xInception")

![2xInception](/images/DailyPaper/02/30.png "2xInception")  
The second method is to utilize auxiliary classifiers such as batch normalization. The third method is to reduce efficient grid size. The modified Inception module is shown below.

![Inception Module that Reduces the Grid-size](/images/DailyPaper/02/32.png "Inception Module that Reduces the Grid-size")

- Therefore, Inception V2 is presented below. One thing that needs to be noticed; the input has changed from 224 to 299. If BN-auxiliary is combined with Inception v2, then Inception V3 is generated.
  
![New Inception V2](/images/DailyPaper/02/31.png "New Inception V2")  

### [14__Xception_ Deep Learning with Depthwise Separable Convolutions](http://arxiv.org/abs/1610.02357)

- This paper proposes **Depthwise Separable Convolution** to replace the Inception to improve the performance. The Xception architecture is a linear stack of depthwise separable convolution layers with residual connections. The extreme version of the Inception module of depth separable convolution is shown below. This kind of Inception module uses $k_1$ different convolutions on each channel separately to decrease the parameter to the $\frac{1}{k_1}$ of the regular convolution to speed up the convolution significantly.

![An Extreme Version of Inception Module](/images/DailyPaper/02/37.png "An Extreme Version of Inception Module")

- The overall Xception architecture is shown below.

![Xception](/images/DailyPaper/02/36.png "Xception") 

### [17_Inceptionv4, Inception-ResNet and the impact of residual connections on Learning](http://arxiv.org/abs/1602.07261)

- This paper raises **Inception v4** which combines Inception and ResNet. Since there are many diagrams, I only show the Schema of the Inception-v4 Network and the Schema for Inception-ResNet-v1 and Inception-ResNet-v2 Networks here. The diagram can be seen in the original paper.

![Schema of Inception-v4 Network](/images/DailyPaper/02/33.png "Schema of Inception-v4 Network")
![Schema for Inception-ResNet-v1 and Inception-ResNet-v2 Networks](/images/DailyPaper/02/34.png "Schema for Inception-ResNet-v1 and Inception-ResNet-v2 Networks")  
The combination network replaces the filter concatenation stage of the Inception architecture with residual connections and adds more Inception and better connections. Besides, it is helpful to scale the residuals to avoid the 0 that occurs when the number of filters exceeds 1000, and the scaling block is shown below. Moreover, ResNet is not used to improve depth and accuracy but to speed up. It is the more extensive network size that improves the network accuracy.

![Scaling Block](/images/DailyPaper/02/35.png "Scaling Block")  

### [111_Highway Networks](http://arxiv.org/abs/1505.00387)

- This paper is a brief version of the next one.

### [111_Details of Highway Network_Training Very Deep Networks](http://arxiv.org/abs/1507.06228)

- These two papers propose **Highway Network** to solve the problem of hard training deep neural network and the degradation problem. Inspired by the gating mechanism in LSTM, highway network utilize the **Stochastic Gradient Descent(SGD)** to simplify the training of deep neural network. The output y in ith layer will be **y = H(x, $W_H$)T(x, $W_T$) + xC(x, $W_C$)**. If C is set as 1-T, then y = x, if T(x, $W_T$) = 0; H(x, $W_H$), if T(x, $W_T$) = 1. In this equation, T(x) = $\sigma (W_T^T x+b_T)$. So a portion of the input is processed and a portion is passed directly through. However, in the following experiment as shown below, the Highway network could alleviate the problem of the larger error in the deeper layer but there still exists some problem in the deep layer. This problem is handled in ResNet which will be introduced next.

![Comparisonn of Optimization of Plain Network and Highway Network](/images/DailyPaper/02/58.jpeg "Comparisonn of Optimization of Plain Network and Highway Network") 

### [18_ResNet_Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385)

- This paper is an excellent paper on the evolution process of computer vision. This paper proposes a **Residual Learning Framework** to ease the training pressure of training deep networks and solve the degradation problem. The degradation problem is shown below. It could be seen that the performance of 56-layer is worse than 20-layer.

![Degradation](/images/DailyPaper/02/60.jpeg "Degradation")  
So how can we solve this problem? The best method is to keep the x term. The residual Block is shown below. Then the output H(x) will be F(x) + x in ith layer. If F(x) equals to 0, then this will be identity mapping which provides the shortcut. If F(x) does not equal to 0, then F(x) will be relatively smaller than x which make the gradient more sentitive.

![Residual Block](/images/DailyPaper/02/61.jpeg "Residual Block")  
Moreover, a bottleneck building block is builded as below. This block will be often mentioned in the future. 1 x 1 layers are used to reduce and increase dimension.

![Bottleneck Residual Block](/images/DailyPaper/02/63.jpeg "Bottleneck Residual Block")  

- Therefore, if we adopts the residual block in the network, then it will be shown below.

![VGG-19, 34-layer Plain, 34-layer Residual](/images/DailyPaper/02/62.jpeg "VGG-19, 34-layer Plain, 34-layer Residual")  
Then we have the obvious improvement compared with the plain network as below. ResNet is undoubtedly a significant invention in computer vision.

![Improved Result](/images/DailyPaper/02/59.jpeg "Improve Result") 

### [18_ResNeXt_Aggregated Residual Transformations for Deep Neural Networks](http://ieeexplore.ieee.org/document/8100117/)

- This paper proposes **ResNeXt** which combines ResNet and Inception. In ResNeXt, the Group Convolution is adopted. Cardinality which is the size of the set of transformations is used to control the dimension. The idea of cardinality likes divide and conquer. The ResNeXt strcuture is shown below. Also, the equivalent building blocks of ResNeXt is shown in the following. The equivalent blocks use the split-transform-merge which splits x into D features to perform linear transformer and finally merges. In equation form, it will be $\sum_{i=1}^D w_i x_i$. Inception is modified as F(x) = $\sum_{i=1}^C T_i(x)$ which merges cardinality. Moreover, this equation will be y = x + $\sum_{i=1}^C T_i(x)$ with residual network.

![ResNeXt](/images/DailyPaper/02/64.jpeg "ResNeXt")

![Equivalent Building Block](/images/DailyPaper/02/65.jpeg "Equivalent Building Block")
 
### [19_DenseNet_Densely Connected Convolutional Networks](https://ieeexplore.ieee.org/document/8099726/)

- This paper proposes **DenseNet** which connects each layer to every other layer in a feed-forward fashion. This structure will reduce the parameter and the passing process to efficiently improve the results. For a L layers networks, DenseNet will have $\frac{L(L+1)}{2}$ direct connections. The illustration of 5-layer dense block is shown below. Also, the architecture of DenseNet is shown in the following. In equation form, the layer l will be $x_l = H_l([x_0, x_1, ..., x_{l-1}])$, $[x_0, x_1, ..., x_{l-1}]$ is the concatenation of the feature-maps produced in layer 0 to l-1. This kind of connection help train the network with the upgrading of gradient back-propagation. Also, the parameter for DenseNet is smaller with the concating features. From my perspective, DenseNet likes a special case of ResNet.

![Illustration of 5-layer Dense Block](/images/DailyPaper/02/66.jpeg "Illustration of 5-layer Dense Block")

![The Architecture of DenseNet](/images/DailyPaper/02/67.jpeg "The Architecture of DenseNet")

### [19_HRNet_High-Resolution Representations for Labeling Pixels and Regions](http://arxiv.org/abs/1904.04514)
### [19_HRNet_Deep High-Resolution Representation Learning](http://arxiv.org/abs/1908.07919)

- These two papers show **HRNet v1, HRNet v2 and HRNet v2p**. There are some methods to computing high-resolution: Recover high-resolution representations from low-resolution representation(e.g., Hourglass, SegNet, DeconvNet, U-Net and encoder-decoder), Cascade pyramid(RefineNet), Maintain the high-resolution representation throigh high-resolution convolutions and strengthen the representations with parallel low-resolution convolutions, Dilated convolution to increase the receptive field. The HRNet maintains high-resolution representations by connecting high-to-low resolution convolutions in parallel and repeatedly conducting multi-scale fusions across parallel convolutions. The sample structure of HRNet is shown below.

![The Architecture of HRNet](/images/DailyPaper/02/70.jpeg "The Architecture of HRNet")

- The multi-resolution blocks are also shown in paper. The differences between multi-resolution convolution and the normal convolution lie in two-fold.(1) In a multi-resolution convolution each subset of channels is over a different res-
olution. (2) The connection between input channels and output channels needs to handle. The resolution decrease is implemented by using several 2-strided 3 × 3 convolutions. The resolution increase is simply implemented by bilinear (nearest neighbor) upsampling.

![Multi-resolution Block](/images/DailyPaper/02/68.jpeg "Multi-resolution Block")

- The mainly difference among HRNet v1, v2 and v2p is the head of network as shown below. (a) is for HRNet v1, (b) is for HRNet v2 and (c) is for HRNet v2p. (a) only output the representation from the high-resolution convolution stream, (b) concatenates the (upsampled) representations that are from all the resolutions used for semantic segmentation, (c) forms a feature pyramid from the representation by HRNetV2 and used for the objection detection.

![The Head of HRNetv1, v2 and v2p](/images/DailyPaper/02/69.jpeg "The Head of HRNetv1, v2 and v2p")

### [19_SENet_Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)

- This paper proposes the **Squeeze-and-Excitation(SE) block** which is shown below. In squeeze part which can be considered as Global Information Embedding, this paper used global average pooling to achieve. In equation, $z_c=F_{sq}(u_c)=\frac{1}{H W}\sum_{i=1}^H \sum_{j=1}^W u_c(i,j)$. This could exploiting channel dependencies which utilizes the correspondings of channels. In excitation part, two fully connected layers are applied to fully capture channel-wise dependence. In equation, s = $F_{ex}(z, W)=\sigma (g(z, W)) = \sigma (W_2 \delta(W_1, z))$ where $\delta$ is the ReLU function and $\sigma$ is the softmax function. After excitation, the real scale of channel could be found. SENet strengthens the features of important channels and weakens the features of non-important channels.

![A Squeeze-and-Excitation Block](/images/DailyPaper/02/72.jpeg "A Squeeze-and-Excitation Block")

- The comparison between Inception and SE-Inception/ Residual and SE-ResNet is shown below. The structure of ResNet-50, SE-ResNet-50 and SE-ResNeXt-50 is also shown in the following.

![The Comparison between Inception and SE-Inception/ Residual and SE-ResNet](/images/DailyPaper/02/71.jpeg "The Comparison between Inception and SE-Inception/ Residual and SE-ResNet")

![The Structure of ResNet-50, SE-ResNet-50 and SE-ResNeXt-50](/images/DailyPaper/02/73.jpeg "The Structure of ResNet-50, SE-ResNet-50 and SE-ResNeXt-50")

- I think SENet could combine with channel-attention mechanism to do some improments. I will furture explore.

### [110_CSPNET A NEW BACKBONE THAT CAN ENHANCE LEARNING](http://arxiv.org/abs/1911.11929)

- This paper proposes **Cross Stage Partial Network(CSPNet)** which respects the variability of the gradient by integrating feature maps from the beginning and the end of a network stage. The main purpose of designing CSPNet is to enable this architecture to achieve a richer gradient combination while reducing the amount of computation. This aim is achieved by partitioning feature map of the base layer into two parts and then merging them through a proposed cross-stage hierarchy. CSPNet solves three problems: Strengthen learning ability of a light CNN, Remove computational bottlenecks, Reduce memory costs. So what aspects does the CSPNet improve? CSPNet separates feature map of the base layer into two part, one part will go through a dense block and a transition layer; the other one part is then combined with transmitted feature map to the next stage.

- CSPDenseNet which is consists of a partial dense block and a partial transition layer is proposed as shown below. It could be seen from the equation that CSPDenseNet truncates gradient flow from receiving the repeated gradient information.

![DenseNet and CSPDenseNet](/images/DailyPaper/02/74.jpeg "DenseNet and CSPDenseNe")  
The partial dense block is designed for increasing gradient path, balancing cimputation of each layer, and reducing memory traffic. The partial transition layer is designed to maxizize the difference of gradient combination. The feature fusion strategies are shown below.

![Different Feature Fusion Strategies](/images/DailyPaper/02/77.jpeg "Different Feature Fusion Strategies")  
The exact fusion model is used to look exactly to predict, aggregate feature pyramid and balance computation. The different feature pyramid fusion strategies are shown below.

![Different Feature Pyramid Fusion Strategies](/images/DailyPaper/02/75.jpeg "Different Feature Pyramid Fusion Strategies")

### [110_EfficientNet Rethinking Model Scaling for Convolutional Neural Networks](http://arxiv.org/abs/1905.11946)

- This paper proposes the **EfficientNet** which could uniformly scales all dimensions of depth/width/resolution using compound coefficient. The model scaling could achieve the good grid but the scaling could result in the tedious parameter adjusting. The following figure shows the model scaling cases. It is significant to find a balance point among depth, width and resolution which is a every-one-love trade-off problem. That's what EfficientNet works for. EfficientNet use the compound scaling method to enlarge the neural network with increase the width height depth by the ratio simultaneously. To simplify the problem, it will be an optimization problem:  
$max_{d,w,r}$ Accuracy(N(d,w,r)) s.t. N(d,w,r) = $\bigodot$ $F_i^{dL_i}(X_{<rH_i, rW_i, wC_i>})$, Memory(N) $\le$ target_memory, FLOP(N) $\le$ target_flops.

![Model Scaling](/images/DailyPaper/02/76.jpeg "Model Scaling")

- Depth is for the richer and more complex features. Width is for the more fine-grained features. Resolution is for the more fine-grained patterns. Intuitively, for higher resolution images, we should increase network depth, such that the larger receptive fields can help capture similar features that include more pixels in bigger images. Correspondingly, we should also increase network width when resolution is higher, in order to capture more fine-grained patterns with more pixels in high resolution images. Therefore, in coupound scaling method, a compound coefficient $\phi$ is used to uniformly scales network width, depth and resolution:  
depth: d = $\alpha^\phi$, width: w = $\beta^\phi$, resolution: r = $\gamma^\phi$. s.t. $\alpha\beta^2\gamma^2\approx 2$, $\alpha \ge 1, \beta \ge 2, \gamma \ge 1$.  
Also, EfficientNet-B0 is proposed to improve the accuracy of searching and decrease FLOPs.

### [111_PReLu_Delving Deep into Rectifiers_Surpassing Human-Level Performance on ImageNet Classification](http://ieeexplore.ieee.org/document/7410480/)

- This paper proposes **Parametric Rectified Linear Unit(PReLU)**, which is the upgraded version of ReLu and LReLu. The definition of PReLU is **f($y_i$) = $y_i$, if $y_i$ > 0; $a_i$$y_i$, if $y_i$ $\le$ 0 = max(0, $y_i$) + $a_i$min(0, $y_i$)**. The comparison between ReLU and PReLU is shown below. If $a_i$ is very small and fixed, PReLU could be seen as LReLU. The upgrade of $a_i$ could be achieved by back-propagation, whose details will not be shown here.

![ReLU and PReLU](/images/DailyPaper/02/18.jpeg "ReLU and PReLU")

- Only a tiny number of parameters are added in PReLU, which means that the computational effort of the network and the risk of over-fitting are increased only a little. In the following experiments, ReLU is replaced by PReLU. The improved network received excellent results in ImageNet 2012. Besides, a new initialization method based on PReLU is developed, which converges much faster than the Xavier method.

## Image Segmentation

### [112_FCN_Fully Convolutional Networks for Semantic Segmentation](http://arxiv.org/abs/1411.4038)

### [112_FPN_Feature Pyramid Networks for Object Detection](http://arxiv.org/abs/1612.03144)

### [112_U-Net_Convolutional Networks for Biomedical](http://arxiv.org/abs/1505.04597)

## GAN

### [112_GAN_Generative Adversarial Nets](http://arxiv.org/abs/1406.2661)

- This paper proposes a framework for $estimating generative models via an adversarial process$. This framework needs two models: **a generative model(G) and a discriminative model(D)**. The relation between these two models like the police and thief. G will be trained to maximize the probability of D making a mistake while D will estimate the probability that a sample came from the training data rather than G. That is why the network is called **Generative Adversarial Nets(GAN)**. The training process of generative and discriminative is shown below. After k cycles of updating the discriminator, the generator's parameters are updated once using a lower learning rate. The generator is trained to minimize the gap between the generated and actual samples, which is equivalent to making the discriminator discriminate errors as much as possible.
Having a good discriminator first makes it possible to distinguish the actual samples from the generated samples well before the generator can be updated more accurately. A sufficient condition for a GAN global minimum is when the probability distribution of the generator and the probability distribution of the actual values are the same Loop.

![Training Process](/images/DailyPaper/02/19.jpeg "Training Process of Generative and Discriminative")

- This paper also proposes one theorem and two propositions: Theorem 1. The global minimum of the virtual training criterion C(G) is achieved if and only if $p_g$ = $p_{data}$. At that point, C(G) achieves the value − log 4.;  
Proposition 1. For G fixed, the optimal discriminator D is *$D_G^{\*} (x)= \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$*;  
Proposition 2. If G and D have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given G, and $p_g$ is updated so as to improve the criterion: *$E_{x~p_{data}}[log D^{\*}_G(x)] + E_{x~p_g}[log(1 - D^{\*}_G(x))]$, then $p_g$ converges to $p_{data}$*.
The overall algorithm of training is shown below. The loss function could be inituitively distinguished: **$min_G max_D$V(D, G) = $E_{x~p_{data}(x)}[logD(x)] + E_{z~p_z(z)}[log(1-D(G(Z)))]$**. Moreover, from the paper, the disadvantages are primarily that there is no explicit representation of $p_g$(x), and that D
must be synchronized well with G during training (in particular, G must not be trained too much without updating D, in order to avoid “the Helvetica scenario” in which G collapses too many values of z to the same value of x to have enough diversity to model pdata), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov
chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model.

![Algorithm for Training GAN](/images/DailyPaper/02/57.jpeg "Algorithm for Training GAN")

- From my perspective, GAN is very instresting. If let me use a Chinese to describe this, I would like to express as "以子之矛攻子之盾".

## Object Detection

### [113_Selective Search for Object Recognition](http://link.springer.com/10.1007/s11263-013-0620-5)

- This paper proposes the Selective Search algorithm, which performs segmentation and then search exaustively. A selective search algorithm is subject to the following design considerations: Capture All Scale, Diversification and Fast to Compute. The selective search algorithm contains two parts: **Hierarchical Grouping Algorithm and Diversification Strategies**.The mainly idea of this algorithm is to use the greedy algorithm to calculate the similarity, then merge the most similar areas, and then calculate the similarity between combination area and neighbor area, finally continue this process to merge all figures as one area.  The algorithm of Hierarchical Grouping Algorithm is shown below. 

![Hierarchical Grouping Algorithm](/images/DailyPaper/02/51.png "Hierarchical Grouping Algorithm")  
For the Diversification Strategies, this paper uses different color space , adopt different similarity which is the sum of $s_{colour}, s_{texture}, s_{size}, s_{fill}$. The details of these equation are shown in the paper. After using selective algorithm, the model is fast to compute than other methods at that time. But the selective algorithm still has many aspects to improve, such as the greedy algorithm which could be replaced by HOG + SVM.

### [113_OverFeat_Integrated Recognition, Localization and Detection using Convolutional Networks](http://arxiv.org/abs/1312.6229)

- This paper proposes **OverFeat** which is a CNN network does classification, localization and detection at the same time. Also, the OverFeat integrates Multi-scale, Sliding Window, and Offset Pooling to improve the network. 

- The network structure of OverFeat is similar with AlexNet. The first five convolution layers is regarded as feature extraction layers and the final three fully connected layers are replaced by fully convolution layers as classification layers. In the classification part, the multiscale input with horizontal flip is also adopted in OverFeat. Moreover, offset pooling is used in the fifth layer to replace max pooling to increase the detection grid of images. The details of offset pooling is shown below.

![Offset Pooling](/images/DailyPaper/02/54.jpeg "Offset Pooling")  

- To better fit the convolution network, Sliding Window method is used in this network. It is efficient in the fully convolution architecture. Sliding Window is applied in the final three convolution layer. Besides, this method is suitable for multiscale inputs. The working process of sliding window is shown below.

![Sliding Window](/images/DailyPaper/02/55.jpeg "Sliding Window")

- After classification, localization is performed. Firstly, classifier and regressor networks are simulaneously functioned to generate object bounding box. Secondly, train the regressor. Finally, merge the bounding box to combine predictions. The illustration of the localization/detection is shown below. To perform the objection detection, the background type should be added in training.

![Localization/Detection Pipeline](/images/DailyPaper/02/53.jpeg "Localization/Detection Pipeline")

### [114_R-CNN_Rich feature hierarchies for accurate object detection and semantic segmentation](http://arxiv.org/abs/1311.2524)

### [114_SPPNet_Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](http://arxiv.org/abs/1406.4729)

### [114_Fast R-CNN](http://arxiv.org/abs/1504.08083)

### [114_Faster R-CNN_Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)

### [114_Mask R-CNN](http://arxiv.org/abs/1703.06870) (This paper focuses on Image Segmentation)

### [115_R-FCN_Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409)

### [115_SSD_Single Shot MultiBox Detector](http://arxiv.org/abs/1512.02325)

### [115_YOLOv1_You Only Look Once_Unified Real-Time Object Detection](http://arxiv.org/abs/1506.02640)

### [116_YOLOv2_YOLO9000_Better, Faster, Stronger](http://arxiv.org/abs/1612.08242)

### [116_YOLOv3_An Incremental Improvement](http://arxiv.org/abs/1804.02767)

### [116_YOLOv4_Optimal Speed and Accuracy of Object Detection](http://arxiv.org/abs/2004.10934)

—————— **The following three papers show some applications of YOLO series** ——————

### [117_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object Detection on Drone-Captured Scenarios](https://ieeexplore.ieee.org/document/9607487/)

- This paper proposes **TPH-YOLOv5** which use Transformer Prediction Heads(TPH) to replace the original prediction heads. Also, Convolutional Block Attention Model(CBAM) is adopted to find the attention region in the dense region. This model is applied in the drone to capture the photos and recognize. The overall architecture of TPH-YOLO v5 is shown below. 

![TPH-YOLO v5](/images/DailyPaper/02/47.jpeg "TPH-YOLO v5")  
Besides, the architecture of TPH and CBAM are shown below. From my perspective, this module could be the determined cause for the good performance of this model since the scenaries captured by drone are very small. Attention mechanism performs well in assisting recognizing small objects.

![TPH](/images/DailyPaper/02/45.jpeg "TPH")
![CBAM](/images/DailyPaper/02/46.jpeg "CBAM")

### [117_YOLO v5_Improved YOLOv5 network for real-time multi-scale traffic sign detection](https://link.springer.com/10.1007/s00521-022-08077-5)

- This paper replaces FPN with **AF-FPN which utilizes the Adaptice Attention Module (AAM) and Feature Enhancement Module(FEM)**. Also, this structrue aimed to decrease FLOP(Floating-Point Operations per Second). The YOLO V5 architecture proposed in this paper uses the AAM and FEM as the neck which is shown below.

![YOLO V5 Architecture](/images/DailyPaper/02/48.png "YOLO V5 Architecture")  

- The architecture of the AF-FPN is shown below.

![AF-FPN](/images/DailyPaper/02/51.png "AF-FPN")  
The AAM aims to reduce the loss of context information in the high-level feature map due to the reduced feature channels. The AAM uses multiscale innormation to improve the ability. The structure of AAM is shown below.

![AAM](/images/DailyPaper/02/49.png "AAM")  
The FPN is adopted to enhance the representation of feature pyramids and accelerates the inference speed while achieving state-of-the-art performance. Dilated convolution is used in FPN to increase the reception field. The structure of FPN is shown below.

![FPN](/images/DailyPaper/02/50.png "FPN")  

- The Data argumentation methods in this paper are search space and search algorithm.
The search space contains 5 sub-strategies, each of which consists of two simple image enhancement operations applied in sequence. Moreover, the searching algorithm is considered as a discrete optimization problem. These data argumentation are used to suit the real sceneries. Besides, C(Completed)IoU is used to replace GIoU as the metrics. In the following experiment part, the network received the SOTA results.

### [117_YOLOV_Making Still Image Object Detectors Great at Video Object Detection](http://arxiv.org/abs/2208.09686)

- This paper improved the YOLOX by adding **Feature Selection Module(FSM) and Feature Aggregation Module(FAM)** to solve the Video Object Detection problem. The overall of this architecture is shown below.

![Overall Structure of YOLOV](/images/DailyPaper/02/44.jpeg "Overall Structure of YOLOV")

—————— **The following three papers show the evolution process of YOLO series** ——————

### [118_YOLOX_Exceeding YOLO Series in 2021](http://arxiv.org/abs/2107.08430)

- 2021

### [118_PP-YOLOE_An evolved version of YOLO](http://arxiv.org/abs/2203.16250)

- 2022

### [118_YOLO-Z_Improving small object detection in YOLOv5 for autonomous vehicles](http://arxiv.org/abs/2112.11798)

- 2023

## Light Networks

### [119_MobileNets_Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)

### [119_MobileNetV2_Inverted Residuals and Linear Bottlenecks](https://ieeexplore.ieee.org/document/8578572/)

### [119_MobileNetv3_Searching for MobileNetV3](https://ieeexplore.ieee.org/document/9008835/)

### [120_ShuffleNet_An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/abs/1707.01083)

### [120_ShuffleNet V2_Practical Guidelines for Efficient CNN Architecture Design](https://link.springer.com/10.1007/978-3-030-01264-9_8)

### [120_ShuffleNet v3_An efficient solution for semantic segmentation_ShuffleNet v2 with atrous seperable convolutions](http://link.springer.com/10.1007/978-3-030-20205-7_4)

## Graph Neural Networks

### [121_A Comprehensive Survey on Graph Neural Networks](http://arxiv.org/abs/1901.00596)
### [121_Deep Learning on Graphs_A Survey](http://arxiv.org/abs/1812.04202)
### [121_Graph neural networks_A review of methods and applications](https://arxiv.org/abs/1812.08434)

- These three papers are used to know more about **Graph Neural Networks(GNN)**. As we have discussed so many CNN networks, why we uses CNN in the graph netork? Because the image and frame in video sequence have local translational invariance and the graph does not have this kind of invariance, that is Euclidean space versus non-Euclidean space. The comparison between image and graph is shown below. 

![The Comparison between Image and Graph](/images/DailyPaper/02/38.jpeg "The Comparison between Image and Graph")

- To handle the graph signals, the first step is to use measured points to be the vertexes. Second, the graph wll be connected by edges and vertexes. Third, measure the signal of graph to perform the local average or average with weights. Therefore, if we want to perform convolutional network in the graph, some people suggests **Graph Convolutional Network(GCN)**, this kinds of convolution perform in the frequency domain since it is hard to perform in the space domain. Therefore, GCN-1(Deep Locally Connected Networks), GCN-2(ChbeyNet), and GCN-3 were proposed. The simple example of the spatial convolution operation is shown below. The input and output will all be graph structure.

![An Example of the Spatial Convolution Operation](/images/DailyPaper/02/39.jpeg "An Example of the Spatial Convolution Operation")
![An Example of the Spatial Convolution Operation](/images/DailyPaper/02/40.jpeg "An Example of the Spatial Convolution Operation")

### [121_Temporal Convolutional Networks for Action Segmentation and Detection](https://arxiv.org/abs/1611.05267)
### [121_Temporal Convolutional Networks_A Unified Approach to Action Segmentation](https://arxiv.org/abs/1608.08242)
### [213_An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)

- These two paper are used to know more about **Temporal Convolution Networks(TCN)**. Why TCN? Firstly, CNN is hard to capture the tenporal , and the traditional convolution has the problem of leaking or missing the details. TCN uses **causal convolution and dilated** to solve these problems.

- The causal convolution pad the zeros in the original kernel and shift when performing the convolution. The diagram of causal convolution is shown below. Generally speaking, TCN is similar with FCN combined with causal convolutions at the begining.

![Causal Convolution](/images/DailyPaper/02/43.jpeg "Causal Convolution")

- Since the causal convolution is adopted, dilated is needed to cover the input length to reduce the neural network. The details of dilated convolution has been mentioned in the previous blog part. The dilated TCN model is shown below.
  
![Dilated TCN Model](/images/DailyPaper/02/42.jpeg "Dilated TCN Model")

- If the residual block is also added, then a improved TCN could be represented as below. The residual block is to help train the deeper network and extract features.
  
![TCN](/images/DailyPaper/02/41.jpeg "TCN")

## Vision Transformer

### [122_ViT v1_Attention Is All You Need](http://arxiv.org/abs/1706.03762)

### [122_ViT v2_AN IMAGE IS WORTH 16X16 WORDS_TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/abs/2010.11929)

### [122_Swin Transformer V1_Hierarchical Vision Transformer using Shifted Windows](https://ieeexplore.ieee.org/document/9710580/)

### [122_Swin Transformer V2_Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)

### [122_ViTAE_Vision Transformer Advanced by Exploring Intrinsic Inductive Bias](http://arxiv.org/abs/2106.03348)

- This paper proposes a new **Vision Transformer Advanced by Exploring intrinsic IB from convolutions(ViTAE)** which combines CNN and ViT by using CNN in the shallow layer and using ViT in the deep layer. The structure of ViTAE is shown below. RC means Reduction Cell and NC means Normal Cell. In RC, a Pyramid Reduction Module(PRM) which consisted of different dilated convolution with different dilated rate is added compared with Transformer block of ViT. Also, the PRM's output feature is enlength and input the Attention module. In NC, a convolution branch is added in calculating attention. ViTAE is a new step in the transformer.

![Structure of ViTAE](/images/DailyPaper/02/56.jpeg "Structure of ViTAE")

### [122_ViTAEv2_Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond](http://arxiv.org/abs/2202.10108)

- This paper raises **ViTAE v2** which has an enormous parameters(644M) by scaling up. In many experiment, ViTAE receives SOTA results. In the ablation experiment part, some mechanisms' effects are discussed. In the future, the ViTAE will be improved by decreaing the training consume and designing the better model structure.





