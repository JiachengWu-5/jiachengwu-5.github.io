---
layout: post
title: Daily Paper (2023.1.3 - 2023.1.22)
categories: [Daily Paper]
description: Classical CV papers
keywords: Computer Vision, Visualizing, Convolutional Network, DRN, Deformable ConvNets v1 & v2, LeNet, AlexNet, VGGNet, ZFNet, Inception v1 & v2 & v3 & v4, GooLeNet, Xception, Highway Networks, ResNet, ResNeXt, DenseNet, HRNet, SENet, CSPNet, EfficientNet, PReLU, FCN, FPN, U-Net, GAN, Selective Search, OverFeat, R-CNN, SPPNet, Fast R-CNN, Faster R-CNN, Mask R-CNN, R-FCN, SSD, YOLO v1 & v2 & v3 & v4 & v5, YOLOX, YOLOZ, PP-YOLOE, MobileNet v1 & v2 & v3, ShuffleNet v1 & v2, GAN, TCN, ViT, Transformer, Swin Transformer v1 & v2, ViTAE
---

This blog shows some classical CV papers and the evolution process of CV models.

## Convolution

### [14_Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)

- This paper introduces a novel way to visualize the Convolutional Network. The convolutional network is like a black box. It is meaningful to find out how it functions inside. Three essential methods are used: **Unpooling, Rectification(ReLU), and Filtering** to realize the purpose.

- Deconvnet performs the work that maps features to pixels reverse to convnet maps pixels to features. Unpooling uses switches variables that record the max locations within each pooling region to place the reconstructions from the layer above into appropriate locations. Rectification uses the *relu* function to reconstruct. Filtering flips each filter vertically and horizontally. The details of the deconvnet are shown below.

![Deconvnet](/images/DailyPaper/02/14.png "Deconvnet")

- After deconvnet and visualizing the layers, the visualization of features of a network performed on ImageNet is shown below. Even though CNN could not be explained thoroughly, it can be observed that the features that CNN learned are divided into layers. The lower layer learns abstract features, while the high layer learns specific features. This kind of learning characteristic is similar to the human visual system. Generally speaking, the deeper neural network will learn more generalized features. From my perspective, this work is meaningful to the further CNN works, and future CNN works also provide examples to verify the kernel of this paper. For instance, the deeper *VGGNet*.

![Visualization of Features](/images/DailyPaper/02/15.png "Visualization of Features")

- Besides, the lower layer converges after a few epochs, and the higher layer needs more epochs to converge. Another essential point in this paper is that the convolution performs good invariance in the vertical translation and scale and performs bad invariance in the rotation. The graph of this experiment is shown below. The graph provides the idea of data argumentation that rotated images could be used as the new data set to train the network. Moreover, the following experiment shows that the deep neural network can implicitly establish the correspondence between some locations in the image.

![Translation, Scale and Rotation Invariance](/images/DailyPaper/02/16.png "Translation, Scale and Rotation Invariance")

### [14_Learning Deconvolution Network for Semantic Segmentation](http://arxiv.org/abs/1505.04366)

- This paper proposes the deconvolution network by adding the deconvolution network composed of deconvolution, ReLU, and unpooling layers at the top of VGG16. Modifying is because of the two apparent disadvantages of a Fully Connected Network(FCN). First, the network can handle only a single scale semantics within the image due to the fixed-size receptive field. Second, the detailed structures of an object are often lost or smoothed because the label map, input to the deconvolutional layer, is too coarse, and the deconvolution procedure is overly simple. FCN often judges large objects as inconsistent labels and misses the small objects and more details. The improved deconvolution network's structure is shown below. It is the Decoder-Encoder structure. More details about Unpooling and Deconvolution will not be discussed here since other blog parts have been discussed. The illustration of deconvolution and unpooling operations is shown below. This network performs very well in the following test performed in PASCAL VOC 2012.

![Structure of Deconvolution Network](/images/DailyPaper/02/20.png "Structure of Deconvolution Network")

![Illustration of Deconvolution and Unpooling Operations](/images/DailyPaper/02/21.png "Illustration of Deconvolution and Unpooling Operations")

- This network treated the segmentation as instance-wise segmentation rather than pixel-wise segmentation. In the following experiment part, the EDevonnet combined with CRF shows this method performs well. Moreover, in the training part, Batch Normalization, Two-stage Training, Aggregation Instance-wise Segmentation Maps, and Ensemble with FCN are also adopted to help the network train better in the small-samples dataset.

### [15_Dilated Residual Networks](http://ieeexplore.ieee.org/document/8099558/)

- **Dilated Residual Network (DRN)** is raised to cope with the problem of the loss of spatial acuity caused by reduced resolution. The other methods to keep the high resolution are *up-convolutions, skip connections, and other post-hoc measures.* This paper improves the resolution of the input image of ResNet by four times. The comparison between ResNet and DRN architecture is shown below.

![DRN](/images/DailyPaper/02/3.jpeg "ResNet and DRN")  

The details of Dilated Convolution are shown below.

![Dilated Convolution](/images/DailyPaper/02/1.gif "Dilated Convolution")

- However, **Degridding** will appear after using DRN. Gridding artifacts occur when a feature map has higher-frequency content than the sampling rate of the dilated convolution. An example of a gridding artifact is shown below. The structure of DRN is modified to handle this problem.

![A Gridding Artifact](/images/DailyPaper/02/5.jpeg "A Gridding Artifact")

- In DRN-B and DRN-C, the pooling layer is removed since the max pooling operation leads to high-amplitude, high-frequency activations. Besides, a 2-dilated residual block followed by a 1-dilated block is used as layer-7 and layer-8 of DRN-B and DRN-C. Moreover, DRN-C removes the residual connections. The details of DRN-A, DRN-B, and DRN-C are shown below. In the experiment section, DRN-C achieves the best performance. The visualization of DRN-C is also the best.

![Improved DRN](/images/DailyPaper/02/4.jpeg "Improved DRN Structure")

### [15_Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)

- This paper proposes **Deformable Convolution and Deformable RoI Pooling** to enhance the transformation modeling capability of CNNs. The resulting CNNs are called *Deformable Convolutional Networks or Deformable ConvNets*

- Deformable Convolution adds 2D offsets to the regular sampling locations in the standard convolution. The details of deformable convolution are shown below. The offset contributes to the irregular convolution shape. Therefore, the convolution kernel will fit with the shape and size of the objection to increasing the network's accuracy.

![Deformable Convolution](/images/DailyPaper/02/9.jpeg "Deformable Convolution")

![3x3 Deformable Convolution](/images/DailyPaper/02/8.jpeg "3x3 Deformable Convolution")

- Deformable RoI Pooling adds an offset to each bin position in the regular bin partition of the previous RoI pooling. The details of deformable RoI Pooling are shown below. The position-sensitive RoI pooling adopts a specific positive-sensitive score map $x_{i,j}$ to obtain the actual offset $\Delta p_{i,j}$.

![3x3 Deformable RoI Pooling](/images/DailyPaper/02/6.jpeg "3x3 Deformable RoI Pooling")

![3x3 Deformable Position-Sensitive RoI Pooling](/images/DailyPaper/02/7.jpeg "3x3 Deformable Position-Sensitive RoI Pooling")

- In the following experiments, deformable convolution is added in different locations of ResNet. Besides, big and small objects are used for testing and validation. Moreover, this paper points out that **deformable convolution is a generalization of dilated convolution**. The performance improvement is not very significant, and the major improvement is based on the methods. (The performance improves in the Deformable ConvNets v2.)

- From my perspective, it is similar to applying the attention mechanism. The deformable convolution may combine with the attention mechanism to improve the model performance.

### [29_Deformable ConvNets v2_More Deformable, Better Results](https://arxiv.org/pdf/1811.11168.pdf)

- This paper improves the problem that occurred in Deformable ConvNet v1, which is the cover of irrelevant context. This paper raises three metrics: **Effective Reception Fields, Effective Sampling/Bin Locations, and Error-bounded Saliency Regions**. The Deformable ConvNet is incorporated into  *Fast R-CNN and Mask R-CNN* with different backbones in the experiment part.

- Moreover, three methods are tackled to improve the Deformable ConvNet v2. First, more deformable convolutions are used in the network. Second, a new weight is trained and applied to alleviate the effects of irrelevant. The new weight is represented as: $y(p)=\sum_{k=1}^K\omega_k x(p+p_k+\Delta p_k) \Delta m_k$. Third, mimicking the R-CNN feature to eliminate irrelevant information. In R-CNN, cropping and resizing the input images can effectively remove the irrelevant information in the background. The information on crop patches is more valuable than the information in the full image. The network training with the R-CNN feature mimicking is shown below. The performance of this new version network has improved by about 2% more than the original network.

![Network Training with R-CNN Feature Mimicking](/images/DailyPaper/02/12.jpeg "Network Training with R-CNN Feature Mimicking")

## Classical Models

### [29_LeNet_Gradient-Based Learning Applied to Document Recognition](https://ieeexplore.ieee.org/document/726791)

- **"Hello World"**

- This paper raises the LeNet-5 network to solve the problem of recognizing hand-written digits problem. The general architecture is the following:  
Input layer(32*32) -> C1(kernel size = 5x5, stride  = 1, depth = 6) -> S2(Kernel size = 2x2, stride = 2, Sigmoid) -> C3(kernel size = 5x5, stride = 1; To control the parameters and achieve the asymetric, the feature map is not generated by S2.) -> S4(Kernel size = 2x2, stride = 2) -> C5(Kernel size = 5x5, stride = 1) -> F6(Full connected layer) -> Output layer
The architecture of LeNet-5 is shown below. In Caffe, the architecture of LeNet used **ReLU** to replace Sigmoid and increase the kernels to meet the hardware conditions nowadays to improve accuracy and speed.

![The Architecture of LeNet](/images/DailyPaper/02/11.png "The Architecture of LeNet")

- The effect of this network is shown below. This gif is cited from [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/)

![LeNet](/images/DailyPaper/02/10.gif "LeNet")

### [15_AlexNet_Imagenet-classification-with-deep-convolutional-neural-networks-Paper](https://dl.acm.org/doi/10.1145/3065386)

- **AlexNet** is the classical network. In this paper, **Dropout** is proposed to reduce overfitting in the fully-connected layers. The training and testing dataset are *ImageNet LSVRC-2010*. Also, **Rectified Linear Units (ReLUs)** are applied to improve the network's speed. AlexNet's structure is similar to LeNet. The overall architecture of AlexNet is shown below. It consists of 5 convolution layers and three fully connected layers. Because of the enormous calculation parameters, 2 GPUs train in parallel.

![Dilated Convolution](/images/DailyPaper/02/2.jpeg "AlexNet")

- Besides, **Local Response Normalization (LPN)** increases the generalization ability. The equation of LPN is given by:
$b_{x,y}^i=a_{x,y}^i/(k=\alpha \sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(\alpha_{x,y}^i)^2)^\beta$
In addition, **Overlapping Pooling** is applied to decrease the top-1 and top-5 errors.

- To reduce the overfitting problem, two methods are applied. The first one is **Data Augmentation**. This method has two forms: *generate image translations and horizontal reflections, altering the RGB channels' intensities in training images.* The second method is the **Dropout** layer. This layer randomly ignores some neural to avoid the overfitting problem.

### [16_VGGNet_VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](http://arxiv.org/abs/1409.1556)

- This paper focuses on the effect of network depth and proposes **VGGNet**. The dataset used to test is *ImageNet Challenge 2014*. This paper also points out that the LRN is not helpful but increases memory consumption and computation time. The ConvNet configurations are shown below. There are two kinds of VGGNet, one is VGG16, and the other one is VGG19. The main difference between these two networks is the depth.

![VGGNet](/images/DailyPaper/02/13.jpeg "VGGNet")

- To increase the depth and keep the same receptive field, **3 x 3 convolutional kernels replace the 7 x 7 convolution kernel, and two 3 x 3 convolutional kernels replace the 5 x 5 convolution kernel**. The convolution diagram is shown below. After replacing, the number of parameters decreases from $7^2 C^2$ to $3^3 C^2$. Besides, it can be seen from the table that the 1 x 1 convolutional layer incorporates into the network. **The function of the 1 x 1 convolution layer is to increase the decision function's non-linearity without affecting the convolutional layer's receptive fields.** The 1 x 1 conv. layer works like a kind of non-linearity shift. In the following experiments, the deeper network performs well, which shows that **the depth of convolution net is very critical**. However, the deeper network will consume more computational resources and memory. The high consumer mostly is because of the fully connected layers. Some papers indicate that the fully connected layer could be removed without affecting the network's performance. The details will be introduced in the following blog. Also, VGGNet contributes a general pre-trained model for the future network, which is meaningful for the researcher.

![Small Convolutional Kernel](/images/DailyPaper/02/17.jpeg "3 x 3 Convolutions Replaces 5 x 5 and 7 x  7Convolutions")

### [16_ZFNet_Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901/)

- The analysis of ZFNet is shown in the first paper analysis of this blog.

### Inception

### [17_Inceptionv1_GoogLeNet_Going deeper with convolutions](http://arxiv.org/abs/1409.4842)

- This paper proposes a deep convolutional neural network architecture named **Inception v1** and the **GoogLeNet**. At that time, the method to improve the network's performance is to increase depth and width. However, this will result in overfitting because of many parameters, increased use of computational resources, and difficulty training the network because of the disappearing gradient. This paper proposes using sparse structures to replace the fully connected layers, the Inception module, to solve these problems. The structure of the Inception module is shown below. The 1 x 1 convolutions before the 3 x 3 and 5 x 5 convolutions decrease the computation complexity and deepen the layer to increase the ability. Moreover, this paper suggests that the Inception module should be used in the high layer of the network. One of the main benefits of this architecture is that it significantly increases the number of units at each stage without an uncontrolled computational complexity blow-up. Another valuable aspect of this design is that it aligns with the intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from different scales simultaneously. The role of Inception is to replace the manual determination of the type of filter in the convolutional layer or whether to create convolutional and pooling layers, allowing the network to learn what parameters it exactly needs.

![Inception Module](/images/DailyPaper/02/22.png "Inception Module")

- Based on the Inception module, this paper builds the GoogLeNet. The structure of GooLeNet is shown below. In the following testing in ILSVRC 2014 classification challenge, the GoogLeNet performs very well.

![GoogLeNet Incarnation of the Inception Srchitecture](/images/DailyPaper/02/23.png "GoogLeNet Incarnation of the Inception Srchitecture")

### [17_Inceptionv2_Batch Normalization_Accelerating Deep Network Training](http://arxiv.org/abs/1502.03167)

- This paper mainly raises a revolution algorithm: **Batch Normalization** to solve the Internal Covariate Shift in training the deep neural network and improve **Inception V2**. What is the *Internal Covariate Shift*? 'The distribution of each layer's inputs changes during training as the parameters of the previous layers change. Internal Covariate Shift slows down the training by requiring lower learning rates and careful parameter initialization and makes it notoriously hard to train models with saturating non-linearities.' In short, the parameters in the n layer will change in every epoch. The batch normalization idea is from the whitened images. The batch normalization algorithm is shown below. The difference between LRN and Batch Normalization is that LRN is a local normalization of the position of a point on a feature map, while BN is a normalization operation for a feature map.

![Batch Normalization](/images/DailyPaper/02/24.png "Batch Normalization")

- The training process of the batch-normalized network is shown below.

![Training a Batch Normalized Network](/images/DailyPaper/02/25.png "Training a Batch Normalized Network")

- Besides, the Inception V2 module use two 3 x 3 convolutions to replace 5 x 5 convolution. The structure of Inception V2 is shown below. The Inception V2 network also *increases the learning rate, removes Dropout and L2 regulation, accelerates the learning rate decay, removes Local Response Normalization(LRN), shuffled training samples, and reduces the photometric distortions* to fasten and improve the accuracy.

![Inception V2](/images/DailyPaper/02/26.png "Inception V2")

### [17_Inceptionv3_Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)

- This paper proposes an **Inception v3** module and modifies some layers in GoogLeNet. Moreover, this paper raises some general design principles: avoid representational bottlenecks, higher dimensional representations are easier to process locally within a network, spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power, and balance the width and depth of the network.

- To improve the Inception module, this paper suggests some methods. The first one is to factorize a large convolution to small convolutions (e.g., 5 x 5 to be two 3 * 3) or irregular convolutions(Factorize n x n to be 1 x n and n x 1). The figure below illustrates factorizing 3 x 3 to 1 x 3 and 3 x 1.

![Factorization](/images/DailyPaper/02/27.png "Factorization")  
After adopting this unregulated factorization, the 3xInception, 5xInception, and 2xInception modules will be like the below.

![3xInception](/images/DailyPaper/02/28.png "3xInception")

![5xInception](/images/DailyPaper/02/29.png "5xInception")

![2xInception](/images/DailyPaper/02/30.png "2xInception")  
The second method is to utilize auxiliary classifiers such as batch normalization. The third method is to reduce efficient grid size. The modified Inception module is shown below.

![Inception Module that Reduces the Grid-size](/images/DailyPaper/02/32.png "Inception Module that Reduces the Grid-size")

- Therefore, Inception V2 is presented below. One thing that needs to be noticed; the input has changed from 224 to 299. If BN-auxiliary is combined with Inception v2, then Inception V3 is generated.
  
![New Inception V2](/images/DailyPaper/02/31.png "New Inception V2")  

### [14_Xception_ Deep Learning with Depthwise Separable Convolutions](http://arxiv.org/abs/1610.02357)

- This paper proposes **Depthwise Separable Convolution** to replace the Inception to improve the performance. The Xception architecture is a linear stack of depthwise separable convolution layers with residual connections. The extreme version of the Inception module of depth separable convolution is shown below. This kind of Inception module uses $k_1$ different convolutions on each channel separately to decrease the parameter to the $\frac{1}{k_1}$ of the regular convolution to speed up the convolution significantly.

![An Extreme Version of Inception Module](/images/DailyPaper/02/37.png "An Extreme Version of Inception Module")

- The overall Xception architecture is shown below.

![Xception](/images/DailyPaper/02/36.png "Xception") 

### [17_Inceptionv4, Inception-ResNet and the impact of residual connections on learning](http://arxiv.org/abs/1602.07261)

- This paper raises **Inception v4** which combines Inception and ResNet. Since there are many diagrams, I only show the Schema of the Inception-v4 Network and the Schema for Inception-ResNet-v1 and Inception-ResNet-v2 Networks here. The diagram can be seen in the original paper.

![Schema of Inception-v4 Network](/images/DailyPaper/02/33.png "Schema of Inception-v4 Network")
![Schema for Inception-ResNet-v1 and Inception-ResNet-v2 Networks](/images/DailyPaper/02/34.png "Schema for Inception-ResNet-v1 and Inception-ResNet-v2 Networks")  
The combination network replaces the filter concatenation stage of the Inception architecture with residual connections and adds more Inception and better connections. Besides, it is helpful to scale the residuals to avoid the 0 that occurs when the number of filters exceeds 1000, and the scaling block is shown below. Moreover, ResNet is not used to improve depth and accuracy but to speed up. It is the more extensive network size that improves the network accuracy.

![Scaling Block](/images/DailyPaper/02/35.png "Scaling Block")  

### [111_Highway Networks](http://arxiv.org/abs/1505.00387)

- This paper is a brief version of the next one.

### [111_Details of Highway Network_Training Very Deep Networks](http://arxiv.org/abs/1507.06228)

- These two papers propose **Highway Network** to solve the problem of hard training the deep neural network and the degradation problem. Inspired by the gating mechanism in LSTM, the Highway network utilizes the **Stochastic Gradient Descent(SGD)** to simplify the training of the deep neural network. The output y in ith layer will be **y = H(x, $W_H$)T(x, $W_T$) + xC(x, $W_C$)**. If C is set as 1-T, then y = x, if T(x, $W_T$) = 0; H(x, $W_H$), if T(x, $W_T$) = 1. In this equation, T(x) = $\sigma (W_T^T x+b_T)$. So a portion of the input is processed, and a portion is passed directly through. However, in the following experiment, as shown below, the Highway network could alleviate the problem of the larger error in the deeper layer, but some problems still exist in the deep layer. This problem is handled in ResNet, which will be introduced next.

![Comparisonn of Optimization of Plain Network and Highway Network](/images/DailyPaper/02/58.jpeg "Comparisonn of Optimization of Plain Network and Highway Network") 

### [18_ResNet_Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385)

- This paper is an excellent paper on the evolution process of computer vision. This paper proposes a **Residual Learning Framework** to ease the training pressure of training deep networks and solve the degradation problem. The degradation problem is shown below. It could be seen that the performance of the 56-layer is worse than the 20-layer.

![Degradation](/images/DailyPaper/02/60.jpeg "Degradation")  
So how can we solve this problem? The best method is to keep the x term. The residual Block is shown below. Then the output H(x) will be F(x) + x in the ith layer. If F(x) equals 0, then this will be identity mapping which provides the shortcut. If F(x) does not equal 0, then F(x) will be relatively smaller than x, which makes the gradient more sensitive.

![Residual Block](/images/DailyPaper/02/61.jpeg "Residual Block")  
Moreover, a bottleneck building block is built as below. This block will be often mentioned in the future. 1 x 1 layers are used to reduce and increase dimension.

![Bottleneck Residual Block](/images/DailyPaper/02/63.jpeg "Bottleneck Residual Block")  

- Therefore, if we integrate the residual block in the network, it will be shown below.

![VGG-19, 34-layer Plain, 34-layer Residual](/images/DailyPaper/02/62.jpeg "VGG-19, 34-layer Plain, 34-layer Residual")  
Then we have the apparent improvement compared with the plain network as below. ResNet is undoubtedly a significant invention in computer vision.

![Improved Result](/images/DailyPaper/02/59.jpeg "Improve Result") 

### [18_ResNeXt_Aggregated Residual Transformations for Deep Neural Networks](http://ieeexplore.ieee.org/document/8100117/)

- This paper proposes **ResNeXt** which combines ResNet and Inception. In ResNeXt, the Group Convolution is adopted. Cardinality, the size of the set of transformations, controls the dimension. The idea of cardinality is like divide and conquer. The ResNeXt structure is shown below. Also, the equivalent building blocks of ResNeXt are shown in the following. The equivalent blocks use the split-transform-merge, which splits x into D features to perform linear transformer and finally merge. In equation form, it will be $\sum_{i=1}^D w_i x_i$. Inception is modified as F(x) = $\sum_{i=1}^C T_i(x)$ which merges cardinality. Moreover, this equation will be y = x + $\sum_{i=1}^C T_i(x)$ with residual network.

![ResNeXt](/images/DailyPaper/02/64.jpeg "ResNeXt")

![Equivalent Building Block](/images/DailyPaper/02/65.jpeg "Equivalent Building Block")
 
### [19_DenseNet_Densely Connected Convolutional Networks](https://ieeexplore.ieee.org/document/8099726/)

- This paper proposes **DenseNet**, which connects each layer to every other layer in a feed-forward fashion. This structure will reduce the parameter and the transmitting process to improve the results efficiently. For an L layers network, DenseNet will have $\frac{L(L+1)}{2}$ direct connections. The illustration of a 5-layer dense block is shown below. Also, the architecture of DenseNet is shown in the following. In equation form, the layer l will be $x_l = H_l([x_0, x_1, ..., x_{l-1}])$, $[x_0, x_1, ..., x_{l-1}]$ is the concatenation of the feature-maps produced in layer 0 to l-1. This kind of connection helps train the network by upgrading gradient back-propagation. Also, the parameter for DenseNet is smaller with the concatenation features. From my perspective, DenseNet is like a particular case of ResNet.

![Illustration of 5-layer Dense Block](/images/DailyPaper/02/66.jpeg "Illustration of 5-layer Dense Block")

![The Architecture of DenseNet](/images/DailyPaper/02/67.jpeg "The Architecture of DenseNet")

### [19_HRNet_High-Resolution Representations for Labeling Pixels and Regions](http://arxiv.org/abs/1904.04514)
### [19_HRNet_Deep High-Resolution Representation Learning](http://arxiv.org/abs/1908.07919)

- These two papers show **HRNet v1, HRNet v2 and HRNet v2p**. There are some methods for computing high-resolution: Recover high-resolution representations from low-resolution representations (e.g., Hourglass, SegNet, DeconvNet, U-Net, and encoder-decoder), Cascade pyramid(RefineNet), Maintain the high-resolution representation through high-resolution convolutions and strengthen the representations with parallel low-resolution convolutions, Dilated convolution to increase the receptive field. The HRNet maintains high-resolution representations by connecting high-to-low-resolution convolutions in parallel and conducting multi-scale fusions across parallel convolutions. The sample structure of HRNet is shown below.

![The Architecture of HRNet](/images/DailyPaper/02/70.jpeg "The Architecture of HRNet")

- The multi-resolution blocks are also shown in the paper. The differences between multi-resolution convolution and regular convolution lie two-fold. (1) In a multi-resolution convolution, each subset of channels is over a different resolution. (2) The connection between input channels and output channels needs to handle. The resolution decrease is implemented by using several 2-stride 3 × 3 convolutions. The resolution increase is implemented by bilinear (nearest neighbor) upsampling.

![Multi-resolution Block](/images/DailyPaper/02/68.jpeg "Multi-resolution Block")

- The main difference among HRNet v1, v2, and v2p is the head of the network as shown below. (a) is for HRNet v1, (b) is for HRNet v2 and (c) is for HRNet v2p. (a) only output the representation from the high-resolution convolution stream, (b) concatenates the (upsampled) representations that are from all the resolutions used for semantic segmentation, (c) forms a feature pyramid from the representation by HRNetV2 and used for the objection detection.

![The Head of HRNetv1, v2 and v2p](/images/DailyPaper/02/69.jpeg "The Head of HRNetv1, v2 and v2p")

### [19_SENet_Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)

- This paper proposes the **Squeeze-and-Excitation(SE) block** which is shown below. In the squeeze part, which can be considered as Global Information Embedding, this paper used global average pooling to achieve. In equation, $z_c=F_{sq}(u_c)=\frac{1}{H W}\sum_{i=1}^H \sum_{j=1}^W u_c(i,j)$. The global average pooling could exploit channel dependencies that utilize the correspondings of channels. In the excitation part, two fully connected layers are applied to capture channel-wise dependence fully. In equation, s = $F_{ex}(z, W)=\sigma (g(z, W)) = \sigma (W_2 \delta(W_1, z))$ where $\delta$ is the ReLU function and $\sigma$ is the softmax function. After excitation, the real scale of the channel could be found. SENet strengthens the features of important channels and weakens the features of non-important channels.

![A Squeeze-and-Excitation Block](/images/DailyPaper/02/72.jpeg "A Squeeze-and-Excitation Block")

- The comparison between Inception and SE-Inception/ Residual and SE-ResNet is shown below. The structure of ResNet-50, SE-ResNet-50, and SE-ResNeXt-50 is also shown in the following.

![The Comparison between Inception and SE-Inception/ Residual and SE-ResNet](/images/DailyPaper/02/71.jpeg "The Comparison between Inception and SE-Inception/ Residual and SE-ResNet")

![The Structure of ResNet-50, SE-ResNet-50 and SE-ResNeXt-50](/images/DailyPaper/02/73.jpeg "The Structure of ResNet-50, SE-ResNet-50 and SE-ResNeXt-50")

- I think SENet could combine with the channel-attention mechanism to make some improvements. I will further explore.

### [110_CSPNET A NEW BACKBONE THAT CAN ENHANCE LEARNING](http://arxiv.org/abs/1911.11929)

- This paper proposes **Cross Stage Partial Network(CSPNet)**, which respects the variability of the gradient by integrating feature maps from the beginning and the end of a network stage. The primary purpose of designing CSPNet is to enable this architecture to achieve a more prosperous gradient combination while reducing the amount of computation. This aim is achieved by partitioning the feature map of the base layer into two parts and then merging them through a proposed cross-stage hierarchy. CSPNet solves three problems: Strengthen the learning ability of a light CNN, Remove computational bottlenecks, and Reduce memory costs. So what aspects does the CSPNet improve? CSPNet separates the feature map of the base layer into two parts. One part will go through a dense block and a transition layer; the other is combined with a transmitted feature map to the next stage.

- CSPDenseNet, which consists of a partial dense block and a partial transition layer, is proposed as shown below. It could be seen from the equation that CSPDenseNet truncates gradient flow from receiving the repeated gradient information.

![DenseNet and CSPDenseNet](/images/DailyPaper/02/74.jpeg "DenseNet and CSPDenseNe")  
The partial dense block is designed to increase the gradient path, balancing the computation of each layer and reducing memory traffic. The partial transition layer is designed to maximize the difference in gradient combinations. The feature fusion strategies are shown below.

![Different Feature Fusion Strategies](/images/DailyPaper/02/77.jpeg "Different Feature Fusion Strategies")  
The exact fusion model is used to look precisely to predict, aggregate feature pyramid, and balance computation. The different feature pyramid fusion strategies are shown below.

![Different Feature Pyramid Fusion Strategies](/images/DailyPaper/02/75.jpeg "Different Feature Pyramid Fusion Strategies")

### [110_EfficientNet Rethinking Model Scaling for Convolutional Neural Networks](http://arxiv.org/abs/1905.11946)

- This paper proposes the **EfficientNet**, which could uniformly scale all depth/width/resolution dimensions using a compound coefficient. The model scaling could achieve a good grid, but the scaling could result in tedious parameter adjusting. The following figure shows the model scaling cases. It is significant to find a balance point between depth, width, and resolution which is an every-one-love trade-off problem. That is how EfficientNet works. EfficientNet uses the compound scaling method to enlarge the neural network by increasing the width and height depth by the ratio. To simplify the problem, it will be an optimization problem:  
$max_{d,w,r}$ Accuracy(N(d,w,r)) s.t. N(d,w,r) = $\bigodot$ $F_i^{dL_i}(X_{<rH_i, rW_i, wC_i>})$, Memory(N) $\le$ target_memory, FLOP(N) $\le$ target_flops.

![Model Scaling](/images/DailyPaper/02/76.jpeg "Model Scaling")

- Depth is for the richer and more complex features. Width is for the more fine-grained features. The Resolution is for the more fine-grained patterns. Intuitively, we should increase network depth for higher-resolution images so that larger receptive fields can help capture similar features that include more pixels in bigger images. Correspondingly, we should increase network width when the resolution is higher to capture more fine-grained patterns with more pixels in high-resolution images. Therefore, in the compound scaling method, a compound coefficient $\phi$ is used to uniformly scales network width, depth, and resolution:  
depth: d = $\alpha^\phi$, width: w = $\beta^\phi$, resolution: r = $\gamma^\phi$. s.t. $\alpha\beta^2\gamma^2\approx 2$, $\alpha \ge 1, \beta \ge 2, \gamma \ge 1$.  
Also, EfficientNet-B0 is proposed to improve the accuracy of searching and decrease FLOPs.

### [111_PReLu_Delving Deep into Rectifiers_Surpassing Human-Level Performance on ImageNet Classification](http://ieeexplore.ieee.org/document/7410480/)

- This paper proposes **Parametric Rectified Linear Unit(PReLU)**, which is the upgraded version of ReLu and LReLu. The definition of PReLU is **f($y_i$) = $y_i$, if $y_i$ > 0; $a_i$$y_i$, if $y_i$ $\le$ 0 = max(0, $y_i$) + $a_i$min(0, $y_i$)**. The comparison between ReLU and PReLU is shown below. If $a_i$ is very small and fixed, PReLU could be seen as LReLU. The upgrade of $a_i$ could be achieved by back-propagation, whose details will not be shown here.

![ReLU and PReLU](/images/DailyPaper/02/18.jpeg "ReLU and PReLU")

- Only a tiny number of parameters are added in PReLU, which means that the computational effort of the network and the risk of over-fitting are increased only a little. In the following experiments, ReLU is replaced by PReLU. The improved network received excellent results in ImageNet 2012. Besides, a new initialization method based on PReLU is developed, which converges much faster than the Xavier method.

## Image Segmentation

### [112_FCN_Fully Convolutional Networks for Semantic Segmentation](http://arxiv.org/abs/1411.4038)

- This paper presents **Fully Convolutional Network(FCN)** for pixelwise predictionand from supervised pre-training. This network can take input of the arbitrary size and produce correspondingly-sized output with efficient inference and learning. Since the semantic segmentation task is to perform patchwise recognition, the FCN replaces the final three full connection layers of AlexNet with fully convolutional layers. Therefore, FCN could output a heatmap to perform pixel-wise prediction, shown below.

![FCN](/images/DailyPaper/02/80.jpeg "FCN")  
The process of generating heatmaps is upsampling. The upsampling process is to map the input C x H x W to the feature matrix in (H' x W')x(C x k x k), H' = $\frac{H-k+2pad}{stride}$+1, W' = $\frac{W-k+2pad}{stride}$+1. Up-sampling is to retain the original image to do the segmentation. Besides, skip architecture is adopted to optimize the output. It upsamples different pooling layers and combines them to optimize the output. The method of skip architecture is shown below. The samples of 32x, 16x, and 8x are shown in the following.

![Skip Architecture](/images/DailyPaper/02/82.jpeg "Skip Architecture")  

![Samples of Refining FCN](/images/DailyPaper/02/81.png "Samples of Refining FCN")  

### [112_FPN_Feature Pyramid Networks for Object Detection](http://arxiv.org/abs/1612.03144)

- This paper proposes **Feature Pyramid Network(FPN)**, which could help recognize small objects. This pyramid architecture combines low-resolution, semantically robust features with high-resolution, semantically weak features via a top-down pathway and lateral connections. So the high resolution of low-level features and the high semantic information of high-level features can be used simultaneously to achieve a good prediction effect by fusing these different layers of features. There are several types of pyramid architecture, as shown below. FPN is the most accurate and efficient in them.

![Pyramid Architecture](/images/DailyPaper/02/84.jpeg "Pyramid Architecture") 

- FPN consists of three parts: **Bottom-up, Top-down and Lateral Connection**. The Bottom-up and Top-down architecture is shown below. The process of Bottom-up is to input the images into the backbone and output the same size features as a stage. The Top-down up-samples the feature map obtained from the high layer and passes down, which will spread the semantic information to the lower feature.

![Bottom-up and Top-down Architecture](/images/DailyPaper/02/83.jpeg "Bottom-up and Top-down Architecture")  
The Lateral Connection is shown below. The lateral connection contains three steps. Firstly, use 1 x 1 convolution to decrease the dimension of the feature map output at every stage. Secondly, add the obtained feature with $P_{n+1}$ obtained in the previous layer. Thirdly, perform a 3 x 3 convolution to obtain this layer's feature output $P_n$. The 3 x 3 convolution is to eliminate the aliasing effect caused by up-sampling, and the channels of these 3 x 3 convolutions are all 256 because the output features of all layers in the pyramid share classifiers.

![Lateral Connection](/images/DailyPaper/02/85.jpeg "Lateral Connection") 

- In Application, FPN is combined with Faster R-CNN to extract proposal. More details will be introduced in the R-CNN part. Besides, the FPN used for object segment proposals with MLP is shown below.

![FPN for Object Segmentation Proposals with MLP](/images/DailyPaper/02/86.jpeg "FPN for Object Segmentation Proposals with MLP") 

### [112_U-Net_Convolutional Networks for Biomedical](http://arxiv.org/abs/1505.04597)

- This paper proposes **U-Net** to solve the problem of medical image segmentation. The architecture of U-Net is shown below. It is a classical encoder-decoder architecture. The advantage of U-Net is the restoration of lost features after the concatenation. The feature in the deeper layer has the larger receptive field, but some edge features will be lost after the down-sample. The concatenation makes it possible to recover the lost features in the shallow layer. Moreover, U-Net has a jump-connected code structure and can fuse features from different layers. The fixation structure and small sample size of medical images together make U-Net the best model in medical image segmentation. Also, the semantic information of medical images is simple but essential, so U-Net can keep all features to do the segmentation. Moreover, the data for the medical image is rare and difficult to obtain so the light U-Net could perform well compared with the other model. Additionally, U-Net is more suitable for modifying to fit the multimodal data in medical image tasks. Therefore, U-Net is generally used in medical image segmentation tasks. 

![U-Net](/images/DailyPaper/02/87.jpeg "U-Net")

## GAN

### [112_GAN_Generative Adversarial Nets](http://arxiv.org/abs/1406.2661)

- This paper proposes a framework for $estimating generative models via an adversarial process$. This framework needs two models: **a generative model(G) and a discriminative model(D)**—the relation between these two models, like the police and thief. G will be trained to maximize the probability of D making a mistake while D will estimate the probability that a sample came from the training data rather than G. That is why the network is called **Generative Adversarial Nets(GAN)**. The training process of generative and discriminative is shown below. After k cycles of updating the discriminator, the generator's parameters are updated once using a lower learning rate. The generator is trained to minimize the gap between the generated and actual samples, equivalent to making the discriminator discriminate errors as much as possible.
Having a good discriminator first makes it possible to distinguish the actual samples from the generated samples well before the generator can be updated more accurately. A sufficient condition for a GAN global minimum is when the generator's probability distribution and the actual values' probability distribution are in the same Loop.

![Training Process](/images/DailyPaper/02/19.jpeg "Training Process of Generative and Discriminative")

- This paper also proposes one theorem and two propositions: Theorem 1. The global minimum of the virtual training criterion C(G) is achieved if $p_g$ = $p_{data}$. At that point, C(G) achieves the value − log 4.;  
Proposition 1. For G fixed, the optimal discriminator D is *$D_G^{\*} (x)= \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$*;  
Proposition 2. If G and D have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given G, and $p_g$ is updated to improve the criterion: *$E_{x~p_{data}}[log D^{\*}_G(x)] + E_{x~p_g}[log(1 - D^{\*}_G(x))]$, then $p_g$ converges to $p_{data}$*.
The overall algorithm of training is shown below. The loss function could be inituitively distinguished:  
**$min_G max_D$V(D, G) = $E_{x~p_{data}(x)}[logD(x)] + E_{z~p_z(z)}[log(1-D(G(Z)))]$**  
Moreover, from the paper, the disadvantages are that there is no explicit representation of $p_g$(x) and that D must be synchronized well with G during training. In particular, G must not be trained too much without updating D to avoid "the Helvetica scenario," in which G collapses too many values of z to the same value of x to have enough diversity to model data. Much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov
chains are never needed, the only backdrop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model.

![Algorithm for Training GAN](/images/DailyPaper/02/57.jpeg "Algorithm for Training GAN")

- From my perspective, GAN is very interesting. If let me use Chinese to describe this, I would like to express it as "以子之矛攻子之盾."

## Object Detection

### [113_Selective Search for Object Recognition](http://link.springer.com/10.1007/s11263-013-0620-5)

- This paper proposes the Selective Search algorithm, which performs segmentation and then searches exhaustively. A selective search algorithm is subject to the following design considerations: Capture All Scales, Diversification, and Fast to Compute. The selective search algorithm contains the **Hierarchical Grouping Algorithm and Diversification Strategies**. The main idea of this algorithm is to use the greedy algorithm to calculate the similarity, merge the most similar areas, calculate the similarity between the combination area and neighbor area, and continue this process to merge all figures as one area. The algorithm of the Hierarchical Grouping Algorithm is shown below. 

![Hierarchical Grouping Algorithm](/images/DailyPaper/02/51.png "Hierarchical Grouping Algorithm")  
For the Diversification Strategies, this paper uses different color spaces, and adopt different similarity, which is the sum of $s_{color}, s_{texture}, s_{size}, s_{fill}$. The details of this equation are shown in the paper. After using the selective algorithm, the model is fast to compute than other methods at that time. Nevertheless, the selective algorithm still has many aspects to improve, such as the greedy algorithm, which could be replaced by HOG + SVM.

### [113_OverFeat_Integrated Recognition, Localization and Detection using Convolutional Networks](http://arxiv.org/abs/1312.6229)

- This paper proposes **OverFeat**, a CNN network that handles classification, localization, and detection simultaneously. Also, the OverFeat integrates Multi-scale, Sliding windows, and Offset pooling to improve the network. 

- The network structure of OverFeat is similar to AlexNet. The first five convolution layers are regarded as feature extraction layers, and the final three fully connected layers are replaced by fully convolution layers as classification layers. In the classification part, the multi-scale input with a horizontal flip is also adopted in OverFeat. Moreover, offset pooling is used in the fifth layer to replace max pooling to increase the detection grid of images. The details of offset pooling are shown below.

![Offset Pooling](/images/DailyPaper/02/54.jpeg "Offset Pooling")  

- To better fit the convolution network, the Sliding Window method is used in this network. It is efficient in the full convolution architecture. The sliding window is applied in the final three convolution layers. Besides, this method is suitable for multi-scale inputs. The working process of the sliding window is shown below.

![Sliding Window](/images/DailyPaper/02/55.jpeg "Sliding Window")

- After classification, localization is performed. Firstly, classifier and regressor networks simultaneously function to generate an object bounding box. Secondly, train the regressor. Finally, merge the bounding box to combine predictions. The illustration of the localization/detection is shown below. The background type should be added in training to perform object detection.

![Localization/Detection Pipeline](/images/DailyPaper/02/53.jpeg "Localization/Detection Pipeline")

### R-CNN

### [114_R-CNN_Rich feature hierarchies for accurate object detection and semantic segmentation](http://arxiv.org/abs/1311.2524)

- This proposes **Region with CNN Features(R-CNN)**, which can be seen as the combination of Selective Search, CNN, and SVM. R-CNN combines two key insights: Apply high-capacity convolutional neural networks to bottom-up region proposals to localize and segment objects, and a paradigm for training large CNNs when labeled training data is scarce. The overview of R-CNN is shown below.  
Firstly,  identify about 1000-2000 candidate frames in the image using Selective Search. Secondly, scale the image blocks in each candidate frame to the same size and input them into the CNN for feature extraction. Thirdly, use a classifier to determine whether the features extracted from the candidate frames belong to a specific class. Finally, the regressor is used to adjust their positions for the candidate frames belonging to a particular class.  
The input images of R-CNN should be the same size of 224 x 224 in this paper. R-CNN is very slow because it needs to extract the feature in every region proposal and use SVM to classify.

![R-CNN](/images/DailyPaper/02/100.png "R-CNN")

### [114_SPPNet_Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](http://arxiv.org/abs/1406.4729)

- This paper proposes **Spatial Pyramid Pooling(SPP-net)**, which can generate a fix-length representation regardless of image size/scale. SPP-net could remove the fixed-size input image limitation of CNN. The comparison between SPP-net and the traditional structure is shown below. SPP-net has some remarkable properties instead of deep CNNs. Firstly,  SPP can generate a fixed-length output regardless of the input size, while the sliding window pooling used in the previous deep networks cannot. Secondly, SPP uses multi-level spatial bins, while sliding window pooling uses only a single window size. Multi-level pooling is robust to object deformations. Thirdly, SPP can pool features extracted at variable scales thanks to the flexibility of input scales. Therefore, we could use SPP-net to generate the candidate windows, and this kind of multi-Scale training can efficiently improve the accuracy.

![Architecture of SPP-net](/images/DailyPaper/02/101.png "Architecture of SPP-net")

- The crucial part of SPP-net is **Spatial Pyramid Pooling** which could generate fixed size outputs for arbitrary size inputs as shown below. The idea is to divide a feature map of arbitrary size into 16, 4, and 1 blocks and then maximize pooling on each block, pooling the features to obtain a fixed dimensional output.

![Spatial Pyramid Pooling](/images/DailyPaper/02/103.png "Spatial Pyramid Pooling")  
If we apply SPP to the object detection task, it will be like the one below. The pooling is performed in candidate windows. SPP is beneficial in object detection tasks. Firstly, the network can introduce arbitrarily sized inputs to be easily trained in multiple sizes. Secondly, SPP can produce a fixed output for any size of the input, which makes it possible to extract features once from multiple region proposals of an image. The process with SPP contains four steps.  
First, a series of region proposals are generated by selective Search. Secondly, train the multisize recognition network to extract the region features. This step means first training one size in each epoch to generate a model, then loading this model and training the second size until all sizes are trained. The scales used for spatial pyramid pooling are 1x1, 2x2, 3x3, and 6x6, which is 50 bins. Thirdly, each region proposal is selected to extract the corresponding features at a size that brings the number of pixels it contains closest to 224*224. Since SPP can accept any size of the input, each region proposal is mapped to a feature map, and then only this feature map is spatially pyramid pooled to obtain a fixed dimension of features for training the CNN. Finally, train SVM and perform BB(Bounding Box) regression.

![Spatial Pyramid Pooling in Object Detection](/images/DailyPaper/02/102.png "Spatial Pyramid Pooling in Object Detection")

- SPP-net only computes the convolution of the original image once to obtain the convolutional feature map of the whole image, then finds the patch of each candidate frame on the feature map and inputs this patch as the convolutional feature of each candidate frame to the SPP layer and the subsequent layers to complete the feature extraction. In this way, R-CNN computes the convolution for each region, while SPP-net only needs to compute the convolution once, which saves a lot of computation time and has a speedup of about 100 times compared with R-CNN.

### [114_Fast R-CNN](http://arxiv.org/abs/1504.08083)

- This paper proposes **Fast R-CNN**, which is the combination of R-CNN and SPP-net(RoI Pooling Layer). Also, softmax replaces SVM to classify. Moreover, the bbox regressor is combined with classification as a multi-task model. So the loss function is the multi-task loss. The whole training process is end-to-end. The architecture of Fast R-CNN is shown below. The problems of R-CNN are obvious: training is a multi-stage pipeline and is expensive in space and time, and object detection is slow. Therefore, Fast R-CNN is proposed to solve these problems.

![Architecture of Fast R-CNN](/images/DailyPaper/02/104.png "Architecture of Fast R-CNN")

- The process of Fast R-CNN is the following.  
Firstly, identify about 1000-2000 candidate frames in the image using Selective Search. Secondly, input the whole image into CNN and get the feature map. Thirdly, find the patch of each candidate frame on the feature map, and input this patch as the convolutional features of each candidate frame to the SPP layer and the subsequent layers. Fourthly, use the classifier to identify whether the features extracted from the candidate boxes belong to a specific class. Finally, the regressor is used to further adjust their positions for the candidate boxes belonging to a particular class.  
In the whole process, Fast R-CNN only extracts features once, which could increase detection speed.

### [114_Faster R-CNN_Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)

- This paper proposes **Faster R-CNN**, the combination of Fast R-CNN and Region Proposal Network (RPN). Since the Selective Search is time-consuming, RPN is used to replace it to find the region proposals. RPN is placed behind the last convolution layer to obtain the proposal region. Furthermore, an anchor box is introduced to fix the size of proposals. The overall structure of Faster R-CNN is shown below. 

![Architecture of Faster R-CNN](/images/DailyPaper/02/105.png "Architecture of Faster R-CNN")

- The details of RPN are shown below. A sliding window on the feature map is used to build a neural network for object classification and regression of box positions. The position of the sliding window provides information about the general position of the object, and the regression of the box provides a more accurate position of the box. The regression layer has 4k outputs encoding the coordinates of k boxes, and the classification layer outputs 2k scores that estimate the probability of an object or not an object for each proposal. So in this network, four loss functions are used: RPN classification, RPN regression, Fast R-CNN classification, and Fast R-CNN regression. From my perspective, I think this will be much beneficial for object detection accuracy.

![RPN](/images/DailyPaper/02/106.png "RPN")

- The process of Faster R-CNN is following.  
Firstly, input the whole image into CNN to get a feature map. Secondly, input the convolutional features to RPN to get the feature information of the candidate frame. Thirdly, extract the features from the candidate frames and use the classifier to determine whether they belong to a specific class. Finally, the regressor is used to further adjust their positions for the candidate boxes belonging to a particular class.

### [114_Mask R-CNN](http://arxiv.org/abs/1703.06870) (This paper focuses on Image Segmentation)

- This paper proposes **Mask R-CNN**, which is designed for image segmentation. The framework of Mask R-CNN is shown below. Mask R-CNN extends Faster R-CNN by adding a branch(ResNet-FPN) for predicting an object mask in parallel with the existing branch for bounding box recognition. Therefore, the multi-task loss of Mask R-CNN is L = $L_{cls}$ + $L_{box}$ + $L_{mask}$(Mask R-CNN = ResNet-FPN + Faster RCNN + Mask). The head architecture of Mask R-CNN is used to identify bounding boxes(classification and regression) and predict masks. The head architecture is shown in the following.

![Framework of Mask R-CNN](/images/DailyPaper/02/107.png "Framework of Mask R-CNN")

![Head Architecture](/images/DailyPaper/02/108.png "Head Architecture")

- Another essential improvement in Mask R-CNN is to use RoIAlign to replace RoI Pooling. RoIAlign is shown below. Using bilinear interpolation RoI Align to replace quantization RoI Pooling can solve the misalignment problem introduced by the quantizations. RoIAlign computes the value of each sampling point by bilinear interpolation from the nearby grid points on the feature map.

![RoIAlign](/images/DailyPaper/02/107.png "RoIAlign")

- Mask R-CNN can be used to detect the human key points. The method is to regard each key point as a One-hot binary mask. This idea is utilized in my previous project.

### [115_R-FCN_Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409)

- This paper proposes **R-FCN** which merges RPN and R-FCN. R-FCN is faster than Faster R-CNN and improves the position sensitivity based on Faster R-CNN. It also solves the problem between translation invariance in image classification and translation variance in object detection. The structure of R-FCN is shown below. It composes of an RoI-wise sub-network (Position-sensitive score map + Position-sensitive RoI pooling+Position-sensitive regression). The key idea is a position-sensitive scope map. In an RoI, all k x k subregions should have the corresponding part of an object, and then the classifier will judge this region as this object. The position-sensitive score map will add a convolution layer after the shared convolution layers. The last convolutional layer produces a bank of $k^2$ position-sensitive score maps for each category and thus has a $k^2$(C + 1)-channel output layer with C object categories (+1
for background). So the part of the object will have the highest score in the sub-region.

![Structure of R-FCN](/images/DailyPaper/02/89.jpeg "Structure of R-FCN")  
The Position of Sensitive RoI Pooling, shown below, is also important. The equation for pooling in subregion is $r_c (i,j|\theta) = \sum_{(x, y)\in bin(i,j)}$ $z_{i,j,c}(x+x_0,y+y_0|\theta)/n$. So the overall equation is $r_c(\theta)=\sum_{i,j}r_c(i,j|\theta)$ and $s_c(\theta)=e^{r_c(\theta)}$/$\sum_{c'=0}^C e^{r_{c'}(\theta)}$.

![Position Sensitive RoI Pooling](/images/DailyPaper/02/88.jpeg "Position Sensitive RoI Pooling")  
After pooling, Position-Sensitive Regression is used to regress. The dimension of the regression score map will be 4$k^2$, and each RoI will have four parameters as the coordination and the offset of length and width.

- This paper solves the problem of my mind in the last month, which is why we can not divide an object into small pieces and detect them respectively.

### YOLO

### [115_SSD_Single Shot MultiBox Detector](http://arxiv.org/abs/1512.02325)

- This paper proposes **Single Shot MultiBox Detector(SSD)**, which removes the process of generating proposals. The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. The structure of SSD compared with YOLO is shown below. The training process sets an IOU threshold to find the ground truth. If not matched, then the bounding box will be a negative sample. Since the negative sample is far larger than the positive sample, if the IOU of ground truth is greater than a certain threshold for the remaining unmatched prior frames, then that last frame is also matched with this ground truth. This strategy means that a ground truth may match more than one a priori frame, which is possible. However, the reverse is not permitted. The loss function is defined as the sum with weight: L(x,c,l,g) = $\frac{1}{N}(L_{conf}(x, c)) + \alpha L_{loc}(x, l,g)$. Moreover, data augmentation is used to improve the performance, like Using the entire original input image, Sampling a patch so that the minimum Jaccard overlap with the objects is 0. x, and Randomly sampling a patch.

![Structure of SSD compared with YOLO](/images/DailyPaper/02/99.png "Structure of SSD compared with YOLO")

- The SSD framework is shown below. SSD predicts category scores and box offsets for a fixed set of default bounding boxes using small convolutional filters applied to feature maps. Firstly, use multi-scale feature maps for detection. Secondly, use convolution to detect the result. Thirdly, use prior boxes to set the anchors, which contain c + 1 confidence.

![SSD Framework](/images/DailyPaper/02/98.png "SSD Framework")

- The disadvantage of SSD is that it relies on experience and performs poorly on small objects. There are some reasons for the faster SSD. Firstly, SSD is a one-stage network, which only needs one stage to output results, whereas Faster R-CNN is a two-stage network. Although the bounding boxes of Faster R-CNN are much fewer, it needs a lot of forward and backward inference (training phase), and it needs to train both networks alternately. Secondly, Faster R-CNN requires training not only RPN but also Fast R-CNN, while SSD is equivalent to an optimized RPN network, which does not require backward detection and takes much time for forward inference alone. Thirdly, although YOLO networks look simpler than SSD networks, YOLO networks contain a large number of fully connected layers. Compared with the fully connected layer, the convolution layer has fewer parameters. Meanwhile, the operation of YOLO to obtain candidate bounding boxes is time-consuming. In the SSD algorithm, the architecture of the VGG network is adjusted, and the fully connected layer is replaced by the convolution layer, which will significantly improve the speed. Fourth, the Atrous algorithm is used, which can speed up 20%. Finally, SSD sets the input image size and will crop different size images to 300x300 or 512x512. Compared to Faster-R-CNN, there will be much less computation on the input.

### [115_YOLOv1_You Only Look Once_Unified Real-Time Object Detection](http://arxiv.org/abs/1506.02640)

- This paper proposes **You Only Look Once(YOLO)**, a one-stage object detection method rather than a two-stage one. YOLO frames object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. Since YOLO is speedy, it enables assistive devices to convey real-time scene information to human users. The detection process of YOLO is shown below.

![YOLO](/images/DailyPaper/02/90.png "YOLO")  
Firstly, the input image is divided into S×S grids, for example, 7×7=49 grids here. If the object's center falls into a grid cell, that grid cell is responsible for detecting the object. Note that not the whole object falls into the grid, but only the center of the object.  
Secondly, each grid cell predicts B (B=2 in the figure) bounding boxes and a confidence score for these boxes. This score reflects the probability Pr (Object) that this box contains an object and the positional accuracy IOU of the predicted box, so the confidence score is also defined by these two components. Moreover, each bounding box should contain five predictions, x, y, w, h, and confidence. (x,y) The box's center is the coordinate relative to the grid cell. w and h are the width and height of the box equivalent to the whole map. Confidence represents the IOU between the box and the ground truth, and if no object score is in the box, confidence is directly 0.  
At the same time, since both location and category need to be predicted, each cell outputs the conditional probability of the object in addition to the bounding box. Each grid cell outputs a probability set regardless of how many bounding boxes the grid predicts.  
Finally, multiply the conditional classification probability with the confident prediction of each box as the confidence score for each box specific to each class. Then output the final prediction.  

- Compared with R-CNN, YOLO removes the region proposals and combines multi-stage into one stage to improve the speed. Also, the generalizable representations of YOLO learning are much better. However, a grid can only predict two boxes and one category, and this spatial constraint necessarily limits the number of predictions. Furthermore, YOLO only supports the exact resolution of the train images. Besides, the object IOU error and the small object IOU error contribute relative values to the loss in network training, regardless of the size of the bounding box approximated by the loss function as the detection performance. This results in YOLO not predicting the small objects that appear in groups. The architecture of YOLO is shown below.

![Architecture of YOLO](/images/DailyPaper/02/91.png "Architectiure of YOLO")  

### [116_YOLOv2_YOLO9000_Better, Faster, Stronger](http://arxiv.org/abs/1612.08242)

- This paper proposes **YOLO9000(YOLO v2 + Joint Train Algorithm)** based on YOLOv1.
YOLOv2 uses the following methods to improve the performance of YOLOv1: **Batch Normalization, High-Resolution Classifier, Convolutional With Anchor Boxes, Dimension Clusters(d(box, centroid) = 1 - IOU(box, centroid)), Direct Location Prediction(As shown below, this bounds the center of predicting box in the grid), Fine-Grained Features(Use passthrough layer to keep details), Multi-Scale Training**. Also, YOLO v2 proposes a new network, Darknet-19.

![Direct Location Prediction](/images/DailyPaper/02/92.png "Direct Location Prediction")

- **YOLO9000** uses **Joint classification and detection** and adopts **Dataset combination with WordTree**(This is shown below). This kind of training method is meant to improve performance. The main idea is to mix detection and classification data. If the training process encounters detection images with labels, back propagates the entire loss function based on YOLOv2. If the training process encounters classification images, back propagates only the classification loss of the network. Therefore, YOLO9000 is represented.

![Combining Datasets using WordTree Hierarchy](/images/DailyPaper/02/93.png "Combining Datasets using WordTree Hierarchy")

### [116_YOLOv3_An Incremental Improvement](http://arxiv.org/abs/1804.02767)

- This paper proposes **YOLO v3** which is the improved version of YOLO v2. YOLO v3 mainly improves YOLO v2 in three aspects. Firstly, YOLOv3 uses logistic classifiers to predict an objectness score for each bounding box based on the overlap between the predicted box and the object. If a box has a higher overlap than all other boxes, its score is 1, ignoring those boxes that are not the best and have an overlap greater than a certain threshold (0.5). Secondly, YOLO v3 uses FPN to perform multi-scale detection. Thirdly, the feature extractor of YOLO v3 adopts Darknet-53, shown below. Darknet adopts residual layers inside. YOLO v3 is faster and more accurate than YOLO v2.

![Darknet-53](/images/DailyPaper/02/94.png "Darknet-53")

### [116_YOLOv4_Optimal Speed and Accuracy of Object Detection](http://arxiv.org/abs/2004.10934)

- This paper presents **YOLO v4** which is the improved version of YOLO v3. The backbone is CSPDarknet53, and the neck uses SPP and PAN. YOLO v3 is adopted as the head. Bag of Freebies (BoF) for backbone uses CutMix and Mosaic data augmentation, DropBlock regularization, and Class label smoothing. Bag of Specials (BoS) for backbone adopts Mish activation, Cross-stage partial connections (CSP), and Multi-input weighted residual connections (MiWRC). Bag of Freebies (BoF) for detector takes CIoU-loss, CmBN, DropBlock regularization, Mosaic data augmentation, Self-Adversarial Training, Eliminate grid sensitivity, Using multiple anchors for a single ground truth, Cosine annealing scheduler, Optimal hyperparameters, Random training shapes. The bag of Specials (BoS) for the detector utilizes Mish activation, SPP-block, SAM-block, PAN path-aggregation block, and DIoU-NMS. This paper summarizes almost all the detection techniques, then sifts and permutates them, and finally verifies which ones work by ablation study. The illustration of the object detector is shown below. The modified SAM and PAN are shown in the following.

![Objection Detector](/images/DailyPaper/02/95.png "Objection Detector")

![Modified SAM and PAN](/images/DailyPaper/02/96.png "Modified SAM and PAN")

—————— **The following three papers show some applications of the YOLO series** ——————

### [117_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object Detection on Drone-Captured Scenarios](https://ieeexplore.ieee.org/document/9607487/)

- This paper proposes **TPH-YOLOv5** which use Transformer Prediction Heads(TPH) to replace the original prediction heads. Also, Convolutional Block Attention Model(CBAM) is adopted to find the attention region in the dense region. This drone model is applied to capture and recognize the photos. The overall architecture of TPH-YOLO v5 is shown below.

![TPH-YOLO v5](/images/DailyPaper/02/47.jpeg "TPH-YOLO v5")  
The architecture of TPH and CBAM is shown below. From my perspective, this module could be the determined cause for the good performance of this model since the scenes captured by drone are very small. The attention mechanism performs well in assisting in recognizing small objects.

![TPH](/images/DailyPaper/02/45.jpeg "TPH")
![CBAM](/images/DailyPaper/02/46.jpeg "CBAM")

### [117_YOLO v5_Improved YOLOv5 network for real-time multi-scale traffic sign detection](https://link.springer.com/10.1007/s00521-022-08077-5)

- This paper replaces FPN with **AF-FPN, which utilizes the Adaptive Attention Module (AAM) and Feature Enhancement Module(FEM)**. Also, this structure aimed to decrease FLOP(Floating-Point Operations per Second). The YOLO V5 architecture proposed in this paper uses the AAM and FEM as the neck, shown below.

![YOLO V5 Architecture](/images/DailyPaper/02/48.png "YOLO V5 Architecture")  

- The architecture of the AF-FPN is shown below.

![AF-FPN](/images/DailyPaper/02/51.png "AF-FPN")  
The AAM aims to reduce the loss of context information in the high-level feature map due to the reduced feature channels. The AAM uses multi-scale information to improve the ability. The structure of AAM is shown below.

![AAM](/images/DailyPaper/02/49.png "AAM")  
The FPN is adopted to enhance the representation of feature pyramids and accelerates the inference speed while achieving state-of-the-art performance. Dilated convolution is used in FPN to increase the reception field. The structure of FPN is shown below.

![FPN](/images/DailyPaper/02/50.png "FPN")  

- The Data argumentation methods in this paper are search space and search algorithm.
The search space contains five sub-strategies, each consisting of two simple image enhancement operations applied in sequence. Moreover, the search algorithm is considered a discrete optimization problem. These data arguments are used to suit the natural scenery. Besides, C(Completed)IoU is used to replace GIoU as the metrics. In the following experiment part, the network received the SOTA results.

### [117_YOLOV_Making Still Image Object Detectors Great at Video Object Detection](http://arxiv.org/abs/2208.09686)

- This paper improved the YOLOX by adding **Feature Selection Module(FSM) and Feature Aggregation Module(FAM)** to solve the Video Object Detection problem. The overall of this architecture is shown below.

![Overall Structure of YOLOV](/images/DailyPaper/02/44.jpeg "Overall Structure of YOLOV")

—————— **The following three papers show the evolution process of the YOLO series** ——————

### [118_YOLOX_Exceeding YOLO Series in 2021](http://arxiv.org/abs/2107.08430)

- This paper proposes **YOLOX** based on YOLO v3. The decoupled head of YOLO v3 is modified, and the difference between them is shown below. Also, stronger data augmentation methods are adopted (Mosaic and MixUp). Moreover, YOLOX changes the anchor-base frame of YOLO v3 into anchor-free. Since anchor-free is used, multi-positive is utilized to assist. Besides, SimOTA is utilized. This paper proposes key insights for an advanced label assignment: Loss/Quality Awre, Center Prior, Dynamic Number of Positive Anchors for Each Ground-truth(Dynamic Top-k), and Global View. The cost in SimOTA is $c_{ij} = L_{ij}^{cls}+\lambda L_{ij}^{reg}$. In addition, YOLOX is end-to-end. The modification in CSPNet and Tiny/Nano detector is used.

![Illustration of Difference between YOLOv3 Head and YOLOX](/images/DailyPaper/02/97.png "Illustration of Difference between YOLOv3 Head and YOLOX")

### [118_PP-YOLOE_An evolved version of YOLO](http://arxiv.org/abs/2203.16250)

- This paper presents **PP-YOLOE**, which optimizes based on the previous PP-YOLOv2, using an anchor-free paradigm, more powerful backbone, and neck equipped with CSPRepResStage, ET-head, and dynamic label assignment algorithm TAL.

- First, the anchor-free method is adopted. Eliminating the anchor-generating process makes the network efficient. Secondly, CSPRepResNet, the combination of CSPNet and RMNet, is the more robust backbone. The architecture of PP-YOLOE and the structure of CSPRepResStage is shown below.

![Architecture of PP-YOLOE and The Structure of CSPRepResStage] (/images/DailyPaper/02/79.jpeg "Architecture of PP-YOLOE and The Structure of CSPRepResStage")  
Thirdly, Task Alignment Learning(TAL) adopts the method of dynamic label assignment and task-aligned loss. In equation, it will be $L_{cls-pos} = \sum_{i=1}^{N_pos}BCE(p_i,t_i)$. Fourthly, Efficient Task-aligned Head(ET-Head) uses the method of VFL(Varifocal Loss) and DFL(Distribution Focal Loss). The loss funtion is Loss = $\frac{\alpha loss_{VFL} + \beta loss_{GIoU} +\gamma loss_{DFL}}{\sum_i^{N_{pos}} t}$. DFL is a good method to improve performance.

- Compared with YOLOX, PP-YOLOE stills uses the normalization as YOLOv3. Besides, PP-YOLOE will resize the images input to the fixed size.

### [118_YOLO-Z_Improving small object detection in YOLOv5 for autonomous vehicles](http://arxiv.org/abs/2112.11798)

- This paper proposes **YOLO-Z**, which improves the performance of the small objects of YOLOv5. YOLOv5 provides four different scales for its models, S, M, L, and X, representing Small, Medium, Large, and XLarge, respectively. Each of these scales applies different multipliers to the depth and width of the model, meaning that the overall structure of the model remains the same, but the size and complexity of each model are scaled proportionally. Some modification of YOLO-Z is shown below. This study explores the effect of redirecting the connections involving higher-resolution feature maps in order for them to be fed directly to the neck and head. This aim can be done in an 'inclusive' manner by expanding the neck to fit an extra feature map or in an 'exclusive' fashion by replacing the lowest-resolution feature map to fit the new one. YOLO-Z improves the small object detection in YOLO v5 for autonomous vehicles.

![Some Modification in YOLO-Z](/images/DailyPaper/02/78.png "Some Modification in YOLO-Z")  

## Light Networks

### MobileNet

### [119_MobileNets_Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)

- This paper proposes **MobileNet**, which uses depthwise separable convolutions to realize the lightweight deep neural network. What is depthwise separable convolution? A form of factorized convolution consists of a depthwise convolution and a 1x1 convolution called a pointwise convolution. This kind of convolution can efficiently decrease the complexity of computation. Assume the traditional convolution has an N convolution kernel whose size is $D_k \times D_k \times M$, and each one will perform $D_F \times D_F$ times. Then the computation cost is $D_k \times D_k \times M \times N \times D_F \times D_F$. Assume the kernel size of depthwise convolution is $D_k \times D_k \times M$ and the kernel size of pointwise convolution is $1 \times 1 \times M$, and the total number of the convolution kernel is N. Then the computation cost will be $D_k \times D_k \times M \times D_F \times D_F + 1 \times 1 \times M \times N \times D_F \times D_F$. If we use the computation cost of depthwise separable convolution to divide the computation cost of traditional convolution, then we have $\frac{1}{N} + \frac{1}{D_K^2}$. Assuming a 3 x 3 convolution kernel is used, then the computation of depthwise separable convolution will be $\frac{1}{9}~\frac{1}{8}$ of traditional convolution. The depthwise separable convolution is shown below. Moreover, the depthwise separable convolution with Batch Normalization and ReLU is shown in the following. The ReLU used here is ReLU(6), which equals min(max(0, x), 6). ReLU is a nonlinear activation function with stronger robustness under low-precision calculation.

![Depthwise Seperable Convolution](/images/DailyPaper/02/119.png "Depthwise Seperable Convolution")  

![Depthwise Seperable Convolution with BN and ReLU](/images/DailyPaper/02/120.png "Depthwise Seperable Convolution with BN and ReLU")

- The architecture of MobileNet is shown below. Also, this paper introduces Width Multiplier($\alpha$) to make the model thinner and Resolution Multiplier($\rho$) to reduce representation. The computation cost of modified depthwise separable convolution is $D_k \times D_k \times \alpha M \times \rho D_F \times \rho D_F + \alpha M \times \alpha N \times \rho D_F \times \rho D_F$. The computation cost can be further controlled by the $\alpha$ and $\rho$.

![Architecture of MobileNet](/images/DailyPaper/02/121.png "Architecture of MobileNet")

### [119_MobileNetV2_Inverted Residuals and Linear Bottlenecks](https://ieeexplore.ieee.org/document/8578572/)

- This paper proposes **MobileNet V2**, which removes non-linearities in the thin layers to maintain representational power. The inverted residual with linear bottleneck is added. This module takes a low-dimensional compressed representation as an input, which is first expanded to a high dimension and filtered with a lightweight depthwise convolution. Features are subsequently projected back to a low-dimensional representation with a linear convolution. The back-propagation is because the deep convolutional kernels of the degree convolution part are easy to waste after training, so many of the convolutional kernels trained by deep convolution are empty. This phenomenon is because ReLU operations on low dimensions can easily cause information loss. In contrast, ReLU operations in high dimensions will result in little information loss. So Linear bottleneck is adopted for the low dimension. The new depthwise separable convolution is shown below.

![Evolution of Separable Convolution Blocks](/images/DailyPaper/02/122.png "Evolution of Separable Convolution Blocks")

- Moreover, since pointwise convolution, also known as 1 × 1 convolution, can be used to increase and decrease dimensions, then the pointwise convolution can be used to increase dimension before the depthwise convolution, and then the convolution operation in a higher dimensional space to extract features(Pointwise convolution->Depthwise convolution-> Pointwise convolution). Besides, a shortcut structure is used to reuse the features. So the comparison of convolutional blocks for different architecture is shown below.

![Comparison of Convolutional Blocks for Different Architecture](/images/DailyPaper/02/123.png "Comparison of Convolutional Blocks for Different Architecture")

- Moreover, this paper proposes SSDLite used in object detection, which replaces all traditional convolution with depthwise separable convolution. SSDLite could decrease parameters and costs.

### [119_MobileNetv3_Searching for MobileNetV3](https://ieeexplore.ieee.org/document/9008835/)

- This paper proposes **MobileNet v3**, a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and subsequently improved through novel architecture advances. Furthermore, two MobileNet models are developed: MobileNetV3-Large and MobileNetV3-Small, to fit the different sceneries(High resource and Low resource). The main improvements are the following. Firstly, the **h-switch** activation function, whose characteristics are no upper bound and lower bound, smooth, and non-monotonic, is used. The equation of switch and h-switch are respectively: switch x = x $\sigma(x)$ and h-switch[x] = x$\frac{ReLU6(x+3)}{6}$. Their function shape of them is shown below. Moreover, the h-switch is only utilized in the model's last part.

![Sigmoid and Switch](/images/DailyPaper/02/125.png "Sigmoid and Switch")  
Also, the SE block is added in the MobileNet v3 block, shown below.

![MobileNet v2 Block and MobileNet v3 Block](/images/DailyPaper/02/124.png "MobileNet v2 Block and MobileNet v3 Block")  
Moreover, NAS and NetAdapt have been adopted. NAS is used to search the network to optimize each block with a limited number of computations and parameters, so it is called Block-wise Search. NetAdapt is used to finetune the number of convolutional kernels in each layer of the network after each block is determined, so it is called Layer-wise Search. The architecture of Mobile V3's large and small versions is shown below.

![Architecture of MobileNet v3 Large and Small](/images/DailyPaper/02/126.png "Architecture of MobileNet v3 Large and Small")

### ShuffleNet

### [120_ShuffleNet_An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/abs/1707.01083)

- This paper proposes **ShuffleNet** which utilizes **pointwise group convolution and channel shuffle**. The channel shuffle is shown below. The feature map after the group convolution is uniformly broken up so that the next group convolution is used with inputs from different groups so that the information can flow between the different groups. Implementing channel shuffle is very easy. Assume that the input layer is divided into g groups and the total number of channels is g\*n. First, split that channel's dimension into two dimensions (g,n). Secondly, transpose those two dimensions into (n,g). Finally, reshape them into one dimension g\*n.

![Channel Shuffle](/images/DailyPaper/02/110.png "Channel Shuffle")  
Combined with pointwise group convolution, a ShuffleNet Unit, as shown below, can be produced. The ShuffleNet architecture is shown in the following. The pointwise convolution is adopted to solve the computation problem brought by the group convolution. Since the enormous pointwise convolution will bring the problem of communication loss in a different group, channel shuffle is introduced. Therefore, we can see that channel shuffle and 1 x 1 pointwise convolution is all used in the ShuffleNet units.

![ShuffleNet Units](/images/DailyPaper/02/111.png "ShuffleNet Units")

![ShuffleNet Architecture](/images/DailyPaper/02/112.png "ShuffleNet Architecture")

### [120_ShuffleNet V2_Practical Guidelines for Efficient CNN Architecture Design](https://link.springer.com/10.1007/978-3-030-01264-9_8)

- This paper proposes **ShuffleNet V2** which solve some problems in ShuffleNet V1. Also, this paper raises **Memory Access Cost(MAC)** to evaluate the performance of ShuffleNetv2 and performs a series of experiments to test. The experiments conclude four observations.  
**Conclusion①: Equal channel width minimizes memory access cost (MAC).**  
**Conclusion②: Excessive group convolution increases MAC.**  
**Conclusion③: Network fragmentation reduces the degree of parallelism.**  
**Conclusion④：Element-wise operations are non-negligible.**  
Therefore, ShuffleNet v2, based on these conclusions, improves the architecture. The building block of ShuffleNet v2 is shown below. Furthermore, the ShuffleNet v2 with SE/residual block is shown in the following. Channel Spit is used to improve the v1 version. Divide the channel into equal halves and take place groups to satisfy conclusions 1 and 2. Moreover, Shuffle is moved after the concatenation because of conclusion 3. Moreover, Concat replaces Element-wise because of conclusion 4.

![ShuffleNet v2 Block](/images/DailyPaper/02/113.png "ShuffleNet v2 Block")

![ShuffleNet v2 with SE/Residual](/images/DailyPaper/02/114.png "ShuffleNet v2 with SE/Residual")

### [120_ShuffleNet v2 Improvement_An efficient solution for semantic segmentation_ShuffleNet v2 with atrous seperable convolutions](http://link.springer.com/10.1007/978-3-030-20205-7_4)

- This paper combines ShuffleNetv2 with atrous separable convolution and depthwise convolution to efficiently improve the performance of ShuffleNev2 on the semantic segmentation task.

## Graph Neural Networks

### GNN

### [121_A Comprehensive Survey on Graph Neural Networks](http://arxiv.org/abs/1901.00596)
### [121_Deep Learning on Graphs_A Survey](http://arxiv.org/abs/1812.04202)
### [121_Graph neural networks_A review of methods and applications](https://arxiv.org/abs/1812.08434)

- These three papers are used to learn more about **Graph Neural Networks(GNN)**. As we have discussed so many CNN networks, why do we use CNN in the graph network? Because the image and frame in the video sequence have local translational invariance, the graph does not have this kind of invariance: Euclidean space versus non-Euclidean space. The comparison between the image and graph is shown below.

![The Comparison between Image and Graph](/images/DailyPaper/02/38.jpeg "The Comparison between Image and Graph")

- To handle the graph signals, the first step is using measured points as the vertexes. Second, the graph will be connected by edges and vertexes. Third, measure the signal of the graph to perform the local average or average with weights. Therefore, if we want to perform a convolutional network in the graph, some suggest **Graph Convolutional Network(GCN)**. This kind of convolution performs in the frequency domain since it is hard to perform in the space domain. Therefore, GCN-1(Deep Locally Connected Networks), GCN-2(ChbeyNet), and GCN-3 were proposed. A simple example of the spatial convolution operation is shown below. The input and output will all be graph structures.

![An Example of the Spatial Convolution Operation](/images/DailyPaper/02/39.jpeg "An Example of the Spatial Convolution Operation")
![An Example of the Spatial Convolution Operation](/images/DailyPaper/02/40.jpeg "An Example of the Spatial Convolution Operation")

### TCN

### [121_Temporal Convolutional Networks for Action Segmentation and Detection](https://arxiv.org/abs/1611.05267)
### [121_Temporal Convolutional Networks_A Unified Approach to Action Segmentation](https://arxiv.org/abs/1608.08242)
### [213_An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)

- These two papers are used to know more about **Temporal Convolution Networks(TCN)**. Why TCN? Firstly, CNN is hard to capture the temporal, and traditional convolution has the problem of leaking or missing the details. TCN uses **causal convolution and dilated** to solve these problems.

- The causal convolution pad the zeros in the original kernel and shift when performing the convolution. The diagram of causal convolution is shown below. Generally speaking, TCN is similar to FCN combined with causal convolutions at the beginning.

![Causal Convolution](/images/DailyPaper/02/43.jpeg "Causal Convolution")

- Since the causal convolution is adopted, dilated is needed to cover the input length to reduce the neural network. The details of dilated convolution have been mentioned in the previous blog part. The dilated TCN model is shown below.
  
![Dilated TCN Model](/images/DailyPaper/02/42.jpeg "Dilated TCN Model")

- If the residual block is added, an improved TCN could be represented below. The residual block is to help train the deeper network and extract features.
  
![TCN](/images/DailyPaper/02/41.jpeg "TCN")

## Vision Transformer

### ViT

### [122_ViT v1_Attention Is All You Need](http://arxiv.org/abs/1706.03762)

- This paper proposes revolutionalized **Transformer**, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Transformer uses stacked self-attention and pointwise, fully connected layers for both the encoder(Left part) and decoder(Right part). The architecture of the Transformer is shown below. It is easy to see that the whole structure is made up of **Multi-head Attention, Position Encoding, Feed Forward, Input/Output Embedding, and Add&Norm Layers.**

![Architecture of Transformer](/images/DailyPaper/02/127.png "Architecture of Transformer")

- The multi-head attention is shown below. This module is the core of the whole self-attention module. The basic idea is to map the original features into V, K, Q by three Fully Connected Layers and weight Q by calculating the similarity of V, K. The final output, in practice, is controlled by the three linear layers of the input. With such a structure, the attention process can have its own feature decision without relying on other input information. So such a structure is called a self-attention structure.

![Multi-head Attention](/images/DailyPaper/02/128.png "Multi-head Attention")  
Position Encoding indicates that the token is an input order message.  
Input/Output Embedding converts one hot token into a vector by a linear projection.  
The equation of Feed Forward is FFN(x) = max(0, x$W_1$ + $b_1$)$W_2$ + $b_2$, which is the architecture of two linear layers sandwiching a RELU layer.  
Add and Norm layers are similar to the ResNet structure. The original features and the features after attention are summed to retain the original information and facilitate the convergence of the model, and the features after attention are added to the new features to form a new feature by the norm. Therefore, the convergence of the model and the use of the learned self-attention features can be guaranteed to the maximum.  
So these are the component of the Transformer.

### [122_ViT v2_AN IMAGE IS WORTH 16X16 WORDS_TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/abs/2010.11929)

- This paper proposes **ViT v2** . The overview of the ViT model is shown below. The main improvements are **Spit the Images, class_token, Pretrain the data, and High-Resolution Finetune**.

![ViT Model Overview](/images/DailyPaper/02/129.png "ViT Model Overview")

- Split the images to convert the continuous image into tokens. Splitting the images is similar to the token in NLP.  
The class_token is added in the encoder's first position, and this feature is used as the classifier.  
Pretrain data in the large dataset is helpful to the overall Transformer's performance.  
High-resolution finetune is to increase the size of inputs in the small dataset and perform the finetune. The finetune will not affect the receptive field of the general Transformer and increase the number of tokens.  

### Swin Transformer

### [122_Swin Transformer V1_Hierarchical Vision Transformer using Shifted Windows](https://ieeexplore.ieee.org/document/9710580/)

- This paper proposes a new vision Transformer **Swin Transformer**, which utilizes the idea of divide-and-conquer. This model is proposed to be able to model NLP and CV models jointly. In this Transformer, the hierarchical structure is used to obtain the global information, and the shifted window method is applied to solve the information interaction. The hierarchical feature map is shown below.

![Hierarchical Feature Maps](/images/DailyPaper/02/115.png "Hierarchical Feature Maps")  
Shifted window approach is adopted. With this method, the complexity of the Swin Transformer decreases from O($N^2$)($(hw)^2$) to O(n)($M^2$\*hw). The shifted window approach is shown below. 

![Shifted Window](/images/DailyPaper/02/116.png "Shifted Window")

- The architecture of the Swin Transformer is shown below. Firstly, transform RGB pixel resolution images into patches resolution images and change the number of input channels according to the specific model size. Secondly, implement window self-attention and shifted window self-attention, and implement a hierarchical structure through patch merging to reduce computational complexity and enable processing of images of different scales. Finally, convert it into the corresponding output according to the task requirements. The number of Swin Transformers decreases, and the perceived range of each patch increases as the network depth increases.

![Architecture of the Swin Transformer](/images/DailyPaper/02/117.png "Architecture of the Swin Transformer")

### [122_Swin Transformer V2_Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)

- This paper proposes **Swin Transformer v2**. This paper is to solve three major issues in the training and application of large vision models, including training instability, resolution gaps between pretraining and finetuning, and hunger for labeled data. Furthermore, the three main technologies proposed are **(1) a residual-post-norm method combined with cosine attention to improving training stability; (2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; (3) A self-supervised pretraining method, SimMIM, to reduce the needs of vast labeled images.** The adaptation modified in Swin Transformer v1 is shown below.

![Adaptation modified in the Swin Transformer V1](/images/DailyPaper/02/118.png "Adaptation modified in the Swin Transformer V1")

- To solve the problem of the small-scale model, the paper uses post-norm, which moves the Layer Norm layer in the Transformer block of the previous generic ViT in the front of the Attention layer to the back. The advantage of this is that the output is normalized after the computation of the attention, and the output is stabilized. Also, scaled cosine similarity is adopted to stable the Attention output.

- To solve the problem of model mismatch, which represents the window and training resolution, continuous relative position bias and log-spaced coordinates are utilized. Continuous relative position bias makes the computation more flexible. Log-spaced coordinates change the linear transform to the log transform, efficiently decreasing the extrapolation and limiting the similarity to (0, 1). The equation is $\Delta \hat{x} = sign(x) \cdot ln(1 + abs(\Delta x))$, $\Delta \hat{y} = sign(y) \cdot ln(1 + abs(\Delta y))$. In the following experiment, the Swin Transformer V2 is very successful. A series of works on the Swin Transformer aims to make it a general model for vision.

### ViTAE

### [122_ViTAE_Vision Transformer Advanced by Exploring Intrinsic Inductive Bias](http://arxiv.org/abs/2106.03348)

- This paper proposes a new **Vision Transformer Advanced by Exploring intrinsic IB from convolutions(ViTAE)**, combining CNN and ViT by using CNN in the shallow layer and ViT in the deep layer. The structure of ViTAE is shown below. RC means Reduction Cell, and NC means Normal Cell. In RC, a Pyramid Reduction Module(PRM), which consists of different dilated convolutions with different dilated rates, is added compared with the Transformer block of ViT. Also, the PRM's output feature is the length and input of the Attention module. In NC, a convolution branch is added to calculate attention. ViTAE is a new step in the transformer.

![Structure of ViTAE](/images/DailyPaper/02/56.jpeg "Structure of ViTAE")

### [122_ViTAEv2_Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond](http://arxiv.org/abs/2202.10108)

- This paper raises **ViTAE v2** which has an enormous parameters(644M) by scaling up. In many experiments, ViTAE receives SOTA results. In the ablation experiment part, some mechanisms' effects are discussed. In the future, the ViTAE will be improved by decreasing the training consumption and designing a better model structure.
