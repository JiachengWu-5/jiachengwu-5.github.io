---
layout: post
title: Daily Paper (2023.1.3 - 2023.1.22)
categories: [Daily Paper]
description: Classical CV papers
keywords: Computer Vision
---

## Convolution

### [14_Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)

- This paper introduces a novel way to visualize the Convolutional Network. To realize this purpose, three key methods are used: **Unpooling, Rectification(ReLU) and Filtering**
- 
Vital factor to the stability and performance of the network

### [14_Xception_ Deep Learning with Depthwise Separable Convolutions](http://arxiv.org/abs/1610.02357)

### [14_Learning Deconvolution Network for Semantic Segmentation](http://arxiv.org/abs/1505.04366)

### [15_Dilated Residual Networks](http://ieeexplore.ieee.org/document/8099558/)
- **Dilated Residual Network (DRN)** is rose to copy with the problem of the loss of spatial acuity caused by reducing resolution. The other methods to keep the high resolution are *up-convolutions, skip connections, and other post-hoc measure.* This paper improves the resolution of input image of ResNet by 4 times. The comparison between ResNet and DRN architecture is shown below.

![DRN](/images/DailyPaper/02/3.jpeg "ResNet and DRN")  
The details of Dilated Convolution is shown below.

![Dilated Convolution](/images/DailyPaper/02/1.gif "Dilated Convolution")

- However, **Degridding** will appear after using DRN. Gridding artifacts occur when a feature map has higher-frequency content than the sampling rate of the dilated convolution. The example of a gridding artifact is shown below. To handle with this problem, the structure of DRN is modified.

![A Gridding Artifact](/images/DailyPaper/02/5.jpeg "A Gridding Artifact")

- In DRN-B and DRN-C, the pooling layer is removed since the max pooling operation leads to high-amplitude high-frequency activations. Besides, a 2-dilated residual block followed by a 1-dilated block is used as the layer-7 and layer-8 of DRN-B and DRN-C. And DRN-C removes the residual connections. The details of DRN-A, DRN-B and DRN-C are shown below. In the experiemnt section, DRN-C achieves the best performance. The visualization of DRN-C is also the best.

![Improved DRN](/images/DailyPaper/02/4.jpeg "Improved DRN Structure")

### [15_Deformable Convolutional Networks](https://arxiv.org/abs/1703.06211)

- This paper proposes **Deformable Convolution and Deformable RoI Pooling** to enhance the transformation modeling capabiliy of CNNs. The resulting CNNs are called *Deformable Convolutional Networks or Deformable ConvNets*

- Deformable Convolution adds 2D offsets to the rgular sampling locations in the standard convolution. The details of deformable convolution are shown below. The offset contributes to the irregular convolution shape. Therefore, the convolution kernel will fit with the shape and size of objection to increase the accuracy of the network.

![Deformable Convolution](/images/DailyPaper/02/9.jpeg "Deformable Convolution")

![3x3 Deformable Convolution](/images/DailyPaper/02/8.jpeg "3x3 Deformable Convolution")

- Deformable RoI Pooling adds an offset to each bin position in the regular bin partition of the previous RoI pooling. The details of deformable RoI Pooling are shown below. The position-sentitive RoI pooling adopts a specific positive-sensitive score map $x_{i,j}$ to obtain the real offset $\Delta p_{i,j}$.

![3x3 Deformable RoI Pooling](/images/DailyPaper/02/6.jpeg "3x3 Deformable RoI Pooling")

![3x3 Deformable Position-Sensitive RoI Pooling](/images/DailyPaper/02/7.jpeg "3x3 Deformable Position-Sensitive RoI Pooling")

- In the following experiments, deformable convolution is added in different location of ResNet. Besides, big and small objects are used to test and validation. What's more, this paper points that **deformable convolution is a generalization of dilated convolution**. The improvement in performance is not very significant, and the major improvement is based on the methods.(The performance improves in the Deformable ConvNets v2.)

### [29_Deformable ConvNets v2_More Deformable, Better Results](https://arxiv.org/pdf/1811.11168.pdf)

## Classical Models

### [29_LeNet_Gradient-Based Learning Applied to Document Recognition](https://ieeexplore.ieee.org/document/726791)

- **"Hello World"**

- The resulting effect of this network is show below. This gif is cited from *[LeNet-5, convolutional neural networks]*(http://yann.lecun.com/exdb/lenet/)

![LeNet](/images/DailyPaper/02/10.gif "LeNet")

-

### [15_AlexNet_Imagenet-classification-with-deep-convolutional-neural-networks-Paper](https://dl.acm.org/doi/10.1145/3065386)

- **AlexNet** is the classical network. In this paper, **Dropout** is proposed to reduce overfutting in the fully-connected layers. The traning and testing dataset is *ImageNet LSVRC-2010*. Also, **Rectified Linear Units (ReLUs)** are applied to improve the speed of network. AlexNet's structure is similar to LeNet. The overall architecture of AlexNet is shown below. It consists of 5 convolution layers and 3 fully connected layers. Because of the huge calculate parameters, 2 GPUs are used to train in parallel.

![Dilated Convolution](/images/DailyPaper/02/2.jpeg "AlexNet")

- What's more, **Local Response Normalization (LPN)** is used to increase the generalization ability. The equation of LPN is given by:
$b_{x,y}^i=a_{x,y}^i/(k=\alpha \sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(\alpha_{x,y}^i)^2)^\beta$
What's more, **Overlapping Pooling** is applied to decrease the top-1 and top-5 errors.

- To reduce the overfiting problem, two methods are applied. The first one is **Data Augmentation**. This method has two forms: *generate image translations and horizontal reflections, alter the intensities of the RGB channels in training images.* The second method is the **Dropout** layer. This layer randomly ignores some neural to avoid the overfitting problem.

### [16_VGGNet_VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](http://arxiv.org/abs/1409.1556)

- This paper focuses on the effect of network depth. The dataset used to test is *ImageNet Challenge 2014*.

- VGGNet

LRN is not helpful
Smaller conv kernel is more useful
conv 1*1 is a kind of non-linarity shift
Depth of conv net is very critical

### [16_ZFNet_Visualizing and Understanding Convolutional Networks](http://arxiv.org/abs/1311.2901/)

### [17_Inceptionv1_GoogLeNet_Going deeper with convolutions](http://arxiv.org/abs/1409.4842)

### [17_Inceptionv2_Batch Normalization_Accelerating Deep Network Training](http://arxiv.org/abs/1502.03167)

- The difference between LPN and BN is that, https://www.jianshu.com/p/ef689144c86e

### [17_Inceptionv3_Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)

### [17_Inceptionv4, Inception-ResNet and the impact of residual connections on Learning](http://arxiv.org/abs/1602.07261)

### [18_ResNet_Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385)

### [18_ResNeXt_Aggregated Residual Transformations for Deep Neural Networks](http://ieeexplore.ieee.org/document/8100117/)

### [19_DenseNet_Densely Connected Convolutional Networks](https://ieeexplore.ieee.org/document/8099726/)

### [19_HRNet v1_Deep High-Resolution Representation Learning](http://arxiv.org/abs/1908.07919)

### [19_HRNet v2_High-Resolution Representations for Labeling Pixels and Regions](http://arxiv.org/abs/1904.04514)

### [19_SENet_Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)

### [110_CSPNET A NEW BACKBONE THAT CAN ENHANCE LEARNING](http://arxiv.org/abs/1911.11929)

### [110_EfficientNet Rethinking Model Scaling for Convolutional Neural Networks](http://arxiv.org/abs/1905.11946)

## Image Classification

### [111_Highway Networks](http://arxiv.org/abs/1505.00387)

### [111_Application of Highway Network_Training Very Deep Networks](http://arxiv.org/abs/1507.06228)

### [111_PReLu_Delving Deep into Rectifiers_Surpassing Human-Level Performance on ImageNet Classification](http://ieeexplore.ieee.org/document/7410480/)

## Image Segmentation

### [112_FCN_Fully Convolutional Networks for Semantic Segmentation](http://arxiv.org/abs/1411.4038)

### [112_FPN_Feature Pyramid Networks for Object Detection](http://arxiv.org/abs/1612.03144)

### [112_U-Net_Convolutional Networks for Biomedical](http://arxiv.org/abs/1505.04597)

## GAN

### [112_GAN_Generative Adversarial Nets](http://arxiv.org/abs/1406.2661)

## Object Detection

### [113_Selective Search for Object Recognition](http://link.springer.com/10.1007/s11263-013-0620-5)

### [113_OverFeat_Integrated Recognition, Localization and Detection using Convolutional Networks](http://arxiv.org/abs/1312.6229)

### [114_R-CNN_Rich feature hierarchies for accurate object detection and semantic segmentation](http://arxiv.org/abs/1311.2524)

### [114_SPPNet_Spatial Pyramid Pooling in Deep Convolutional](http://arxiv.org/abs/1406.4729)

### [114_Fast R-CNN](http://arxiv.org/abs/1504.08083)

### [114_Faster R-CNN_Towards Real-Time Object Detection with Region Proposal Networks](http://arxiv.org/abs/1506.01497)

### [114_Mask R-CNN](http://arxiv.org/abs/1703.06870) (This paper focuses on Image Segmentation)

### [115_R-FCN_Object Detection via Region-based Fully Convolutional Networks](http://arxiv.org/abs/1605.06409)

### [115_SSD_Single Shot MultiBox Detector](http://arxiv.org/abs/1512.02325)

### [115_YOLOv1_You Only Look Once_Unified Real-Time Object Detection](http://arxiv.org/abs/1506.02640)

### [116_YOLOv2_YOLO9000_Better, Faster, Stronger](http://arxiv.org/abs/1612.08242)

### [116_YOLOv3_An Incremental Improvement](http://arxiv.org/abs/1804.02767)

### [116_YOLOv4_Optimal Speed and Accuracy of Object Detection](http://arxiv.org/abs/2004.10934)

### [117_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_Detection on Drone-Captured Scenarios](https://ieeexplore.ieee.org/document/9607487/)

### [117_YOLO v5_Improved YOLOv5 network for real-time multi-scale traffic sign detection](https://link.springer.com/10.1007/s00521-022-08077-5)

### [117_YOLOV_ Making Still Image Object Detectors Great at Video Object Detection](http://arxiv.org/abs/2208.09686)

### [118_PP-YOLOE_An evolved version of YOLO](http://arxiv.org/abs/2203.16250)

### [118_YOLOX_Exceeding YOLO Series in 2021](http://arxiv.org/abs/2107.08430)

### [118_YOLO-Z_Improving small object detection in YOLOv5 for autonomous vehicles](http://arxiv.org/abs/2112.11798)

## Light Networks

### [119_MobileNets_Efficient Convolutional Neutral Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)

### [119_MobileNetV2_Inverted Residuals and Linear Bottlenecks](https://ieeexplore.ieee.org/document/8578572/)

### [119_MobileNetv3_Searching for MobileNetV3](https://ieeexplore.ieee.org/document/9008835/)

### [120_ShuffleNet_An Extremely Efficient Convolutional Neural Network for Mobile Device](https://arxiv.org/abs/1707.01083)

### [120_ShuffleNet V2_Practical Guidelines for Efficient CNN Architecture Design](https://link.springer.com/10.1007/978-3-030-01264-9_8)

### [120_ShuffleNet v3_An efficient solution for semantic segmentation_ShuffleNet v2 with atrous seperable convolutions](http://link.springer.com/10.1007/978-3-030-20205-7_4)

## Graph Neural Networks

### [121_A Comprehensive Survey on Graph Neural Networks](http://arxiv.org/abs/1901.00596)

### [121_Deep Learning on Graphs_A Survey](http://arxiv.org/abs/1812.04202)

### [121_Graph neural networks_A review of methods and applications](https://arxiv.org/abs/1812.08434)

### [121_Temporal Convolutional Networks for Action Segmentation and Detection](https://arxiv.org/abs/1611.05267)

### [121_Temporal Convolutional Networks_A Unified Approach to Action Segmentation](https://arxiv.org/abs/1608.08242)

## Vision Transformer

### [122_ViT v1_Attention Is All You Need](http://arxiv.org/abs/1706.03762)

### [122_ViT v2_AN IMAGE IS WORTH 16X16 WORDS_TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/abs/2010.11929)

### [122_Swin Transformer V1_Hierarchical Vision Transformer using Shifted Windows](https://ieeexplore.ieee.org/document/9710580/)

### [122_Swin Transformer V2_Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883)

### [122_ViTAE_Vision Transformer Advanced by Exploring Intrinsic Inductive Bias](http://arxiv.org/abs/2106.03348)

### [122_ViTAEv2_Vision Transformer Advanced by Exploring Inductice Bias for Image Recognition and Beyond](http://arxiv.org/abs/2202.10108)