---
layout: post
title: Daily Paper (2023.1.23 - 2023.2.10)
categories: [Daily Paper]
description: Some interesting papers
keywords: Computer Vision
---

### [210_Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)

- This paper proposes a plug-and-play module **Spatial Transformer** to handle with the spatial manipulation of data within the network. As mentioned in the previous blog, the CNN lacks of ability to be spatially invariant to the input data. This module is proposed to copy with this problem. This module has the invariance of translation, scaling and rotation. CNNs with spatial transformers can perform better in some multiple tasks, like image classification, co-localisation, and spatial attention.

- The spatial transformer is made up of three parts, **Localisation Network, Parameterised Sampling Grid and Differentiable Image Sampling**.  
Localisation network takes the input feature map U $\in R^{H\times W \times C}$ and outputs $\theta=f_{loc}(U)$. Generally speaking, $f_{loc}()$ can be any form which should finally include a final regression layer to produce the transformation parameters $\theta$.  
Parameterised Sampling Grid aims to obtain the corresponding output coordinations to input coordinations. ($x_i^s$; $y_i^s$)'(Source coordinates in the input feature map) = $T_\theta(G_i)$ = $A_\theta$(Affine transformation matrix) $(x_i^t; y_i^t; 1)'$(Target coordinates of the regular grid in the output feature map) = $[\theta_{11} \theta_{12} \theta_{13}; \theta_{21} \theta_{22} \theta_{23}]$($x_i^t; y_i^t; 1$). This part provides the corresponding location of output for the next part. The examples of this part is shown below.

![Examples of Parameterised Sampling Grid](/images/DailyPaper/03/1.jpg "Examples of Parameterised Sampling Grid")  

Differentiable Image Sampling calculated the gray value of corresponding points in the expected sampling kernel. For example, the bilinear sampling kernel is: $V_i^c=\sum_n^H \sum_m^W U_{nm}^c max(0, 1-|x_i^s - m|) max(0, 1-|y_i^s - n|)$  
The combination of these three part forms the complete spatial transformer, the overall architecture is shown below. The following experiment performs on MNIST dataset and is tested based on FCN and CNN. Finally the ST-CNN receive SOTA results.

![Architecture of Spatial Transformer](/images/DailyPaper/03/2.jpg "Architecture of Spatial Transformer")

### [211_Vision Transformer with Deformable Attention](https://arxiv.org/pdf/2201.00520.pdf)