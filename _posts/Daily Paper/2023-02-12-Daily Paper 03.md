---
layout: post
title: Daily Paper (2023.1.23 - 2023.2.12)
categories: [Daily Paper]
description: Some interesting papers
keywords: Computer Vision
---

### [123_Attend Refine Repeat_Active Box Proposal Generation via In-Out Localization](http://arxiv.org/abs/1606.04446)

### [123_Cascade R-CNN_Delving into High Quality Object Detection](http://arxiv.org/abs/1712.00726)

### [123_FaceNet_A Unified Embedding for Face Recognition and Clustering](http://arxiv.org/abs/1503.03832)

### [123_Focal Loss for Dense Object Detection](http://arxiv.org/abs/1708.02002)

### [124_DeepLung_Deep 3D Dual Path Nets for Antomated Pulmonary Nodule Detection and Classification](http://arxiv.org/abs/1801.09555)

### [124_Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy or Network](https://ieeexplore.ieee.org/document/8642524/)

### [124_Lung Nodule Classification using Deep Local-Global Networks](http://link.springer.com/10.1007/s11548-019-01981-7)

### [125_Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer](https://ieeexplore.ieee.org/document/8737963/)

### [125_DIAGNOSTIC CLASSIFICATION OF LUNG NODULES USING 3D NEURAL NETWORKS](https://ieeexplore.ieee.org/document/8363687/)

### [125_Gated-Dilated Networks for Lung Nodule Classification in CT scans](https://ieeexplore.ieee.org/document/8930524/)

### [126_Identity Mappings in Deep Residual Networks](http://arxiv.org/abs/1603.05027)

### [126_SCA-CNN Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning](http://ieeexplore.ieee.org/document/8100150/)

### [127_Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks](https://ieeexplore.ieee.org/document/8578213/)

### [128_CROSSFORMER_A VERSATILE VISION TRANSFORMER HINGING ON CROSS_SCALE ATTENTION](https://arxiv.org/abs/2108.00154)

### [129_Classification-Then-Grounding_rEFORMULATING vIDEO sCENE gRAPHS AS tEMPORAL bIPARTITE gRAPHS](https://ieeexplore.ieee.org/document/9879749/)

### [129_Rethinking the Two-Stage Framework for Grounded Situation Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/20167)

### [130_On Pursuit of Designing Multi-modal Transformer for Video Grounding](https://aclanthology.org/2021.emnlp-main.773)

### [131_AttentionGAN_ Unpaired Image-to-Image Translation using Attention-Guided Generatice Adversarial Networks](http://arxiv.org/abs/1911.11897)

### [131_PAD-Net_Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing_paper](https://ieeexplore.ieee.org/document/8578175/)

### [21_Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://ieeexplore.ieee.org/document/9879781/)

### [21_DetCLIP_ Dictionary-Enriched Visual-Concept Paralleled Pre-Traing for Open-World Detection](http://arxiv.org/abs/2209.09407)

### [21_InvPT_Inverted Pyramid Multi-task Transformer for Dense Scene Understanding](http://arxiv.org/abs/2203.07997)

### [21_Multi-class Token Transformer for Weakly Supervised Semantic Segmentation](https://ieeexplore.ieee.org/document/9879800/))

### [22_Generalized Binary Search Network for Highly-Efficient Multi-View Stereo](https://ieeexplore.ieee.org/document/9880099/)

### [23_Efficient_Fused-Attention_Model_for_Steel_Surface_Defect_Detection](https://ieeexplore.ieee.org/document/9777969/)

### [23_Geometry-Aware_Facial_Expression_Recognition_via_Attentive_Graph_Convolutional_Networks](https://ieeexplore.ieee.org/document/9454388/)

### [23_IEEE_EMBS_Webinar_Series](https://sites.google.com/view/ieee-biip-webinars/)

### [23_Joint_Spine_Segmentation_and_Noise_Removal_From_Ultrasound_Volume_Projection_Images_With_Selective_Feature_Sharing](https://ieeexplore.ieee.org/document/9684363/)

### [24_Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search](http://arxiv.org/abs/2002.08718)

### [25_Fully Convolutional Networks for Panoptic Segmentation](http://arxiv.org/abs/2012.00720)

### [26_ConDinst_Conditional Convolutions for Instance Segmentation](http://arxiv.org/abs/2003.05664)

### [26_Rethink before Releasing your Model_ML Model Extraction Attack in EDA](https://dl.acm.org/doi/10.1145/3566097.3567896)

### [27_3D-Pruning_A_Model_Compression_Framework_for_Efficient_3D_Action_Recognition](https://ieeexplore.ieee.org/document/9852466/)

### [27_APSNet_Toward_Adaptive_Point_Sampling_for_Efficient_3D_Action_Recognition](https://ieeexplore.ieee.org/document/9844448/)

### [28_Extracting Training Data from Large Language Models](http://arxiv.org/abs/2012.07805)

### [28_Optical Convolutional Neural Networks â€“ Combining Silicon Photonics and Fourier Optics for Computer Vision](http://arxiv.org/abs/2108.05607)

### [29_Deformable ConvNets v2_More Deformable, Better Results](http://arxiv.org/abs/1811.11168)

### [29_LeNet_Gradient-based_learning_applied_to_document_recognition](http://ieeexplore.ieee.org/document/726791/)

### [210_Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)

- This paper proposes a plug-and-play module **Spatial Transformer** to handle with the spatial manipulation of data within the network. As mentioned in the previous blog, the CNN lacks of ability to be spatially invariant to the input data. This module is proposed to copy with this problem. This module has the invariance of translation, scaling and rotation. CNNs with spatial transformers can perform better in some multiple tasks, like image classification, co-localisation, and spatial attention.

- The spatial transformer is made up of three parts, **Localisation Network, Parameterised Sampling Grid and Differentiable Image Sampling**.  
Localisation network takes the input feature map U $\in R^{H\times W \times C}$ and outputs $\theta=f_{loc}(U)$. Generally speaking, $f_{loc}()$ can be any form which should finally include a final regression layer to produce the transformation parameters $\theta$.  
Parameterised Sampling Grid aims to obtain the corresponding output coordinations to input coordinations. ($x_i^s$; $y_i^s$)'(Source coordinates in the input feature map) = $T_\theta(G_i)$ = $A_\theta$(Affine transformation matrix) $(x_i^t; y_i^t; 1)'$(Target coordinates of the regular grid in the output feature map) = $[\theta_{11} \theta_{12} \theta_{13}; \theta_{21} \theta_{22} \theta_{23}]$($x_i^t; y_i^t; 1$). This part provides the corresponding location of output for the next part. The examples of this part is shown below.

![Examples of Parameterised Sampling Grid](/images/DailyPaper/03/1.jpg "Examples of Parameterised Sampling Grid")  

Differentiable Image Sampling calculated the gray value of corresponding points in the expected sampling kernel. For example, the bilinear sampling kernel is: $V_i^c=\sum_n^H \sum_m^W U_{nm}^c max(0, 1-|x_i^s - m|) max(0, 1-|y_i^s - n|)$  
The combination of these three part forms the complete spatial transformer, the overall architecture is shown below. The following experiment performs on MNIST dataset and is tested based on FCN and CNN. Finally the ST-CNN receive SOTA results.

![Architecture of Spatial Transformer](/images/DailyPaper/03/2.jpg "Architecture of Spatial Transformer")

### [211_Vision Transformer with Deformable Attention](https://arxiv.org/pdf/2201.00520.pdf)

- This paper proposes **Deformable Attention Transformer(DAT)** which combines transformer with deformable module. DAT is the deformable self-attention backbone for visual recognition. The reason why I chose this paper is that this idea is similar to the thinking when I read the deformable convolution paper. But it is a pity that some people had produced the successful model. The receptive fields and queries of ViT, Swin Transformer, DCN and DAT are below. It can be seen that DAT merges the characteristics of ViT and DCN.

![ViT, Swin Transformer, DCN and DAT](/images/DailyPaper/03/3.png "ViT, Swin Transformer, DCN and DAT")

- The Deformable Attention Mechanism is shown below. In part (a), a set of reference points uniformly distributed on the feature map is used. Then the offset values are learned by an offset network in part(b) and the offset is applied to the reference points. After that, bilinear interpolation is applied to find the offset coordinates on the input feature map. A small relevant part of the feature map is extracted as the original source of k and v using a bilinear interpolation from the nearest four points around the weighted average. The detail equations could be learned from the paper.

![Deformable Attention Mechanism](/images/DailyPaper/03/4.png "Deformable Attention Mechanism")

- The architecture of DAT is shown below. DAT achieves the better performance than swin transformer in the following experiment. In the ablation part, the deformable attention only in third and fourth stage will result in the better performance than Swin Transformer.
  
![Architecture of DAT](/images/DailyPaper/03/5.png "Architecture of DAT")

### [212_MOSE: A New Dataset for Video Object Segmentation in Complex Scenes](https://arxiv.org/abs/2302.01872)

- Dataset: **comMplex video Object SEgmentation (MOSE)**.  
  This dataset contains the complex classification and recognition tasks. For example the disapperance-reapperance of objects, small/inconspicuous object, heavy occlusions, crowded environments. This paper creates MOSE dataset and benchmarks 18 existing Video Object Segmentation(VOS) methods under 4 different settings on MOSE dataset to perform the comparison experiments. The details will not be discussed here, but there is one thing for certain: the MOSE dataset is new challenge for the current video object segmentation methods because of the poor performance of the existing methods. As mentioned in the paper, *'We find that we are still at a nascent stage of segmenting and tracking objects in complex scenes where crowds, disappearing, occlusions, and inconspicuous/small objects occur frequently'*.

- Moreover, this paper proposes five potential future directions: **Stronger Association to Track Reappearing Objects, Video Object Segmentation of Occluded Objects, Attention on Small & Inconspicuous Objects, Tracking Objects in Crowd, and Long-Term Video Segmentation**. I want to solve some of them if I have the chance in the future.

### [213_213_SWITCH-NERF_LEARNING SCENE DECOMPOSITION WITH MIXTURE OF EXPERTS FOR LARGE-SCALE NEURAL RADIANCE FIELDS ](https://openreview.net/forum?id=PQ2zoIZqvm)
