---
layout: post
title: Daily Paper (2023.1.23 - 2023.?.?)
categories: [Updating]
description: Some interesting papers
keywords: Computer Vision, Spatial Transformer, DAT, MOSE, Switch-NeRF, MRM, ToMe, HexPlane, Simple Multi-Scale Attention, Size-Adaptive Local Attention, PaLI, PDB-ConvLSTM
---

### [123_Attend Refine Repeat_Active Box Proposal Generation via In-Out Localization](http://arxiv.org/abs/1606.04446)

### [123_Cascade R-CNN_Delving into High Quality Object Detection](http://arxiv.org/abs/1712.00726)

### [123_FaceNet_A Unified Embedding for Face Recognition and Clustering](http://arxiv.org/abs/1503.03832)

### [123_Focal Loss for Dense Object Detection](http://arxiv.org/abs/1708.02002)

### [124_DeepLung_Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification](http://arxiv.org/abs/1801.09555)

### [124_Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy or Network](https://ieeexplore.ieee.org/document/8642524/)

### [124_Lung Nodule Classification using Deep Local-Global Networks](http://link.springer.com/10.1007/s11548-019-01981-7)

### [125_Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer](https://ieeexplore.ieee.org/document/8737963/)

### [125_DIAGNOSTIC CLASSIFICATION OF LUNG NODULES USING 3D NEURAL NETWORKS](https://ieeexplore.ieee.org/document/8363687/)

### [125_Gated-Dilated Networks for Lung Nodule Classification in CT scans](https://ieeexplore.ieee.org/document/8930524/)

### [126_Identity Mappings in Deep Residual Networks](http://arxiv.org/abs/1603.05027)

### [126_SCA-CNN Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning](http://ieeexplore.ieee.org/document/8100150/)

### [127_Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks](https://ieeexplore.ieee.org/document/8578213/)

### [128_CROSSFORMER_A VERSATILE VISION TRANSFORMER HINGING ON CROSS_SCALE ATTENTION](https://arxiv.org/abs/2108.00154)

### [129_Classification-Then-Grounding_rEFORMULATING vIDEO sCENE gRAPHS AS tEMPORAL bIPARTITE gRAPHS](https://ieeexplore.ieee.org/document/9879749/)

### [129_Rethinking the Two-Stage Framework for Grounded Situation Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/20167)

### [130_On Pursuit of Designing Multi-modal Transformer for Video Grounding](https://aclanthology.org/2021.emnlp-main.773)

### [131_AttentionGAN_ Unpaired Image-to-Image Translation using Attention-Guided Generatice Adversarial Networks](http://arxiv.org/abs/1911.11897)

### [131_PAD-Net_Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing_paper](https://ieeexplore.ieee.org/document/8578175/)

### [21_Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://ieeexplore.ieee.org/document/9879781/)

### [21_DetCLIP_ Dictionary-Enriched Visual-Concept Paralleled Pre-Traing for Open-World Detection](http://arxiv.org/abs/2209.09407)

### [21_InvPT_Inverted Pyramid Multi-task Transformer for Dense Scene Understanding](http://arxiv.org/abs/2203.07997)

### [21_Multi-class Token Transformer for Weakly Supervised Semantic Segmentation](https://ieeexplore.ieee.org/document/9879800/))

### [22_Generalized Binary Search Network for Highly-Efficient Multi-View Stereo](https://ieeexplore.ieee.org/document/9880099/)

### [23_Efficient_Fused-Attention_Model_for_Steel_Surface_Defect_Detection](https://ieeexplore.ieee.org/document/9777969/)

### [23_Geometry-Aware_Facial_Expression_Recognition_via_Attentive_Graph_Convolutional_Networks](https://ieeexplore.ieee.org/document/9454388/)

### [23_IEEE_EMBS_Webinar_Series](https://sites.google.com/view/ieee-biip-webinars/)

### [23_Joint_Spine_Segmentation_and_Noise_Removal_From_Ultrasound_Volume_Projection_Images_With_Selective_Feature_Sharing](https://ieeexplore.ieee.org/document/9684363/)

### [24_Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search](http://arxiv.org/abs/2002.08718)

### [25_Fully Convolutional Networks for Panoptic Segmentation](http://arxiv.org/abs/2012.00720)

### [26_ConDinst_Conditional Convolutions for Instance Segmentation](http://arxiv.org/abs/2003.05664)

### [26_Rethink before Releasing your Model_ML Model Extraction Attack in EDA](https://dl.acm.org/doi/10.1145/3566097.3567896)

### [27_3D-Pruning_A_Model_Compression_Framework_for_Efficient_ 3D_Action_Recognition](https://ieeexplore.ieee.org/document/9852466/)

### [27_APSNet_Toward_Adaptive_Point_Sampling_for_Efficient_3D _Action_Recognition](https://ieeexplore.ieee.org/document/9844448/)

### [28_Extracting Training Data from Large Language Models](http://arxiv.org/abs/2012.07805)

### [28_Optical Convolutional Neural Networks â€“ Combining Silicon Photonics and Fourier Optics for Computer Vision](http://arxiv.org/abs/2108.05607)

### [29_Deformable ConvNets v2_More Deformable, Better Results](http://arxiv.org/abs/1811.11168)

- This paper was introduced in the previous blog.

### [29_LeNet_Gradient-based_learning_applied_to_document_recognition](http://ieeexplore.ieee.org/document/726791/)

- This paper was introduced in the previous blog.

### [210_Spatial Transformer Networks](https://arxiv.org/abs/1506.02025)

- This paper proposes a plug-and-play module, **Spatial transformer**, to handle the spatial manipulation of data within the network. As mentioned in the previous blog, CNN cannot be spatially invariant to the input data. This module is proposed to cope with this problem. This module has the invariance of translation, scaling, and rotation. CNNs with spatial transformers can perform better in multiple tasks, like image classification, co-localization, and spatial attention.

- The spatial transformer comprises three parts, **Localisation Network, Parameterised Sampling Grid, and Differentiable Image Sampling**.  
Localisation network takes the input feature map U $\in R^{H\times W \times C}$ and outputs $\theta=f_{loc}(U)$. Generally speaking, $f_{loc}()$ can be any form that should include a final regression layer to produce the transformation parameters $\theta$.  
Parameterized Sampling Grid aims to obtain the corresponding output coordinations to input coordinations. ($x_i^s$; $y_i^s$)'(Source coordinates in the input feature map) = $T_\theta(G_i)$ = $A_\theta$(Affine transformation matrix) $(x_i^t; y_i^t; 1)'$(Target coordinates of the regular grid in the output feature map) = $[\theta_{11} \theta_{12} \theta_{13}; \theta_{21} \theta_{22} \theta_{23}]$($x_i^t; y_i^t; 1$). This part provides the corresponding location of output for the next part. The examples of this part are shown below.

![Examples of Parameterised Sampling Grid](/images/DailyPaper/03/1.jpg "Examples of Parameterised Sampling Grid")  

Differentiable Image Sampling calculated the gray value of corresponding points in the expected sampling kernel. For example, the bilinear sampling kernel is: $V_i^c=\sum_n^H \sum_m^W U_{nm}^c max(0, 1-|x_i^s - m|) max(0, 1-|y_i^s - n|)$  
The combination of these three parts forms the complete spatial transformer, and the overall architecture is shown below. The following experiment is performed on the MNIST dataset and is tested based on FCN and CNN. Finally, the ST-CNN received SOTA results.

![Architecture of Spatial Transformer](/images/DailyPaper/03/2.jpg "Architecture of Spatial Transformer")

### [211_Vision Transformer with Deformable Attention](https://arxiv.org/pdf/2201.00520.pdf)

- This paper proposes **Deformable Attention Transformer(DAT)** which combines transformer with deformable module. DAT is the deformable self-attention backbone for visual recognition. I chose this paper because this idea is similar to the thinking when I read the deformable convolution paper. However, it is a pity that some people have produced a successful model. The receptive fields and queries of ViT, Swin Transformer, DCN, and DAT are below. It can be seen that DAT merges the characteristics of ViT and DCN.

![ViT, Swin Transformer, DCN and DAT](/images/DailyPaper/03/3.png "ViT, Swin Transformer, DCN and DAT")

- The Deformable Attention Mechanism is shown below. In part (a), a set of reference points uniformly distributed on the feature map is used. Then the offset values are learned by an offset network in part(b), and the offset is applied to the reference points. After that, bilinear interpolation is applied to find the offset coordinates on the input feature map. A small relevant part of the feature map is extracted as the source of k and v using a bilinear interpolation from the nearest four points around the weighted average. The detailed equations could be learned from the paper.

![Deformable Attention Mechanism](/images/DailyPaper/03/4.png "Deformable Attention Mechanism")

- The architecture of DAT is shown below. DAT achieves better performance than the Swin transformer in the following experiment. In the ablation part, the deformable attention only in the third and fourth stages will perform better than Swin Transformer.
  
![Architecture of DAT](/images/DailyPaper/03/5.png "Architecture of DAT")

### [212_MOSE: A New Dataset for Video Object Segmentation in Complex Scenes](https://arxiv.org/abs/2302.01872)

- Dataset: **comMplex video Object SEgmentation (MOSE)**.  
This dataset contains complex classification and recognition tasks, such as the disapperance-reapperance of objects, small/inconspicuous objects, heavy occlusions, and crowded environments. This paper creates the MOSE dataset and benchmarks 18 existing Video Object Segmentation(VOS) methods under four settings on the MOSE dataset to perform the comparison experiments. The details will not be discussed here, but there is one thing for sure: the MOSE dataset is a new challenge for the current video object segmentation methods because of the poor performance of the existing methods. As mentioned in the paper, *'We find that we are still at a nascent stage of segmenting and tracking objects in complex scenes where crowds, disappearing, occlusions, and inconspicuous/small objects occur frequently'*.

- Moreover, this paper proposes five potential future directions: **Stronger Association to Track Reappearing Objects, Video Object Segmentation of Occluded Objects, Attention on Small & Inconspicuous Objects, Tracking Objects in Crowd, and Long-Term Video Segmentation**. I want to solve some of them.

### [213_SWITCH-NERF_LEARNING SCENE DECOMPOSITION WITH MIXTURE OF EXPERTS FOR LARGE-SCALE NEURAL RADIANCE FIELDS ](https://openreview.net/forum?id=PQ2zoIZqvm)

- This paper proposes **Switch-NeRF** to handle the generalization, poor-learnable, and inconsistency problems. NeRF is short for Neural Radiance Fields, which is applied to reconstruct building-scale and even city-scale scenes. A sparsely Gated Mixture of Experts(MoE) is applied to optimize the NeRF sub-networks. The overall structure of Switch-NeRF is shown below. In this architecture, the Top-1 function on gate values G(x) normalized by Softmax is applied to sparsely select only one expert $E_s$ from a set of NeRF experts. Then the final output E(x) will be $G(x)_s E_s(x)$. The Gating network architecture is also shown below. In this architecture, G(x) equals Softmax(Linear(S(x))), and S(x) is an internal feature of a sub-network.

![Structure of Switch-NeRF](/images/DailyPaper/03/6.jpeg "Structure of Switch-NeRF")

![Structure of Gating Network](/images/DailyPaper/03/7.jpeg "Structure of Gating Network")

### [214_Advance Radiograph Representation Learning with Masked Record Modeling](https://arxiv.org/abs/2302.01872)

- This paper proposes **a unified framework based on Masked Record Modeling(MRM)** for radiograph representation learning to explore the direction of merging self-supervision and associated reports. MRM learns and repairs the masks in reports and radiographs to improve performance. With MRM pre-training, the radiography models could be trained more straightforwardly, faster, and better. The overview of the training strategy is shown below. Firstly, mask some input radiographs and report texts. Second, use the features reconstructed by addition merging to reconstruct the report texts. Thirdly, use the features of images to reconstruct images.

![MRM](/images/DailyPaper/03/8.jpeg "MRM's Training Stradegy")

- In the following experiment, this method outperforms other methods in RSNA Pneumonia and SIIM. Moreover, this method uses 1% labeled data to achieve the performance of other methods with 100% labeled data. In the medical graph image analysis, this is entirely meaningful.

### [215_Token Merging: Your ViT But Faster](https://arxiv.org/pdf/2210.09461)

- This paper proposes **Token Merging(ToMe)** to increase the throughput of the existing ViT model without needing to train. Since pruning token could lose information and needs re-training, ToMe combines tokens and increases the speed. The basic idea of using ToMe is to insert this module between the attention module and MLP in the ViT, as shown below (b). If there are L blocks and each block merges r tokens, it will reduce rL tokens. To find the similar tokens as shown below (a), the average of **keys(K)** features in different heads and **cosine** function are used to measure the similarity when performing the ablation experiment. Besides, $A = softmax(\frac{QK^T}{\sqrt d} + log s)$ is used to adjust the attention weight, and s is a row vector containing the size of each token. Moreover, this paper proposes **Bipartite Soft Matching** to merge tokens, as shown below (c). Firstly, partition the tokens into two sets, A and B, of roughly equal size. Secondly, draw one edge from each token in A to its most similar token in B. Thirdly, keep the r most similar edges. Fourthly, merge tokens that are still connected (e.g., by averaging their features). Finally, concatenate the two sets back together. The matching algorithm here is used to merge the tokens accurately rather than the clustering, which can not accurately decrease the tokens. In the following experiment part, the participation of ToMe improves the model's performance.

![ToMe](/images/DailyPaper/03/9.jpeg "ToMe")


### [216_HexPlane: A Fast Representation for Dynamic Scenes](https://arxiv.org/pdf/2301.09632)

- This paper raises **HexPlane**, which represents the dynamic 3D scenes by six planes of the learned feature. This method is fast than the NeRF method. Pairing with a tiny MLP, the workflow of HexPlane is shown below. The idea of HexPlane is simple, and it decomposes a 4D spacetime grid into six feature planes spanning each pair of coordinate axes, computes a feature vector for 4D points in spacetime by projecting the point onto each feature plane, then aggregating the six resulting feature vectors. Then a tiny MLP is used to predict.

![Work Flow of HexPlane](/images/DailyPaper/03/10.jpeg "Work Flow of HexPlane")

- The method overview of HexPlane is shown below. The details of the formulations will not be shown here. Since HexPlane divides the observation plane into six planes, the memory consumption can be adjusted in each plane. Therefore, HexPlane can save memory consumption. In conclusion, HexPlane alleviates the large memory consumption and sparse detection problem in dynamic scene reconstruction.

![HexPlane](/images/DailyPaper/03/11.jpeg "HexPlane")


### [217_Model-Agnostic Hierarchical Attention for 3D Object Detection](https://arxiv.org/abs/2301.02650)

- This paper proposes **two novel attention mechanisms** as modulized hierarchical designs for transformer-based 3D detectors. One is called **Simple Multi-Scale Attention**, which builds multi-scale tokens from a single-scale input feature to enable feature learning at different scales. The other is called **Size-Adaptive Local Attention**, with adaptive attention ranges for every bounding box proposal to localize feature aggregation. These two modules are plug-and-play. The illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention is shown below. With these two modules, the transformer could improve the ability to detect small objects.

![Illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention](/images/DailyPaper/03/12.png "Illustration of Simple Multi-Scale Attention and Size-Adaptive Local Attention")

- The Simple Multi-Scale Attention part uses two scales of features as key and value in the cross-attention between the target candidate and other points. Furthermore, multi-scale feature aggregation is implemented by multi-scale token aggregation, where different scales of key and value are used in different subsets of attention heads. Therefore, a higher-resolution feature map could be built with richer geometric detail for the point cloud. Size-Adaptive Local attention is used to build the hierarchical architecture by defining local areas. First, the middle-bounding box suggestion is generated with the features of the target candidate. Then a cross-attention function is performed between each candidate and the points sampled from within its corresponding box suggestion. Thus, the size-adaptive local region for each query point could be customized. Also, a max N local is set. If the bounding box is over N local, it will be randomly cropped into N local. If the bounding box is less than N local, it will be a padded token to achieve N local. This strategy is helpful for generalization.

### [218_PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://arxiv.org/abs/2209.06794)

- This paper proposes **Pathways Language and Image Model(PaLI)**, which generates text based on visual and textual inputs, and with this interface, performs many vision, language, and multi-modal tasks in many languages. Moreover, joint scaling of the vision and language components is essential. This paper converts multiple image and language tasks into generalized VQA-like tasks to share knowledge among them. All tasks are constructed using "image + query to answer," where both retrieval and answer are represented as text tokens. This model allows PaLI to use cross-task transfer learning and enhance language-and-image comprehension in various visual and linguistic problems: image description, visual question, and answer, scene text understanding, etc. An example of a PaLI task is shown below.

![Example of PaLI Task](/images/DailyPaper/03/13.png "Example of PaLI Task")

- The architecture of PaLI is simple, which is shown below. The architecture uses an encoder-decoder Transformer model with a large-capacity ViT component for image processing. To provide visual tokens to the text encoder as input: the visual transformer takes the image and outputs the relevant features. Pooling is not applied to the output of the visual transformer until the visual token is passed to the encoder-decoder model via cross-attention. Besides, the training dataset uses WebLI. From my perspective, the most accurate model in the future will be enormous, which is hard to develop by a poor team. The wealthy company or something else will still handle the most accurate model.

![Architecture of PaLI](/images/DailyPaper/03/14.png "Architecture of PaLI")

### [219_Pseudo Pyramid Deeper Bidirectional ConvLSTM for Video Saliency Detection](https://openaccess.thecvf.com/content_ECCV_2018/papers/Hongmei_Song_Pseudo_Pyramid_Deeper_ECCV_2018_paper.pdf)

- This paper proposes a fast video salient object detection model based on a novel recurrent network architecture named **Pyramid Dilated Bidirectional ConvLSTM (PDB-ConvLSTM)**. This network is designed for video saliency object detection. A **Pyramid Dilated Convolution (PDC) module** is first designed for simultaneously extracting spatial features at multiple scales. PDC can further improve the spatial learning ability of DB-ConvLSTM. These spatial features are then concatenated and fed into an extended **Deeper Bidirectional ConvLSTM (DB-ConvLSTM)** to learn spatiotemporal information. This LSTM architecture can capture video sequences' long and short-term memory, contains both temporal and spatial information, and enhances the information exchange between LSTM units in bi-directions. The overall architecture of PDB-ConvLSTM is shown below.

![Architecture of PDB-ConvLSTM](/images/DailyPaper/03/15.png "Architecture of PDB-ConvLSTM")

- The PDC module is shown below. It consists of convolution layers with different dilation ratios to emphasize multi-scale spatial saliency representation learning. PDC lets the network automatically learn the weights of different features, which is more intuitive and effective than ASPP.

![PDC Module](/images/DailyPaper/03/16.png "PDC Module")

- The ConvLSTM and DB-ConvLSTM modules are shown below. ConvLSTM introduces convolution operation into input-to-state and state-to-state transitions. ConvLSTM preserves spatial information as well as models temporal dependency. The Bidirectional ConvLSTM (B-ConvLSTM) should capture temporal characteristics in bi-directions since ConvLSTM only remembers past sequences. However, in B-ConvLSTM, there is no information exchange between the forward and backward directional LSTM units. Therefore, Deeper Bidirectional ConvLSTM (DB-ConvLSTM) improves B-ConvLSTM by organizing the forward and backward ConvLSTM units in a cascaded and tighter way. Moreover, to extract more powerful spatiotemporal information and let the network adapt to salient targets at different scales, DB-ConvLSTM with a PDC-like structure(PDB-ConvLSTM) is presented. It can utilize different receptive fields' features to capture more complementary spatiotemporal features. So this is the evolution process of PDB-ConvLSTM.

![ConvLSTM and DB-ConvLSTM](/images/DailyPaper/03/17.png "ConvLSTM and DB-ConvLSTM")

### [220_SEARCHING FOR ACTIVATION FUNCTIONS](https://arxiv.org/pdf/1710.05941.pdf)

- This paper proposes **Swish(f(x)=x$\times$sigmoid($\beta x$))** which may has the better performance than ReLU. The activation function plays a major role in the success of training deep neural networks. The better activation function will contribute to better neural networks. An automatic searching technique is used in the discovery process.

- Swish = x$\sigma(\beta x)$, $\sigma(z)=(1+exp(-z)^{-1})$, $\beta$ is either a constant or a trainable parameter. The first derivative of Swish is f'(x) = $\beta f(x)$ + $\sigma(\beta x)$(1 - $\beta$f(x)). The figures of Swish and the first derivatives of Swish are shown below. Swish has no upper bound, lower bound, smooth, or non-monotonic properties. In the following experiment, the model with Swish rather than ReLU performs better in some datasets.

![Swish](/images/DailyPaper/03/18.png "Swish")
