---
layout: post
title: Daily Paper (2023.04.01 - 2023.04.30)
categories: [Updating]
description: Some interesting papers
keywords: 
---

Since it is always hard to obtain a suitable dataset to train the model, researchers tend to find a method to create their dataset. However, there exist two principal problems: Data Labeling and Creating Data. I will focus on reading the paper related to data labeling, creating data this month, and reviewing deep learning knowledge.

### [41_Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233.pdf)

- Diffusion models are a class of likelihood-based models that have recently been shown to generate high-quality images while providing desirable properties such as higher distribution coverage, stable training targets, and better scalability. The whole modeling process of the Diffusion model is the process of adding or removing noise to the image. From the original image to the Gaussian noise is the process of adding noise step by step, and from the Gaussian noise back to the original image is the process of reducing noise step by step. This paper hypothesizes that the gap between diffusion models and GANs is at least twofold: first, the model architectures used in the recent GAN literature have been heavily explored and improved; second, GANs can trade-off quality and diversity to generate high-quality samples that do not cover the entire distribution. So this paper improves the model architecture by designing a scheme that trades diversity for quality. Here are some improvements on the architecture: (1) Increase the depth and width to keep the model size relatively constant. (2) Increase the number of attention heads. (3)Use 32×32,16×16 and 8×8 resolution attentions instead of just 16×16. (4) Use BigGAN residual blocks to upsample and downsample activation values. (5) Rescale the number of residual connections by a factor of $\frac{1}{\sqrt{2}}$. From a series of ablation experiments, widening or deepening the network's depth can help improve the model's performance. Also, increasing the number of attention heads and combining attention modules with multiple resolutions helps improve the model performance more than using only a single head with a single resolution. The upsampling and downsampling residual blocks of BigGAN also help to improve the model performance, although modifying the residual connection strength has no positive effect. At the same time, using attention with a smaller number of channels helps to improve performance. The algorithm of the classifier is shown below. The guidance of the classifier gradient improves the performance of the model output by class.

![Classifier](/images/DailyPaper/05/00.png "Classifier")

- The existing generative modeling techniques can be broadly classified into two categories based on how they represent probability distributions. (1) The first is likelihood-based models, which directly learn a distribution's probability density (or mass) function by approximating the maximum likelihood. Typical likelihood-based models include autoregressive models, normalized flow models, energy-based models (EBMs), and variational self-encoders (VAEs). (2) The second type is implicit generative models, in which the probability distribution is represented implicitly by a model of the sampling process. The most prominent example is generative adversarial networks (GANs), which synthesize new samples of data distributions by transforming random Gaussian vectors with neural networks.

### [42_Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)

- This paper introduces **Denoising Diffusion Probabilistic Model (DDPM)**. The general workflow is the following: (1) The Gaussian noise is predicted at each time step by $x_t$ and t, and subsequently, the mean value is obtained; (2) Obtain the variance $\sum_\theta (x_t, t)$, using untrained $\beta_t$ in DDPM. (3) Obtain q($x_{t-1}\|x_t$), and use the re-parameters to obtain $x_{t-1}$. The training process is: (1) Obtain input $x_0$ and randomly sample t from 1 to T. (2) Sample a noise $z_t$ from the standard Gaussian distribution. (3) Minimize the sampling noise. Since the paper has many equations, I will not show them very detailedly. The diffusion diagram is shown below. Training, Sampling, Sending, and Receiving algorithms are shown below.

![Diffusion](/images/DailyPaper/05/01.png "Diffusion")

![Training & Sampling](/images/DailyPaper/05/02.png "Training & Sampling")

![Sending & Receiving](/images/DailyPaper/05/03.png "Sending & Receiving")

### [43_A Survey on Generative Diffusion Model](https://arxiv.org/pdf/2209.02646.pdf)

- This is the general introduction to the diffusion model. Diffusion models have an inherent drawback of many sampling steps and extended sampling compared to generative adversarial models. Diffusion models have an intrinsic disadvantage of a large number of sampling steps and long sampling times compared to generative adversarial networks (GANs) and an inherent drawback of a large number of sampling steps and long sampling times compared to variational auto-encoders (VAEs). This disadvantage is because the diffusion step using the Markov kernel requires only a small perturbation, which leads to a large amount of diffusion. Also, the actionable model requires the same number of steps in the inference process in the inference process. Therefore, it requires thousands of steps to sample noise against random noise until it finally changes to high-quality data similar to the primary data. Therefore, many works want to speed up the diffusion process while improving the sampling quality. For example, DPM-solver exploits the stability of ODE to generate samples in 10 steps to generate state-of-the-art samples. ES-DDPM successfully combines trajectory learning with variational auto-encoders to achieve high-speed sampling of diffusion models. The comparison among the models is shown below.

![Model Comparison](/images/DailyPaper/05/04.png "Model Comparison")

- The general introduction structure of this paper is shown below. Since I label some critical points in the article, I would not write them here: It is just the copy things.

![Paper Architecture](/images/DailyPaper/05/05.png "Paper Architecture")

### [44_Diffusion probabilistic models for 3d point cloud generation](https://arxiv.org/pdf/2103.01458.pdf)

- This paper presents a probabilistic model for the point cloud generation, which is a probabilistic generative model for point clouds inspired by non-equilibrium thermodynamics, exploiting the reverse diffusion process to learn the point distribution. This paper models this backward diffusion process as a Markov chain that transforms the noise distribution into a target distribution. Our goal is to learn its transition kernel so that the Markov chain can reconstruct the desired shape. Moreover, since the purpose of a Markov chain is to model the point distribution, it is not possible to generate point clouds of various shapes by Markov chains alone. For this reason, we introduce a shape latent code (latent code) to find a latent space of shape representations that can be controlled by latent variables, generally through a network to learn the conditions generated as transition kernels. To evaluate the generation quality, we employ the minimum matching distance (MMD), the coverage score (COV), 1-NN classifier accuracy (1-NNA), and the Jenson-Shannon divergence (JSD). The illustration of the training and sampling model is shown below:

![Training & Sampling](/images/DailyPaper/05/06.png "Training & Sampling")

### [45_Score-based point cloud denoising](https://arxiv.org/pdf/2107.10981.pdf)

- This paper models this backward diffusion process as a Markov chain that transforms the noise distribution into a target distribution. Moreover, since the purpose of a Markov chain is to model the point distribution, it is impossible to generate point clouds of various shapes by Markov chains alone. Besides, this paper introduces a latent shape code (latent code) to find a latent space of shape representations that can be controlled by latent variables, generally through a network, to learn the conditions generated as transition kernels. The illustration of different methods is shown below. The network architecture is shown in the following. The denoising methods' representations are shown below, which is very beautiful.

![Illustration if Different Methods](/images/DailyPaper/05/08.png "Illustration if Different Methods")

![Network Architecture](/images/DailyPaper/05/07.png "Network Architecture")

![Denosing Methods' Visualization](/images/DailyPaper/05/09.png "Denosing Methods' Visualization")

### [46_Assessing the reliability of the Laban Movement Analysis System](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0218179&type=printable)

- This paper presents a gre\aph-based representation and a custom video annotation tool for Laban Movement Analysis (LMA). In LMA, movement is observed as a pattern of change that occurs in terms of four components: Body, Effort, Space, and Shape (referred to collectively as BESS). Additionally, LMA defines the meta-category of Phrasing. What LMA does to understand movement is to observe, recognize and describe patterns of change. LMA considers *Efforts* which encompasses four factors: Weight, Time, Space, and Flow. Space relates to how the mover orients their attention to the environment. The mover's sense of urgency is encoded with the Time factor, while weight encodes the mover's impact on the world. Flow captures the mover's attitude towards bodily control. Each Effort Factor is a continuum with two opposite ends called "Elements" (Space: Direct/Indirect, Time: Sudden/Sustained, Weight: Light/Strong, Flow: Bound/ Free). Effort qualities indicate where movement lies on the continuum between these poles. Laban formalized the Space component by dividing what he called the 'Kinesphere,' i.e., the volume defined by the reaching possibilities of the limbs in the 3-dimensional Cartesian space with oneself at its center. Laban also specified different zones in the Kinesphere in which movement can occur: Up, Down, Forward, Backward, Side-Open, and Side-Across. The Laban Effort is shown below.

![Laban Effort](/images/DailyPaper/05/10.png "Laban Effort")

- Besides, this paper introduces a decision tree to annotate the LMA gesture, as shown in the following. It helps generate the LLM, and this is complex.

![Decision Tree](/images/DailyPaper/05/11.png "Decision Tree")


### [47_Communicating emotions and mental states to robots in a real time parallel framework using Laban Movement Analysis.pdf](http://tivipe.com/TVPresearch/LourensBerkelBarakovaRAS2010.pdf)

- This paper introduces that LMA-based computer analysis can be a common language for expressing and interpreting emotional movements between robots and humans. In that way, it resembles the common coding principle between action and perception by humans and primates embodied by the mirror neuron system. Non-kinematic features of movements are the qualitative aspects of motion characterized by intensity, shape, force, flow, and rhythm changes. LMS consists of Body, Effort, Shape, and Space (BESS). Laban considers movement expressions almost always a combination of two or three movement atoms, i.e., dominant effort factors (Space, Time, Flow, Weight). The laban effort graph is shown below.

![Laban Effort Graph](/images/DailyPaper/05/12.png "Laban Effort Graph")

- The description in this paper needed to be analyzed carefully.

### [48_Expressing and interpreting emotional movements in social games with robots.pdf](https://link-springer-com.lib.ezproxy.hkust.edu.hk/article/10.1007/s00779-009-0263-2)

- This paper shows that the quantitative movement parameters can be matched to the emotional state of the embodied agent (human or robot) using the Laban movement analysis. The Laban Movement provides descriptors for the content of human body movements in the following categories: **Body, Space, Effort, and Relationship**. The Effort category relates to the dynamic and expressive characteristics of the action. It comprises four movement qualities: **weight, space (not to be confused with the Space category), time, and flow**. Each grade represents a continuum between opposite polarities: **weight varies between strength and lightness, space between direct and indirect, time between sudden and sustained, and flow between bound and free**. Effort qualities usually appear in combinations called 'states' or 'drives.' **When combining two motion qualities, it is called inner attitudes or incomplete effort. When three motion qualities are combined, they form externalized drives**. The Passion drive is exciting and combines time, weight, and flow. Using these motion determinants that the body takes in space allows a way to differentiate expressive and emotional actions noticeably. As proposed in the paper, **Time can be characterized by looking at the movement's acceleration; A combination of acceleration and velocity characterizes weight; Flow needs the values of all three components to be assessed.** The visualization is shown below.

![Visualization](/images/DailyPaper/05/13.png "Visualization")

- In the experiment part, (1) Happy waving provides a regular waving pattern with a relatively high frequency; (2) Angry waving demonstrates bursts with tremendous acceleration; (3) Sad waving demonstrates a profile of low acceleration; its frequency is relatively low and appears to have a lower frequency compared to the other three emotions; (4) Polite waving is a regular pattern with a relatively high frequency that is obtained by using minimal energy. The frequency is shown below:

![Frequency & Acceleration](/images/DailyPaper/05/14.png "Frequency & Acceleration")

### [49_Computational Laban movement analysis using probability calculus](http://www.cim.pt/docs/84/pdf#page=223)

- This work presents a system which implements the concept ofLaban MovementAnalysis (LMA) using probability calculus and Bayesian theory. The Bayesian framework models the dependencies between the low-level features and the descriptors of LMA. The Bayes-Net of the LMA model is shown below:

![Bayes-Net](/images/DailyPaper/05/15.png "Bayes-Net")


### [410_Designing Gestures for Affective Input: An Analysis of Shape, Effort and Valence](https://core.ac.uk/download/pdf/11433274.pdf)

### [411_Emotion from Motion](http://www.graphicsinterface.org/wp-content/uploads/gi1996-26.pdf)
