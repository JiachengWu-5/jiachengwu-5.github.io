---
layout: post
title: Daily Paper (2023.04.01 - 2023.04.30)
categories: [Updating]
description: Some interesting papers
keywords: 
---

Since it is always hard to obtain a suitable dataset to train the model, researchers tend to find a method to create their dataset. However, there exist two principal problems: Data Labeling and Creating Data. I will focus on reading the paper related to data labeling, creating data this month, and reviewing deep learning knowledge.

### [41_Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233.pdf)

- Diffusion models are a class of likelihood-based models that have recently been shown to generate high-quality images while providing desirable properties such as higher distribution coverage, stable training targets, and better scalability. The whole modeling process of the Diffusion model is the process of adding or removing noise to the image. From the original image to the Gaussian noise is the process of adding noise step by step, and from the Gaussian noise back to the original image is the process of reducing noise step by step. This paper hypothesizes that the gap between diffusion models and GANs is at least twofold: first, the model architectures used in the recent GAN literature have been heavily explored and improved; second, GANs can trade-off quality and diversity to generate high-quality samples that do not cover the entire distribution. So this paper improves the model architecture by designing a scheme that trades diversity for quality. Here are some improvements on the architecture: (1) Increase the depth and width to keep the model size relatively constant. (2) Increase the number of attention heads. (3)Use 32×32,16×16 and 8×8 resolution attentions instead of just 16×16. (4) Use BigGAN residual blocks to upsample and downsample activation values. (5) Rescale the number of residual connections by a factor of $\frac{1}{\sqrt{2}}$. From a series of ablation experiments, widening or deepening the network's depth can help improve the model's performance. Also, increasing the number of attention heads and combining attention modules with multiple resolutions helps improve the model performance more than using only a single head with a single resolution. The upsampling and downsampling residual blocks of BigGAN also help to improve the model performance, although modifying the residual connection strength has no positive effect. At the same time, using attention with a smaller number of channels helps to improve performance. The algorithm of the classifier is shown below. The guidance of the classifier gradient improves the performance of the model output by class.

![Classifier](/images/DailyPaper/05/00.png "Classifier")

- The existing generative modeling techniques can be broadly classified into two categories based on how they represent probability distributions. (1) The first is likelihood-based models, which directly learn a distribution's probability density (or mass) function by approximating the maximum likelihood. Typical likelihood-based models include autoregressive models, normalized flow models, energy-based models (EBMs), and variational self-encoders (VAEs). (2) The second type is implicit generative models, in which the probability distribution is represented implicitly by a model of the sampling process. The most prominent example is generative adversarial networks (GANs), which synthesize new samples of data distributions by transforming random Gaussian vectors with neural networks.

### [42_Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)


### [43_A Survey on Generative Diffusion Model](https://arxiv.org/pdf/2209.02646.pdf)