---
layout: post
title: Daily Paper (2023.04.01 - 2023.04.30)
categories: [Updating]
description: Some interesting papers
keywords: 
---

Since it is always hard to obtain a suitable dataset to train the model, researchers tend to find a method to create their dataset. However, there exist two principal problems: Data Labeling and Creating Data. I will focus on reading the paper related to data labeling, creating data this month, and reviewing deep learning knowledge.

### [41_Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233.pdf)

- Diffusion models are a class of likelihood-based models that have recently been shown to generate high-quality images while providing desirable properties such as higher distribution coverage, stable training targets, and better scalability. The whole modeling process of the Diffusion model is the process of adding or removing noise to the image. From the original image to the Gaussian noise is the process of adding noise step by step, and from the Gaussian noise back to the original image is the process of reducing noise step by step. This paper hypothesizes that the gap between diffusion models and GANs is at least twofold: first, the model architectures used in the recent GAN literature have been heavily explored and improved; second, GANs can trade-off quality and diversity to generate high-quality samples that do not cover the entire distribution. So this paper improves the model architecture by designing a scheme that trades diversity for quality. Here are some improvements on the architecture: (1) Increase the depth and width to keep the model size relatively constant. (2) Increase the number of attention heads. (3)Use 32×32,16×16 and 8×8 resolution attentions instead of just 16×16. (4) Use BigGAN residual blocks to upsample and downsample activation values. (5) Rescale the number of residual connections by a factor of $\frac{1}{\sqrt{2}}$. From a series of ablation experiments, widening or deepening the network's depth can help improve the model's performance. Also, increasing the number of attention heads and combining attention modules with multiple resolutions helps improve the model performance more than using only a single head with a single resolution. The upsampling and downsampling residual blocks of BigGAN also help to improve the model performance, although modifying the residual connection strength has no positive effect. At the same time, using attention with a smaller number of channels helps to improve performance. The algorithm of the classifier is shown below. The guidance of the classifier gradient improves the performance of the model output by class.

![Classifier](/images/DailyPaper/05/00.png "Classifier")

- The existing generative modeling techniques can be broadly classified into two categories based on how they represent probability distributions. (1) The first is likelihood-based models, which directly learn a distribution's probability density (or mass) function by approximating the maximum likelihood. Typical likelihood-based models include autoregressive models, normalized flow models, energy-based models (EBMs), and variational self-encoders (VAEs). (2) The second type is implicit generative models, in which the probability distribution is represented implicitly by a model of the sampling process. The most prominent example is generative adversarial networks (GANs), which synthesize new samples of data distributions by transforming random Gaussian vectors with neural networks.

### [42_Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)

- This paper introduces **Denoising Diffusion Probabilistic Model (DDPM)**. The general workflow is the following: (1) The Gaussian noise is predicted at each time step by $x_t$ and t, and subsequently, the mean value is obtained; (2) Obtain the variance $\sum_\theta (x_t, t)$, using untrained $\beta_t$ in DDPM. (3) Obtain q($x_{t-1}\|x_t$), and use the re-parameters to obtain $x_{t-1}$. The training process is: (1) Obtain input $x_0$ and randomly sample t from 1 to T. (2) Sample a noise $z_t$ from the standard Gaussian distribution. (3) Minimize the sampling noise. Since the paper has many equations, I will not show them very detailedly. The diffusion diagram is shown below. Training, Sampling, Sending, and Receiving algorithms are shown below.

![Diffusion](/images/DailyPaper/05/01.png "Diffusion")

![Training & Sampling](/images/DailyPaper/05/02.png "Training & Sampling")

![Sending & Receiving](/images/DailyPaper/05/03.png "Sending & Receiving")

### [43_A Survey on Generative Diffusion Model](https://arxiv.org/pdf/2209.02646.pdf)

- This is the general introduction to the diffusion model. Diffusion models have an inherent drawback of many sampling steps and extended sampling compared to generative adversarial models. Diffusion models have an intrinsic disadvantage of a large number of sampling steps and long sampling times compared to generative adversarial networks (GANs) and an inherent drawback of a large number of sampling steps and long sampling times compared to variational auto-encoders (VAEs). This disadvantage is because the diffusion step using the Markov kernel requires only a small perturbation, which leads to a large amount of diffusion. Also, the actionable model requires the same number of steps in the inference process in the inference process. Therefore, it requires thousands of steps to sample noise against random noise until it finally changes to high-quality data similar to the primary data. Therefore, many works want to speed up the diffusion process while improving the sampling quality. For example, DPM-solver exploits the stability of ODE to generate samples in 10 steps to generate state-of-the-art samples. ES-DDPM successfully combines trajectory learning with variational auto-encoders to achieve high-speed sampling of diffusion models. The comparison among the models is shown below.

![Model Comparison](/images/DailyPaper/05/04.png "Model Comparison")

- The general introduction structure of this paper is shown below. Since I label some critical points in the article, I would not write them here: It is just the copy things.

![Paper Architecture](/images/DailyPaper/05/05.png "Paper Architecture")

### [44_Diffusion probabilistic models for 3d point cloud generation](https://arxiv.org/pdf/2103.01458.pdf)

- This paper presents a probabilistic model for the point cloud generation, which is a probabilistic generative model for point clouds inspired by non-equilibrium thermodynamics, exploiting the reverse diffusion process to learn the point distribution. This paper models this backward diffusion process as a Markov chain that transforms the noise distribution into a target distribution. Our goal is to learn its transition kernel so that the Markov chain can reconstruct the desired shape. Moreover, since the purpose of a Markov chain is to model the point distribution, it is not possible to generate point clouds of various shapes by Markov chains alone. For this reason, we introduce a shape latent code (latent code) to find a latent space of shape representations that can be controlled by latent variables, generally through a network to learn the conditions generated as transition kernels. To evaluate the generation quality, we employ the minimum matching distance (MMD), the coverage score (COV), 1-NN classifier accuracy (1-NNA), and the Jenson-Shannon divergence (JSD). The illustration of the training and sampling model is shown below:

![Training & Sampling](/images/DailyPaper/05/06.png "Training & Sampling")

### [45_Score-based point cloud denoising](https://arxiv.org/pdf/2107.10981.pdf)

- This paper models this backward diffusion process as a Markov chain that transforms the noise distribution into a target distribution. Moreover, since the purpose of a Markov chain is to model the point distribution, it is impossible to generate point clouds of various shapes by Markov chains alone. Besides, this paper introduces a latent shape code (latent code) to find a latent space of shape representations that can be controlled by latent variables, generally through a network, to learn the conditions generated as transition kernels. The illustration of different methods is shown below. The network architecture is shown in the following. The denoising methods' representations are shown below, which is very beautiful.

![Illustration if Different Methods](/images/DailyPaper/05/08.png "Illustration if Different Methods")

![Network Architecture](/images/DailyPaper/05/07.png "Network Architecture")

![Denosing Methods' Visualization](/images/DailyPaper/05/09.png "Denosing Methods' Visualization")

### [46_Assessing the reliability of the Laban Movement Analysis System](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0218179&type=printable)

- This paper presents a gre\aph-based representsation and a custom video annotation tool for Laban Movement Analysis (LMA).

### [47_Communicating emotions and mental states to robots in a real time parallel framework using Laban Movement Analysis.pdf](http://tivipe.com/TVPresearch/LourensBerkelBarakovaRAS2010.pdf)


### [48_Expressing and interpreting emotional movements in social games with robots.pdf](https://link-springer-com.lib.ezproxy.hkust.edu.hk/article/10.1007/s00779-009-0263-2)