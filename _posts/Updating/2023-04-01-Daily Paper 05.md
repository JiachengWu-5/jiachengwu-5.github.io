---
layout: post
title: Daily Paper (2023.04.01 - 2023.04.30)
categories: [Updating]
description: Some interesting papers
keywords: 
---

Since it is always hard to obtain a suitable dataset to train the model, researchers tend to find a method to create their dataset. However, there exist two principal problems: Data Labeling and Creating Data. I will focus on reading the paper related to data labeling, creating data this month, and reviewing deep learning knowledge.

### [41_Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233.pdf)

- Diffusion models are a class of likelihood-based models that have recently been shown to generate high-quality images while providing desirable properties such as higher distribution coverage, stable training targets, and better scalability. The whole modeling process of the Diffusion model is the process of adding or removing noise to the image. From the original image to the Gaussian noise is the process of adding noise step by step, and from the Gaussian noise back to the original image is the process of reducing noise step by step. This paper hypothesizes that the gap between diffusion models and GANs is at least twofold: first, the model architectures used in the recent GAN literature have been heavily explored and improved; second, GANs can trade-off quality and diversity to generate high-quality samples that do not cover the entire distribution. So this paper improves the model architecture by designing a scheme that trades diversity for quality. Here are some improvements on the architecture: (1) Increase the depth and width to keep the model size relatively constant. (2) Increase the number of attention heads. (3)Use 32×32,16×16 and 8×8 resolution attentions instead of just 16×16. (4) Use BigGAN residual blocks to upsample and downsample activation values. (5) Rescale the number of residual connections by a factor of $\frac{1}{\sqrt{2}}$. From a series of ablation experiments, widening or deepening the network's depth can help improve the model's performance. Also, increasing the number of attention heads and combining attention modules with multiple resolutions helps improve the model performance more than using only a single head with a single resolution. The upsampling and downsampling residual blocks of BigGAN also help to improve the model performance, although modifying the residual connection strength has no positive effect. At the same time, using attention with a smaller number of channels helps to improve performance. The algorithm of the classifier is shown below. The guidance of the classifier gradient improves the performance of the model output by class.

![Classifier](/images/DailyPaper/05/00.png "Classifier")

- The existing generative modeling techniques can be broadly classified into two categories based on how they represent probability distributions. (1) The first is likelihood-based models, which directly learn a distribution's probability density (or mass) function by approximating the maximum likelihood. Typical likelihood-based models include autoregressive models, normalized flow models, energy-based models (EBMs), and variational self-encoders (VAEs). (2) The second type is implicit generative models, in which the probability distribution is represented implicitly by a model of the sampling process. The most prominent example is generative adversarial networks (GANs), which synthesize new samples of data distributions by transforming random Gaussian vectors with neural networks.

### [42_Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)

- This paper introduces **Denoising Diffusion Probabilistic Model (DDPM)**. The general workflow is the following: (1) The Gaussian noise is predicted at each time step by $x_t$ and t, and subsequently, the mean value is obtained; (2) Obtain the variance $\sum_\theta (x_t, t)$, using untrained $\beta_t$ in DDPM. (3) Obtain q($x_{t-1}\|x_t$), and use the re-parameters to obtain $x_{t-1}$. The training process is: (1) Obtain input $x_0$ and randomly sample t from 1 to T. (2) Sample a noise $z_t$ from the standard Gaussian distribution. (3) Minimize the sampling noise. Since the paper has many equations, I will not show them very detailedly. The diffusion diagram is shown below. Training, Sampling, Sending, and Receiving algorithms are shown below.

![Diffusion](/images/DailyPaper/05/01.png "Diffusion")

![Training & Sampling](/images/DailyPaper/05/02.png "Training & Sampling")

![Sending & Receiving](/images/DailyPaper/05/03.png "Sending & Receiving")

### [43_A Survey on Generative Diffusion Model](https://arxiv.org/pdf/2209.02646.pdf)

- This is the general introduction to the diffusion model. Diffusion models have an inherent drawback of many sampling steps and extended sampling compared to generative adversarial models. Diffusion models have an intrinsic disadvantage of a large number of sampling steps and long sampling times compared to generative adversarial networks (GANs) and an inherent drawback of a large number of sampling steps and long sampling times compared to variational auto-encoders (VAEs). This disadvantage is because the diffusion step using the Markov kernel requires only a small perturbation, which leads to a large amount of diffusion. Also, the actionable model requires the same number of steps in the inference process in the inference process. Therefore, it requires thousands of steps to sample noise against random noise until it finally changes to high-quality data similar to the primary data. Therefore, many works want to speed up the diffusion process while improving the sampling quality. For example, DPM-solver exploits the stability of ODE to generate samples in 10 steps to generate state-of-the-art samples. ES-DDPM successfully combines trajectory learning with variational auto-encoders to achieve high-speed sampling of diffusion models. The comparison among the models is shown below.

![Model Comparison](/images/DailyPaper/05/04.png "Model Comparison")

- The general introduction structure of this paper is shown below. Since I label some critical points in the article, I would not write them here: It is just the copy things.

![Paper Architecture](/images/DailyPaper/05/05.png "Paper Architecture")

### [44_Diffusion probabilistic models for 3d point cloud generation](https://arxiv.org/pdf/2103.01458.pdf)

- This paper presents a probabilistic model for the point cloud generation, which is a probabilistic generative model for point clouds inspired by non-equilibrium thermodynamics, exploiting the reverse diffusion process to learn the point distribution. This paper models this backward diffusion process as a Markov chain that transforms the noise distribution into a target distribution. Our goal is to learn its transition kernel so that the Markov chain can reconstruct the desired shape. Moreover, since the purpose of a Markov chain is to model the point distribution, it is not possible to generate point clouds of various shapes by Markov chains alone. For this reason, we introduce a shape latent code (latent code) to find a latent space of shape representations that can be controlled by latent variables, generally through a network to learn the conditions generated as transition kernels. To evaluate the generation quality, we employ the minimum matching distance (MMD), the coverage score (COV), 1-NN classifier accuracy (1-NNA), and the Jenson-Shannon divergence (JSD). The illustration of the training and sampling model is shown below:

![Training & Sampling](/images/DailyPaper/05/06.png "Training & Sampling")

### [45_Score-based point cloud denoising](https://arxiv.org/pdf/2107.10981.pdf)

- This paper models this backward diffusion process as a Markov chain that transforms the noise distribution into a target distribution. Moreover, since the purpose of a Markov chain is to model the point distribution, it is impossible to generate point clouds of various shapes by Markov chains alone. Besides, this paper introduces a latent shape code (latent code) to find a latent space of shape representations that can be controlled by latent variables, generally through a network, to learn the conditions generated as transition kernels. The illustration of different methods is shown below. The network architecture is shown in the following. The denoising methods' representations are shown below, which is very beautiful.

![Illustration if Different Methods](/images/DailyPaper/05/08.png "Illustration if Different Methods")

![Network Architecture](/images/DailyPaper/05/07.png "Network Architecture")

![Denosing Methods' Visualization](/images/DailyPaper/05/09.png "Denosing Methods' Visualization")

### [46_Assessing the reliability of the Laban Movement Analysis System](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0218179&type=printable)

- This paper presents a graph-based representation and a custom video annotation tool for Laban Movement Analysis (LMA). In LMA, movement is observed as a pattern of change that occurs in terms of four components: Body, Effort, Space, and Shape (referred to collectively as BESS). Additionally, LMA defines the meta-category of Phrasing. What LMA does to understand movement is to observe, recognize and describe patterns of change. LMA considers *Efforts* which encompasses four factors: Weight, Time, Space, and Flow. Space relates to how the mover orients their attention to the environment. The mover's sense of urgency is encoded with the Time factor, while weight encodes the mover's impact on the world. Flow captures the mover's attitude towards bodily control. Each Effort Factor is a continuum with two opposite ends called "Elements" (Space: Direct/Indirect, Time: Sudden/Sustained, Weight: Light/Strong, Flow: Bound/ Free). Effort qualities indicate where movement lies on the continuum between these poles. Laban formalized the Space component by dividing what he called the 'Kinesphere,' i.e., the volume defined by the reaching possibilities of the limbs in the 3-dimensional Cartesian space with oneself at its center. Laban also specified different zones in the Kinesphere in which movement can occur: Up, Down, Forward, Backward, Side-Open, and Side-Across. The Laban Effort is shown below.

![Laban Effort](/images/DailyPaper/05/10.png "Laban Effort")

- Besides, this paper introduces a decision tree to annotate the LMA gesture, as shown in the following. It helps generate the LLM, and this is complex.

![Decision Tree](/images/DailyPaper/05/11.png "Decision Tree")

### [47_Communicating emotions and mental states to robots in a real time parallel framework using Laban Movement Analysis.pdf](http://tivipe.com/TVPresearch/LourensBerkelBarakovaRAS2010.pdf)

- This paper introduces that LMA-based computer analysis can be a common language for expressing and interpreting emotional movements between robots and humans. In that way, it resembles the common coding principle between action and perception by humans and primates embodied by the mirror neuron system. Non-kinematic features of movements are the qualitative aspects of motion characterized by intensity, shape, force, flow, and rhythm changes. LMS consists of Body, Effort, Shape, and Space (BESS). Laban considers movement expressions almost always a combination of two or three movement atoms, i.e., dominant effort factors (Space, Time, Flow, Weight). The laban effort graph is shown below.

![Laban Effort Graph](/images/DailyPaper/05/12.png "Laban Effort Graph")

- The description in this paper needed to be analyzed carefully.

### [48_Expressing and interpreting emotional movements in social games with robots. pdf](https://link-springer-com.lib.ezproxy.hkust.edu.hk/article/10.1007/s00779-009-0263-2)

- This paper shows that the quantitative movement parameters can be matched to the emotional state of the embodied agent (human or robot) using the Laban movement analysis. The Laban Movement provides descriptors for the content of human body movements in the following categories: **Body, Space, Effort, and Relationship**. The Effort category relates to the dynamic and expressive characteristics of the action. It comprises four movement qualities: **weight, space (not to be confused with the Space category), time, and flow**. Each grade represents a continuum between opposite polarities: **weight varies between strength and lightness, space between direct and indirect, time between sudden and sustained, and flow between bound and free**. Effort qualities usually appear in combinations called 'states' or 'drives.' **Combining two motion qualities is called inner attitudes or incomplete effort. When three motion qualities are combined, they form externalized drives**. The Passion drive is exciting and combines time, weight, and flow. Using these motion determinants that the body takes in space allows a way to differentiate expressive and emotional actions noticeably. As proposed in the paper, **Time can be characterized by looking at the movement's acceleration; A combination of acceleration and velocity characterizes weight; Flow needs the values of all three components to be assessed.** The visualization is shown below.

![Visualization](/images/DailyPaper/05/13.png "Visualization")

- In the experiment part, (1) Happy waving provides a regular waving pattern with a relatively high frequency; (2) Angry waving demonstrates bursts with tremendous acceleration; (3) Sad waving demonstrates a profile of low acceleration; its frequency is relatively low and appears to have a lower frequency compared to the other three emotions; (4) Polite waving is a regular pattern with a relatively high frequency that is obtained by using minimal energy. The frequency is shown below:

![Frequency & Acceleration](/images/DailyPaper/05/14.png "Frequency & Acceleration")

### [49_Computational Laban movement analysis using probability calculus](http://www.cim.pt/docs/84/pdf#page=223)

- This work presents **a system which implements the concept of Laban Movement Analysis (LMA) using probability calculus and Bayesian theory**. The Bayesian framework models the dependencies between the low-level features and the descriptors of LMA. The Bayes-Net of the LMA model is shown below:

![Bayes-Net](/images/DailyPaper/05/15.png "Bayes-Net")

### [410_Designing Gestures for Affective Input: An Analysis of Shape, Effort, and Valence](https://core.ac.uk/download/pdf/11433274.pdf)

- This paper **identified three underlying dimensions of movements and emotions: shape, effort, and valence, and created a new affective interaction model**. This paper shows some examples of LMA which could be used as a reference. 

![Examples](/images/DailyPaper/05/16.png "Examples")

- Besides, some interpretations are shown below: (1) Excitement – enormously spreading, rising, and advancing movements; (2) Anger – somewhat spreading, rising, and advancing movements; (3) Surprise-afraid – enclosing, somewhat descending and retiring movements; (4) Sulkiness – enclosing, somewhat rising and retiring movements; (5) Surprise-interested – somewhat spreading, neutral in the vertical plane and advancing movements; (6) Pride – somewhat spreading, rising and somewhat advancing movements; (7) Satisfaction – neutral in all planes of movements; (8) Sadness – enclosing, descending and retiring movements; (9) Being in love – somewhat spreading, somewhat rising and somewhat advancing movements. In the graph, they will be:

![Examples](/images/DailyPaper/05/17.png "Examples")

### [410_Emotion from Motion](http://www.graphicsinterface.org/wp-content/uploads/gi1996-26.pdf)

- This paper uses cameras to capture and visualize people's motion. This paper finds the relationship between movement and emotion. For example, the angry motion's frequency is higher than the happy motion's.

### [411_PCE: What is It, How Does It Work and What are Its Limitations?](https://opg.optica.org/jlt/abstract.cfm?uri=jlt-32-4-528)

- This paper is read for the quiz.

- The main idea behind the PCE is to decouple the path computation function from the GMPLS controllers into a dedicated entity with an open and well-defined interface and protocol. A PCE is an entity (component, application, or network node) capable of computing a network path or route based on a network graph (TED) and applying computational constraints. The initial driver for deploying PCEs was the increasing complexity of path computation. The PCE architecture has two main components: the PCC (any client application requesting a path computation to a PCE) and a PCE. PCE protocol (PCEP) is used for communicating both PCC and PCE, as well as between PCEs. One of the critical aspects of the PCE is the synchronization mechanism by which the PCE obtains a copy of the TED to perform path computation.

- Decoupling the path computation from the rest of the control plane also provides further benefits. (1) More flexibility for network operators in the control of their networks (2) The ability to apply network operators' policies in the development of the path computation algorithms, not bound to software updates within the controllers (closed and vendor-dependent). (3) Third-party customized developments and upgrades of path computation algorithms. The PCE architecture supports two path computation models: centralized and distributed. Thus, PCEs (handling different switching layers or domains) can compute suboptimal paths with no collaboration (by expanding loose hops at PCE of transit domains) or collaboratively compute end-to-end optimum multilayer and multi-domain paths

- There exist some limitations: (1) Impairment-Aware Path Computation Problem: The degradation of the optical signal is due to the accumulation of physical impairments while the signal travels from the source toward the destination. (2) PCE-Based Solution for Impairment-Aware Path Computation: The PCE allows to overcome the limitations of the impairment-aware distributed control plane. Physical impairment information is gathered by dedicated monitors and stored in the enhanced TED (ETED) at the PCE. Thus, impairment information needs to be collected and flooded by the GMPLS controllers. Then, the PCE can use the TED and the ETED for path computation. (3) Multi-domain Path Computation Problem： Multi-domain networks are becoming a key component in core transport architectures, given the fact that an operator's network may include several equipment vendors and segmenting the network into domains [as interior gateway protocol(IGP) areas or autonomous systems(AS)] is a means to increase the overall scalability and for confidentiality reasons. （4）PCE-Based Solutions for Multi-domain Path Computation: Most initial research efforts on the PCE-based multi-domain path computation were targeted to improve or extend the backward recursive path computation (BRPC) procedure. (5) Multilayer Path Computation Problem: To fully exploit the advantages of traffic grooming, it is necessary to promote cooperation among the layers using a peer control plane interworking model. A GMPLS unified control plane is adopted, where a single control plane instance (routing and signaling) is applied in a ubiquitous way to control all the switching layers within the same domain. Thus, each node has a global view of all the layers, including the network topology and resource availability in the TED repository. (6) PCE-Based Solutions for Multilayer Path Computation: Network resource optimization can only be performed at each layer independently, leading to an increased blocking probability since the region boundary nodes selected by the source node are not optimal, as happened with the multi-domain path computation.

### [412_MotionCLIP: Exposing Human Motion Generation to CLIP Space](https://arxiv.org/abs/2203.08063)

- The GitHub Project is [here](https://github.com/GuyTevet/MotionCLIP). This paper shows **a method to show the complex motions by clipping them into simple movements and the approaches of projecting text to motion, which is needed in the project, and motion recognition and addition/subtraction**. These works are completed by **MotionCLIP**, which is a 3D human motion auto-encoder. MotionCLIP can align human motion to CLIP space and comprises a transformer-based motion auto-encoder. CLIP is a large-scale visual-textual embedding model. The MotionCLIP overview is shown below:

![Overview of MotionCLIP](/images/DailyPaper/05/18.png "Overview of MotionCLIP")

- The motion auto-encoder is shown below. The goal is to learn a semantic and disentangled motion representation that will serve as a basis for generation and editing tasks. To this end, the mapping to this representation (encoding) should be learned, and the mapping back to explicit motion (decoding). That's why a transformer-based motion auto-encoder is presented. The details of encoding, decoding, and transferring are shown in the paper. I would not like to paste it here. Motion sequences are presented using the SMPL body model. Since text and image are utilized, $L_Text$ and $L_Image$ (Calculated by cosine distance)are shown. Besides, the motions need to be reconstructed (Calculated by L2 losses), so the final loss will be $L = L_{recon} + \lambda_{text} L_{text} + \lambda_{image} L_{image}$. More details are highlighted in the paper. There is one fascinating fact even though CLIP has never seen anything from the motion domain or any other temporal signal. Its latent structure naturally induces semantics and disentanglement. This step is beneficial, but as proposed in conclusion, it needs help understanding directions to capture styles and cultural references. Therefore, there still exists some problems in this problem if we want to use this directly as the template of LLM.

![Motion Auto-encoder](/images/DailyPaper/05/19.png "Motion Auto-encoder")

### [412_Conditional Motion In-betweening](https://arxiv.org/abs/2202.04307)

- The GitHub Project is [here](https://jihoonerd.github.io/Conditional-Motion-In-Betweening/). Motion in Betweening (MIB) is a process of generating intermediate skeletal movement between the given start and target poses while preserving the naturalness of the motion. This project can be seen as a zero-shot learning problem. This paper proposes a method that can handle posture or semantic-conditioned MIB tasks using a unified model and presents a motion augmentation method to improve the quality of pose-conditioned motion generation via defining a distribution over smooth trajectories. If the MIB problem can be figured out, the generated motion can be more flexible and diverse. The MIB method presented in this paper is called **Conditional motion in-betweening**, which can handle pose-conditioned and semantic-conditioned MIB.

- This paper uses a joint position vector p $\in R^3$ and quaternion q $\in R^4$ as a rotational representation of each joint. Pose-conditioned and semantic-conditioned MIB is performed since the middle information will help reconstruct the whole motion. (Pose-conditioned motion in-betweening connects starting and target pose while satisfying a given anchor pose.) The architecture of the model is shown below. Starting ($p_{start}$), optional anchor ($p_k$) and target poses ($p_{target}$) are provided. The joint coordinates are interpolated with linear interpolation (LERP), and joint rotations are interpolated by spherical linear interpolation (SLERP) in quaternion. The details are highlighted in the paper. The Transformer part is very "Transformer."

![Model Architecture](/images/DailyPaper/05/20.png "Model Architecture")

- Randomized Shuffled Anchor Pose, Motion Data Augmentation, and Semantic Embedding are adopted in the training stage. There are three kinds of loss: $L_{semantic}$, $L_{position}$ and $L_{rotation}$. The model uses an AdamW optimizer to train the Transformer.

### [413_A Survey on Deep Learning for Skeleton-Based Human Animation](https://arxiv.org/abs/2110.06901)

- This article proposes a comprehensive survey of the state-of-the-art approaches based on either deep learning or deep reinforcement learning in skeleton-based human character animation. Since this paper is an overview, I highlighted essential parts of the article in Zotero.

### [413_Motion capture in robotics review](https://shib.ust.hk/idp/profile/SAML2/POST/SSO?execution=e1s3&_eventId_proceed=1)

- This paper introduces motion capture technologies and the current challenges in robotic systems. This paper is also a survey, and I highlighted essential parts of the Zotero. The tracking technologies contain **Passive Market(Adopted in the lab)**, Active Marker, Markerless, Inertial, Magnetic, Mechanical, and Acoustic.

### [414_Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1801.07455)

- This paper designs **a generic representation of skeleton sequences for action recognition by extending graph neural networks to a spatial-temporal graph model, called Spatial-Temporal Graph Convolutional Networks (ST-GCN)** rather than the traditional methods (Hand-crafted parts or rules to analyze the spatial patterns). The spatial and temporal graph is shown below. 'ST-GCN is the first to apply graph CNNs to the task of skeleton-based action recognition. It differentiates from previous approaches in that it can implicitly learn the part information by harnessing the locality of graph convolution and the temporal dynamics. By eliminating the need for manual part assignment, the model is easier to design and potent to learn better action representations.'

![Spatial Temporal Graph](/images/DailyPaper/05/21.png "Spatial Temporal Graph")

- The pipeline of ST-GCN is shown below. The input to the ST-GCN is the joint coordinate vectors on the graph nodes. Firstly, the temporal graph on the skeleton sequences in two steps. First, the joints within one frame are connected with edges according to the connectivity of the human body structure. Secondly, each joint will be connected to the same joint in the consecutive frame. The connections in this setup are thus naturally defined without the manual part assignment. This paper simplifies the process by partitioning the neighbor set $B(v_{ti})$ of a joint node vti into a fixed number of K subsets, where each subset has a numeric label. So the mapping will be $w(v_{ti}, v_{tj}) = w'(l_{ti}(v_{v_tj}))$. Since the temporal aspect of the graph is constructed by connecting the same joints across consecutive frames, this enables us to define a simple strategy to extend the spatial graph CNN to the spatial-temporal domain. There are several partition strategies, Uni-labeling (Feature vectors on every neighboring node will have an inner product with the same weight vector, but properties could be lost in this operation), Distance Partitioning (Partition the neighbor set according to the nodes' distance $d(·,v_{ti})$ to the root node $v_{ti}$.), and Spatial Configuration Partitioning (Divide the neighbor set into three subsets: The root node itself; Centripetal group: the neighboring nodes that are closer to the gravity center of the skeleton than the root node; Otherwise the centrifugal group.). The visualization of different partitioning strategies is shown below. To better learn the joint's movements, this paper adds a learnable mask M on every spatial, temporal graph convolution layer. The detailed structure of ST-GCN is shown in the paper.

![Pipeline fo ST-GCN](/images/DailyPaper/05/22.png "Pipeline fo ST-GCN")

![Visualization of Partitioning](/images/DailyPaper/05/23.png "Visualization of Partitioning")

### [415_Learnable Triangulation of Human Pose](https://arxiv.org/abs/1905.05754)

- This paper presents two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. **(1) A primary differentiable algebraic triangulation with the addition of confidence weights estimated from the input images; (2) A novel method of volumetric aggregation from intermediate 2D backbone feature maps.** Since the lab contains many cameras, this multi-view study is beneficial. The aggregated volume is then refined via 3D convolutions that produce final 3D joint heatmaps and allow implicit modeling of a human pose prior. There are two reasons for developing multi0view pose estimation: (1) Multi-view pose estimation is arguably the best way to obtain ground truth for monocular 3D pose estimation in the wild; (2) It can be used directly to track human pose in real-time for the practical end purpose in some cases. The backbone is shown in the paper. The difference between CMU and Human 3.6 M experiments is that in CMU, most cameras do not have the full view of a person the whole time, leading to solid occlusions and missing parts.

- The algebraic triangulation approach processes each joint j independently of each other and triangulates the 2D positions obtained from the j-joint’s backbone heatmaps from different views. The outline is shown below. A softmax function is performed first, then calculate the 2D parts of the joints as the center of mass of the corresponding heatmaps. A linear algebraic triangulation is used to infer the 3D position of the joints from the 2D estimates. RANSAC and the Huber loss are adopted to decrease the degradation.

![Algebraic Triangulation with Learned Confidences](/images/DailyPaper/05/24.png "Algebraic Triangulation with Learned Confidences")

- The volumetric triangulation approach unprojects the uninterpretable feature maps produced by the 2D backbone into 3D volumes, which is done by filling a 3D cube around the person via projecting an output of the 2D network along projection rays inside the 3D cube. The cubes obtained from multiple views are then aggregated together and processed. The world coordinates of the pelvis of the root node of the human body are first estimated using a baseline. Then generate a 3D bounding box of LxLxL around the pelvis of the root node of the human body. This 3D bounding box is discretized into three-dimensional pixels voxel, and the world coordinates $V^{coords}$ of the center of each three-dimensional pixel. Then unproject each camera and fill the cube with bilinear samples. Finally, there are three aggregation methods: Direct Summation, Normalized summation, and Softmax Summation.

![Volumetric Triangulation](/images/DailyPaper/05/25.png "Volumetric Triangulation")

### [416_Shape-aware Multi-Person Pose Estimation from Multi-View Images](https://arxiv.org/abs/2110.02330)

- This paper proposes a method to estimate the 3D poses of multiple people from multi-view images. A coarse-to-fine pipeline first aggregates noisy 2D observations from multiple camera views into 3D space and then associates them into individual instances based on a confidence-aware majority voting technique—the final pose estimates from a novel optimization that links high-confidence multi-view 2D observations and 3D joint candidates. The proposed method merges 2D and 3D to obtain better 2D detection. This method combines concepts from both bottom-up and top-down and presents a simple confidence-aware majority voting technique to accept initial 3D proposals. The first part of the pipeline consists of triangulating the 3D coordinates of all pairs of 2D detections with the same part label, followed by a confidence-aware majority voting technique to cluster the proposals. Then leverage the observation that certain joints are detected more reliably than end-effectors and can be used as a heuristic to decide the number and location of individual humans. The second part of our pipeline refines the initial 3D estimates based on a novel 2D-3D objective. The pipeline is shown below. Two stages will be performed.

![Pipeline](/images/DailyPaper/05/26.png "Pipeline")

- In the first stage, 3D joint candidates will be generated by triangulating 2D human pose estimates from different views. Then a confidence-aware voting-based technique is applied to joint cluster candidates from noisy observations and determine human instances. A practical approach to generate initial 3D pose proposals is proposed based on a confidence-aware voting technique, operating in the global 3D space of joint candidates triangulated from pairs of 2D noisy detections. The next step is to associate triangulated 3D joint candidates with individual instances. Since pairs of joint detections are triangulated, joints that are visible in several views produce dense clusters of 3D candidates. A 3D bounding box with a fixed size is placed and oriented using the hip candidates as anchors

- In the second stage, an energy formulation which includes a multi-view re-projection term and a 3D body model fitting term, is introduced to refine the initial pose X0. Both 3D pose X and SMPL parameters are optimized jointly alternately. For each iteration, the gradient updating network first takes current 3D poses and SMPL estimation as input to guide updating SMPL prediction.

### [417_OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)

- This paper introduces **OpenPose**. The approach uses a non-parametric representation, which we refer to as **Part Affinity Fields (PAFs)**is a set of 2D vector fields that encode the location and orientation of limbs over the image domain. The backbone of OpenPose is the first ten layers of VGG-19. The architecture of the two-branch multi-stage CNN is shown below. The loss function of OpenPose contains two parts: the loss for part maps and the affinity fields. Top-down approaches (e.g., AlphaPose) provide better results for images with few people, but their speed considerably drops for pictures with many people.

![Architecture of the Two-branch Multi-stage CNN](/images/DailyPaper/04/07.png "Architecture of the Two-branch Multi-stage CNN")

- The pipeline is the following: Input image; Two-branch multi-stage CNN network to predict both maps to predict key points and fields to indicate limbs; Dichotomous matching based on maps and areas; Generate the final pose. The part affinity is a 2D vector for each part. A Hungarian algorithm is used to obtain the best match in multiplayer recognition

![Pipeline of OpenPose](/images/DailyPaper/05/27.png "Pipeline of OpenPose")

### [418_Markerless Motion Capture of Multiple Characters Using Multi-view Image Segmentation](https://ieeexplore-ieee-org.lib.ezproxy.hkust.edu.hk/document/6468043)

- This paper introduces **a multi-task segmentation framework for markerless human motion capture**. To this end, a probabilistic shape and appearance model is employed to segment the input images and to assign each pixel uniquely to one person. Given the articulated template models of each person and the labeled pixels, a combined optimization scheme, which splits the skeleton pose optimization problem into a local one and a lower dimensional global one, is applied one by one to each individual, followed by surface estimation to capture detailed non-rigid deformations. The pipeline is shown below. The pipeline consists of Skeleton-based Pose Estimation, Surface Refinement, and Segmentation. MAP inference in a Markov Random Field determines the pixel labels in each image. After labeling each pixel in the input images, two types of boundary pixels are assigned. The first type of pixels lies on the boundary between a person and the background, which can be easily assigned to the correct person. Boundary pixels in regions where two or more persons overlap get the label of the person whose boundary region is closest to the camera. This paper uses the idea of divide-and-conquer.

![Pipeline](/images/DailyPaper/05/28.png "Pipeline")

### [419_A Survey of Vision-Based Human Motion Capture](https://www.sciencedirect.com/science/article/abs/pii/S107731420090897X)

- This paper provides a comprehensive review of recent deep learning-based solutions for 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. I highlight points in the paper in Zotero. The architecture of this paper is shown below.

![Architecture](/images/DailyPaper/05/30.png "Architecture")

### [420_Deep Learning-Based Human Pose Estimation: A Survey](https://arxiv.org/abs/2012.13392)

- This paper surveys the Motion Capturing techniques. I highlight points in the paper in Zotero. The architecture of this paper is shown below.

![Architecture](/images/DailyPaper/05/29.png "Architecture")

### [421_Integral Human Pose Regression](https://arxiv.org/abs/1711.08229)

- This paper shows that **a simple integral operation** relates and unifies the heat map representation and joint regression in 3D pose estimation, thus avoiding the on-differentiable postprocessing and quantization error. The approach is called **integral regression**, which modifies the “taking-maximum” operation to “taking-expectation” and unifies the heat map representation and joint regression approaches. The joint is estimated as integrating all locations in the heat map, weighted by their probabilities (normalized from likelihoods). Because the integral regression is parameter-free and only transforms the pose representation from a heat map to a joint, it does not affect other algorithm design choices and can be combined with any of them, including different tasks, a heat map, and joint losses, network architectures, image, and heat map resolutions. The general workflow of HPE is shown below. To solve the problem of non-differentiable and quantization error, $J_k = \int_{p\in \Omega} p H_k(p) = \sum_{p_z = 1}^D\sum_{p_y = 1}^H\sum_{p_x = 1}^W p H_k(p)$ replaces the arg max operation. The method represented in the paper is more robust to the image and heat map resolution variation, which makes it a better choice when the computational capabilities are restricted in practical scenarios.

![Workflow](/images/DailyPaper/05/31.png "Workflow")

### [422_Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods](https://arxiv.org/abs/2006.01423)

- This paper surveys the recent deep learning-based 2D and 3D human poses estimation methods published since 2014. I highlight points in the paper in Zotero.

### [423_Chapter 3 Motion Capture](https://www.uio.no/studier/emner/hf/imv/MUS2006/v15/litteratur/knkap3-4.pdf)

- This chapter introduced various methods of analyzing correspondences between sound and motion. Since it will be more convenient to check the highlighted parts in this chapter than to check this note, I would like to highlight some essential parts in Zotero.

### [424_PCE: What is It, How Does It Work and What are Its Limitations?](https://opg.optica.org/jlt/abstract.cfm?uri=jlt-32-4-528)

- This paper is read for the quiz.

- The main idea behind the PCE is to decouple the path computation function from the GMPLS controllers into a dedicated entity with an open and well-defined interface and protocol. A PCE is an entity (component, application, or network node) capable of computing a network path or route based on a network graph (TED) and applying computational constraints. The initial driver for deploying PCEs was the increasing complexity of path computation. The PCE architecture has two main components: the PCC (any client application requesting a path computation to a PCE) and a PCE. PCE protocol (PCEP) is used for communicating both PCC and PCE, as well as between PCEs. One of the critical aspects of the PCE is the synchronization mechanism by which the PCE obtains a copy of the TED to perform path computation.

- Decoupling the path computation from the rest of the control plane also provides further benefits. (1) More flexibility for network operators in the control of their networks (2) The ability to apply network operators' policies in the development of the path computation algorithms, not bound to software updates within the controllers (closed and vendor-dependent). (3) Third-party customized developments and upgrades of path computation algorithms. The PCE architecture supports two path computation models: centralized and distributed. Thus, PCEs (handling different switching layers or domains) can compute suboptimal paths with no collaboration (by expanding loose hops at PCE of transit domains) or collaboratively compute end-to-end optimum multilayer and multi-domain paths

- There exist some limitations: (1) Impairment-Aware Path Computation Problem: The degradation of the optical signal is due to the accumulation of physical impairments while the signal travels from the source toward the destination. (2) PCE-Based Solution for Impairment-Aware Path Computation: The PCE allows to overcome the limitations of the impairment-aware distributed control plane. Physical impairment information is gathered by dedicated monitors and stored in the enhanced TED (ETED) at the PCE. Thus, impairment information needs to be collected and flooded by the GMPLS controllers. Then, the PCE can use the TED and the ETED for path computation. (3) Multi-domain Path Computation Problem： Multi-domain networks are becoming a key component in core transport architectures, given the fact that an operator's network may include several equipment vendors and segmenting the network into domains [as interior gateway protocol(IGP) areas or autonomous systems(AS)] is a means to increase the overall scalability and for confidentiality reasons. （4）PCE-Based Solutions for Multi-domain Path Computation: Most initial research efforts on the PCE-based multi-domain path computation were targeted to improve or extend the backward recursive path computation (BRPC) procedure. (5) Multilayer Path Computation Problem: To fully exploit the advantages of traffic grooming, it is necessary to promote cooperation among the layers using a peer control plane interworking model. A GMPLS unified control plane is adopted, where a single control plane instance (routing and signaling) is applied in a ubiquitous way to control all the switching layers within the same domain. Thus, each node has a global view of all the layers, including the network topology and resource availability in the TED repository. (6) PCE-Based Solutions for Multilayer Path Computation: Network resource optimization can only be performed at each layer independently, leading to an increased blocking probability since the region boundary nodes selected by the source node are not optimal, as happened with the multi-domain path computation.

### [Diffusion Probabilistic Models for 3D Point Cloud Generation](https://ieeexplore.ieee.org/document/9578791/)

### [Executing your Commands via Motion Diffusion in Latent Space](http://arxiv.org/abs/2212.04048)

### [High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2112.10752)

### [HumanMAC - Masked Motion Completion for Human Motion Prediction](http://arxiv.org/abs/2302.03665)

### [MCVD - Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation](http://arxiv.org/abs/2205.09853)

### [MoFusion - A Framework for Denoising-Diffusion-based Motion Synthesis](http://arxiv.org/abs/2212.04495)

### [MotionDiffuse - Text-Driven Human Motion Generation with Diffusion Model](http://arxiv.org/abs/2208.15001)

I am very busy in preparing for the final exam and writing the codes. This will currently stop.
